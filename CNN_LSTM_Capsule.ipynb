{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9ECDQJJyKgfc"
      },
      "outputs": [],
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import dstack\n",
        "from pandas import read_csv\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from matplotlib import pyplot as plt\n",
        "import keras\n",
        "from keras import initializers\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras.backend import *\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import Dense, Layer, Lambda, Input, Flatten, Dropout, Conv2D, MaxPooling2D, Conv1D, MaxPooling1D, LSTM, TimeDistributed, ConvLSTM2D, Permute, Reshape, Conv2D\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.python.ops import array_ops\n",
        "from tensorflow.python.ops import math_ops\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping\n",
        "\n",
        "def own_batch_dot(x, y, axes=None):\n",
        "\t\"\"\"Batchwise dot product.\n",
        "\t`batch_dot` is used to compute dot product of `x` and `y` when\n",
        "\t`x` and `y` are data in batch, i.e. in a shape of\n",
        "\t`(batch_size, :)`.\n",
        "\t`batch_dot` results in a tensor or variable with less dimensions\n",
        "\tthan the input. If the number of dimensions is reduced to 1,\n",
        "\twe use `expand_dims` to make sure that ndim is at least 2.\n",
        "\tArguments:\n",
        "\t\tx: Keras tensor or variable with `ndim >= 2`.\n",
        "\t\ty: Keras tensor or variable with `ndim >= 2`.\n",
        "\t\taxes: list of (or single) int with target dimensions.\n",
        "\t\t\tThe lengths of `axes[0]` and `axes[1]` should be the same.\n",
        "\tReturns:\n",
        "\t\tA tensor with shape equal to the concatenation of `x`'s shape\n",
        "\t\t(less the dimension that was summed over) and `y`'s shape\n",
        "\t\t(less the batch dimension and the dimension that was summed over).\n",
        "\t\tIf the final rank is 1, we reshape it to `(batch_size, 1)`.\n",
        "\tExamples:\n",
        "\t\tAssume `x = [[1, 2], [3, 4]]` and `y = [[5, 6], [7, 8]]`\n",
        "\t\t`batch_dot(x, y, axes=1) = [[17, 53]]` which is the main diagonal\n",
        "\t\tof `x.dot(y.T)`, although we never have to calculate the off-diagonal\n",
        "\t\telements.\n",
        "\t\tShape inference:\n",
        "\t\tLet `x`'s shape be `(100, 20)` and `y`'s shape be `(100, 30, 20)`.\n",
        "\t\tIf `axes` is (1, 2), to find the output shape of resultant tensor,\n",
        "\t\t\tloop through each dimension in `x`'s shape and `y`'s shape:\n",
        "\t\t* `x.shape[0]` : 100 : append to output shape\n",
        "\t\t* `x.shape[1]` : 20 : do not append to output shape,\n",
        "\t\t\tdimension 1 of `x` has been summed over. (`dot_axes[0]` = 1)\n",
        "\t\t* `y.shape[0]` : 100 : do not append to output shape,\n",
        "\t\t\talways ignore first dimension of `y`\n",
        "\t\t* `y.shape[1]` : 30 : append to output shape\n",
        "\t\t* `y.shape[2]` : 20 : do not append to output shape,\n",
        "\t\t\tdimension 2 of `y` has been summed over. (`dot_axes[1]` = 2)\n",
        "\t\t`output_shape` = `(100, 30)`\n",
        "\t```python\n",
        "\t\t>>> x_batch = K.ones(shape=(32, 20, 1))\n",
        "\t\t>>> y_batch = K.ones(shape=(32, 30, 20))\n",
        "\t\t>>> xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=[1, 2])\n",
        "\t\t>>> K.int_shape(xy_batch_dot)\n",
        "\t\t(32, 1, 30)\n",
        "\t```\n",
        "\t\"\"\"\n",
        "\tif isinstance(axes, int):\n",
        "\t\taxes = (axes, axes)\n",
        "\tx_ndim = ndim(x)\n",
        "\ty_ndim = ndim(y)\n",
        "\tif axes is None:\n",
        "\t\t# behaves like tf.batch_matmul as default\n",
        "\t\taxes = [x_ndim - 1, y_ndim - 2]\n",
        "\tif x_ndim > y_ndim:\n",
        "\t\tdiff = x_ndim - y_ndim\n",
        "\t\ty = array_ops.reshape(y,\n",
        "\t\t\t\t\t\t\tarray_ops.concat(\n",
        "\t\t\t\t\t\t\t\t[array_ops.shape(y), [1] * (diff)], axis=0))\n",
        "\telif y_ndim > x_ndim:\n",
        "\t\tdiff = y_ndim - x_ndim\n",
        "\t\tx = array_ops.reshape(x,\n",
        "\t\t\t\t\t\t\tarray_ops.concat(\n",
        "\t\t\t\t\t\t\t\t[array_ops.shape(x), [1] * (diff)], axis=0))\n",
        "\telse:\n",
        "\t\tdiff = 0\n",
        "\tif ndim(x) == 2 and ndim(y) == 2:\n",
        "\t\tif axes[0] == axes[1]:\n",
        "\t\t\tout = math_ops.reduce_sum(math_ops.multiply(x, y), axes[0])\n",
        "\t\telse:\n",
        "\t\t\tout = math_ops.reduce_sum(\n",
        "\t\t\tmath_ops.multiply(array_ops.transpose(x, [1, 0]), y), axes[1])\n",
        "\telse:\n",
        "\t\tadj_x = None if axes[0] == ndim(x) - 1 else True\n",
        "\t\tadj_y = True if axes[1] == ndim(y) - 1 else None\n",
        "\t\tout = math_ops.matmul(x, y, adjoint_a=adj_x, adjoint_b=adj_y)\n",
        "\tif diff:\n",
        "\t\tif x_ndim > y_ndim:\n",
        "\t\t\tidx = x_ndim + y_ndim - 3\n",
        "\t\telse:\n",
        "\t\t\tidx = x_ndim - 1\n",
        "\t\t\tout = array_ops.squeeze(out, list(range(idx, idx + diff)))\n",
        "\tif ndim(out) == 1:\n",
        "\t\tout = expand_dims(out, 1)\n",
        "\treturn out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "V-9Tyf2WKgfe"
      },
      "outputs": [],
      "source": [
        "# load a single file as a numpy array\n",
        "def load_file(filepath):\n",
        "    dataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
        "    return dataframe.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RFtsLcBTKgff"
      },
      "outputs": [],
      "source": [
        "# load a list of files and return as a 3d numpy array\n",
        "def load_group(filenames, prefix=''):\n",
        "    loaded = list()\n",
        "    for name in filenames:\n",
        "        data = load_file(prefix + name)\n",
        "        loaded.append(data)\n",
        "    # stack group so that features are the 3rd dimension\n",
        "    loaded = dstack(loaded)\n",
        "    return loaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IDNdqOsIKgff"
      },
      "outputs": [],
      "source": [
        "# load a dataset group, such as train or test\n",
        "def load_dataset_group(group, prefix=''):\n",
        "    filepath = prefix + group + '/Inertial Signals/'\n",
        "    print('File Path : ',filepath)\n",
        "    # load all 9 files as a single array\n",
        "    filenames = list()\n",
        "    # total acceleration\n",
        "    filenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
        "    # body acceleration\n",
        "    filenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
        "    # body gyroscope\n",
        "    filenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
        "    # load input data\n",
        "    X = load_group(filenames, filepath)\n",
        "    # load class output\n",
        "    y = load_file(prefix + group + '/y_'+group+'.txt')\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pGNunmWKgfg",
        "outputId": "aadd67a0-8440-43af-bf4c-08d9ae17adde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File Path :  UCIDataset/train/Inertial Signals/\n",
            "File Path :  UCIDataset/test/Inertial Signals/\n",
            "X_train.shape :  (7352, 128, 9)\n",
            "Y_train.shape :  (7352, 6)\n",
            "X_test.shape :  (2947, 128, 9)\n",
            "Y_test.shape :  (2947, 6)\n"
          ]
        }
      ],
      "source": [
        "# load all train\n",
        "X_train, Y_train = load_dataset_group('train', 'UCIDataset/')\n",
        "# load all test\n",
        "X_test, Y_test = load_dataset_group('test', 'UCIDataset/')\n",
        "\n",
        "# zero-offset class values\n",
        "Y_train = Y_train - 1\n",
        "Y_test = Y_test - 1\n",
        "# one hot encode y\n",
        "Y_train = to_categorical(Y_train)\n",
        "Y_test = to_categorical(Y_test)\n",
        "\n",
        "print('X_train.shape : ', X_train.shape)\n",
        "print('Y_train.shape : ', Y_train.shape)\n",
        "print('X_test.shape : ', X_test.shape)\n",
        "print('Y_test.shape : ', Y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_zhoqoAJKgfg"
      },
      "outputs": [],
      "source": [
        "verbose = 1\n",
        "epochs = 50\n",
        "batch_size = 128\n",
        "\n",
        "n_timesteps = X_train.shape[1]\n",
        "n_features = X_train.shape[2]\n",
        "n_outputs = Y_train.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SCAD5m6UKgfg"
      },
      "outputs": [],
      "source": [
        "checkpoint = ModelCheckpoint(\"CNN_LSTM_Capsule_weights.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='auto')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRrzk8FTKgfg",
        "outputId": "1cb3c0f7-e6c6-42b2-a862-8760c0b86f11",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensor(\"digit_capsule_layer_4/transpose_1:0\", shape=(None, 6, 128), dtype=float32)\n",
            "Tensor(\"digit_capsule_layer_4/Squeeze:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Tensor(\"digit_capsule_layer_4/mul:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Tensor(\"digit_capsule_layer_4/transpose_3:0\", shape=(None, 6, 128), dtype=float32)\n",
            "Tensor(\"digit_capsule_layer_4/Squeeze_2:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Tensor(\"digit_capsule_layer_4/mul_1:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, None, 32, 9)]     0         \n",
            "                                                                 \n",
            " time_distributed_26 (TimeDi  (None, None, 29, 128)    4736      \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_27 (TimeDi  (None, None, 14, 128)    0         \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_28 (TimeDi  (None, None, 14, 128)    65664     \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_29 (TimeDi  (None, None, 7, 128)     0         \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_30 (TimeDi  (None, None, 896)        0         \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 1024)              7868416   \n",
            "                                                                 \n",
            " reshape_5 (Reshape)         (None, 128, 8)            0         \n",
            "                                                                 \n",
            " lambda_7 (Lambda)           (None, 128, 8)            0         \n",
            "                                                                 \n",
            " digit_capsule_layer_4 (Digi  (None, 6, 16)            98304     \n",
            " tCapsuleLayer)                                                  \n",
            "                                                                 \n",
            " lambda_8 (Lambda)           (None, 6)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,037,120\n",
            "Trainable params: 8,037,120\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "Tensor(\"model/digit_capsule_layer_4/transpose_1:0\", shape=(None, 6, 128), dtype=float32)\n",
            "Tensor(\"model/digit_capsule_layer_4/Squeeze:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Tensor(\"model/digit_capsule_layer_4/mul:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Tensor(\"model/digit_capsule_layer_4/transpose_3:0\", shape=(None, 6, 128), dtype=float32)\n",
            "Tensor(\"model/digit_capsule_layer_4/Squeeze_2:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Tensor(\"model/digit_capsule_layer_4/mul_1:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Tensor(\"model/digit_capsule_layer_4/transpose_1:0\", shape=(None, 6, 128), dtype=float32)\n",
            "Tensor(\"model/digit_capsule_layer_4/Squeeze:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Tensor(\"model/digit_capsule_layer_4/mul:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Tensor(\"model/digit_capsule_layer_4/transpose_3:0\", shape=(None, 6, 128), dtype=float32)\n",
            "Tensor(\"model/digit_capsule_layer_4/Squeeze_2:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Tensor(\"model/digit_capsule_layer_4/mul_1:0\", shape=(None, 6, 16), dtype=float32)\n",
            "58/58 [==============================] - ETA: 0s - loss: 1.4583 - accuracy: 0.2458Tensor(\"model/digit_capsule_layer_4/transpose_1:0\", shape=(None, 6, 128), dtype=float32)\n",
            "Tensor(\"model/digit_capsule_layer_4/Squeeze:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Tensor(\"model/digit_capsule_layer_4/mul:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Tensor(\"model/digit_capsule_layer_4/transpose_3:0\", shape=(None, 6, 128), dtype=float32)\n",
            "Tensor(\"model/digit_capsule_layer_4/Squeeze_2:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Tensor(\"model/digit_capsule_layer_4/mul_1:0\", shape=(None, 6, 16), dtype=float32)\n",
            "\n",
            "Epoch 1: val_accuracy did not improve from 0.45877\n",
            "58/58 [==============================] - 18s 251ms/step - loss: 1.4583 - accuracy: 0.2458 - val_loss: 1.2355 - val_accuracy: 0.3797\n",
            "Epoch 2/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 1.0450 - accuracy: 0.4344\n",
            "Epoch 2: val_accuracy did not improve from 0.45877\n",
            "58/58 [==============================] - 16s 271ms/step - loss: 1.0450 - accuracy: 0.4344 - val_loss: 1.0541 - val_accuracy: 0.4530\n",
            "Epoch 3/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 1.0013 - accuracy: 0.4459\n",
            "Epoch 3: val_accuracy did not improve from 0.45877\n",
            "58/58 [==============================] - 17s 290ms/step - loss: 1.0013 - accuracy: 0.4459 - val_loss: 1.0776 - val_accuracy: 0.4486\n",
            "Epoch 4/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.9986 - accuracy: 0.4456\n",
            "Epoch 4: val_accuracy did not improve from 0.45877\n",
            "58/58 [==============================] - 18s 303ms/step - loss: 0.9986 - accuracy: 0.4456 - val_loss: 1.0688 - val_accuracy: 0.4564\n",
            "Epoch 5/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.9947 - accuracy: 0.4467\n",
            "Epoch 5: val_accuracy did not improve from 0.45877\n",
            "58/58 [==============================] - 17s 289ms/step - loss: 0.9947 - accuracy: 0.4467 - val_loss: 1.0906 - val_accuracy: 0.4496\n",
            "Epoch 6/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.9993 - accuracy: 0.4453\n",
            "Epoch 6: val_accuracy did not improve from 0.45877\n",
            "58/58 [==============================] - 17s 291ms/step - loss: 0.9993 - accuracy: 0.4453 - val_loss: 1.0774 - val_accuracy: 0.4510\n",
            "Epoch 7/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.9943 - accuracy: 0.4505\n",
            "Epoch 7: val_accuracy improved from 0.45877 to 0.62911, saving model to CNN_LSTM_Capsule_weights.h5\n",
            "58/58 [==============================] - 17s 291ms/step - loss: 0.9943 - accuracy: 0.4505 - val_loss: 1.0506 - val_accuracy: 0.6291\n",
            "Epoch 8/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6962 - accuracy: 0.6371\n",
            "Epoch 8: val_accuracy improved from 0.62911 to 0.64506, saving model to CNN_LSTM_Capsule_weights.h5\n",
            "58/58 [==============================] - 17s 288ms/step - loss: 0.6962 - accuracy: 0.6371 - val_loss: 0.6960 - val_accuracy: 0.6451\n",
            "Epoch 9/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6507 - accuracy: 0.6382\n",
            "Epoch 9: val_accuracy did not improve from 0.64506\n",
            "58/58 [==============================] - 16s 281ms/step - loss: 0.6507 - accuracy: 0.6382 - val_loss: 0.7237 - val_accuracy: 0.6352\n",
            "Epoch 10/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6500 - accuracy: 0.6382\n",
            "Epoch 10: val_accuracy did not improve from 0.64506\n",
            "58/58 [==============================] - 16s 281ms/step - loss: 0.6500 - accuracy: 0.6382 - val_loss: 0.7051 - val_accuracy: 0.6410\n",
            "Epoch 11/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6499 - accuracy: 0.6382\n",
            "Epoch 11: val_accuracy did not improve from 0.64506\n",
            "58/58 [==============================] - 17s 295ms/step - loss: 0.6499 - accuracy: 0.6382 - val_loss: 0.7134 - val_accuracy: 0.6410\n",
            "Epoch 12/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6501 - accuracy: 0.6382\n",
            "Epoch 12: val_accuracy did not improve from 0.64506\n",
            "58/58 [==============================] - 19s 330ms/step - loss: 0.6501 - accuracy: 0.6382 - val_loss: 0.7127 - val_accuracy: 0.6413\n",
            "Epoch 13/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6498 - accuracy: 0.6382\n",
            "Epoch 13: val_accuracy did not improve from 0.64506\n",
            "58/58 [==============================] - 19s 328ms/step - loss: 0.6498 - accuracy: 0.6382 - val_loss: 0.7232 - val_accuracy: 0.6366\n",
            "Epoch 14/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6498 - accuracy: 0.6382\n",
            "Epoch 14: val_accuracy did not improve from 0.64506\n",
            "58/58 [==============================] - 21s 371ms/step - loss: 0.6498 - accuracy: 0.6382 - val_loss: 0.7080 - val_accuracy: 0.6400\n",
            "Epoch 15/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6497 - accuracy: 0.6382\n",
            "Epoch 15: val_accuracy did not improve from 0.64506\n",
            "58/58 [==============================] - 20s 342ms/step - loss: 0.6497 - accuracy: 0.6382 - val_loss: 0.7136 - val_accuracy: 0.6403\n",
            "Epoch 16/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6496 - accuracy: 0.6382\n",
            "Epoch 16: val_accuracy did not improve from 0.64506\n",
            "58/58 [==============================] - 18s 319ms/step - loss: 0.6496 - accuracy: 0.6382 - val_loss: 0.7194 - val_accuracy: 0.6396\n",
            "Epoch 17/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6496 - accuracy: 0.6382\n",
            "Epoch 17: val_accuracy did not improve from 0.64506\n",
            "58/58 [==============================] - 18s 304ms/step - loss: 0.6496 - accuracy: 0.6382 - val_loss: 0.7186 - val_accuracy: 0.6396\n",
            "Epoch 18/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6496 - accuracy: 0.6382\n",
            "Epoch 18: val_accuracy did not improve from 0.64506\n",
            "58/58 [==============================] - 18s 305ms/step - loss: 0.6496 - accuracy: 0.6382 - val_loss: 0.7220 - val_accuracy: 0.6390\n",
            "Epoch 19/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6496 - accuracy: 0.6382\n",
            "Epoch 19: val_accuracy did not improve from 0.64506\n",
            "58/58 [==============================] - 17s 285ms/step - loss: 0.6496 - accuracy: 0.6382 - val_loss: 0.7199 - val_accuracy: 0.6400\n",
            "Epoch 20/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6495 - accuracy: 0.6382\n",
            "Epoch 20: val_accuracy did not improve from 0.64506\n",
            "58/58 [==============================] - 17s 287ms/step - loss: 0.6495 - accuracy: 0.6382 - val_loss: 0.7226 - val_accuracy: 0.6390\n",
            "Epoch 21/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6495 - accuracy: 0.6382\n",
            "Epoch 21: val_accuracy did not improve from 0.64506\n",
            "58/58 [==============================] - 18s 312ms/step - loss: 0.6495 - accuracy: 0.6382 - val_loss: 0.7215 - val_accuracy: 0.6400\n",
            "Epoch 22/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6495 - accuracy: 0.6382\n",
            "Epoch 22: val_accuracy did not improve from 0.64506\n",
            "58/58 [==============================] - 17s 294ms/step - loss: 0.6495 - accuracy: 0.6382 - val_loss: 0.7232 - val_accuracy: 0.6400\n",
            "Epoch 23/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6495 - accuracy: 0.6382\n",
            "Epoch 23: val_accuracy did not improve from 0.64506\n",
            "58/58 [==============================] - 17s 293ms/step - loss: 0.6495 - accuracy: 0.6382 - val_loss: 0.7235 - val_accuracy: 0.6396\n",
            "Epoch 24/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6495 - accuracy: 0.6382\n",
            "Epoch 24: val_accuracy did not improve from 0.64506\n",
            "58/58 [==============================] - 16s 279ms/step - loss: 0.6495 - accuracy: 0.6382 - val_loss: 0.7241 - val_accuracy: 0.6400\n",
            "Epoch 25/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6495 - accuracy: 0.6382\n",
            "Epoch 25: val_accuracy did not improve from 0.64506\n",
            "58/58 [==============================] - 16s 272ms/step - loss: 0.6495 - accuracy: 0.6382 - val_loss: 0.7250 - val_accuracy: 0.6396\n",
            "Epoch 26/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6495 - accuracy: 0.6382\n",
            "Epoch 26: val_accuracy did not improve from 0.64506\n",
            "58/58 [==============================] - 16s 282ms/step - loss: 0.6495 - accuracy: 0.6382 - val_loss: 0.7256 - val_accuracy: 0.6393\n",
            "Epoch 27/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6495 - accuracy: 0.6382\n",
            "Epoch 27: val_accuracy did not improve from 0.64506\n",
            "58/58 [==============================] - 16s 273ms/step - loss: 0.6495 - accuracy: 0.6382 - val_loss: 0.7253 - val_accuracy: 0.6400\n",
            "Epoch 28/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6495 - accuracy: 0.6382\n",
            "Epoch 28: val_accuracy did not improve from 0.64506\n",
            "58/58 [==============================] - 16s 272ms/step - loss: 0.6495 - accuracy: 0.6382 - val_loss: 0.7266 - val_accuracy: 0.6393\n",
            "Epoch 29/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6495 - accuracy: 0.6382\n",
            "Epoch 29: val_accuracy did not improve from 0.64506\n",
            "58/58 [==============================] - 16s 275ms/step - loss: 0.6495 - accuracy: 0.6382 - val_loss: 0.7263 - val_accuracy: 0.6396\n",
            "Epoch 30/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6494 - accuracy: 0.6382\n",
            "Epoch 30: val_accuracy did not improve from 0.64506\n",
            "58/58 [==============================] - 16s 272ms/step - loss: 0.6494 - accuracy: 0.6382 - val_loss: 0.7271 - val_accuracy: 0.6390\n",
            "Epoch 31/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6494 - accuracy: 0.6382\n",
            "Epoch 31: val_accuracy did not improve from 0.64506\n",
            "58/58 [==============================] - 16s 274ms/step - loss: 0.6494 - accuracy: 0.6382 - val_loss: 0.7265 - val_accuracy: 0.6396\n",
            "Epoch 32/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6494 - accuracy: 0.6382\n",
            "Epoch 32: val_accuracy did not improve from 0.64506\n",
            "58/58 [==============================] - 16s 273ms/step - loss: 0.6494 - accuracy: 0.6382 - val_loss: 0.7270 - val_accuracy: 0.6393\n",
            "Epoch 33/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6494 - accuracy: 0.6382\n",
            "Epoch 33: val_accuracy did not improve from 0.64506\n",
            "58/58 [==============================] - 16s 277ms/step - loss: 0.6494 - accuracy: 0.6382 - val_loss: 0.7276 - val_accuracy: 0.6390\n",
            "Epoch 34/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6494 - accuracy: 0.6382\n",
            "Epoch 34: val_accuracy did not improve from 0.64506\n",
            "58/58 [==============================] - 16s 276ms/step - loss: 0.6494 - accuracy: 0.6382 - val_loss: 0.7277 - val_accuracy: 0.6393\n",
            "Epoch 35/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6494 - accuracy: 0.6382\n",
            "Epoch 35: val_accuracy did not improve from 0.64506\n",
            "58/58 [==============================] - 16s 275ms/step - loss: 0.6494 - accuracy: 0.6382 - val_loss: 0.7283 - val_accuracy: 0.6390\n",
            "Epoch 36/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6494 - accuracy: 0.6382\n",
            "Epoch 36: val_accuracy did not improve from 0.64506\n",
            "58/58 [==============================] - 16s 274ms/step - loss: 0.6494 - accuracy: 0.6382 - val_loss: 0.7287 - val_accuracy: 0.6386\n",
            "Epoch 37/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6494 - accuracy: 0.6382\n",
            "Epoch 37: val_accuracy did not improve from 0.64506\n",
            "58/58 [==============================] - 16s 277ms/step - loss: 0.6494 - accuracy: 0.6382 - val_loss: 0.7279 - val_accuracy: 0.6393\n",
            "Epoch 38/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6494 - accuracy: 0.6382\n",
            "Epoch 38: val_accuracy did not improve from 0.64506\n",
            "58/58 [==============================] - 16s 274ms/step - loss: 0.6494 - accuracy: 0.6382 - val_loss: 0.7287 - val_accuracy: 0.6396\n",
            "Epoch 39/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6494 - accuracy: 0.6382\n",
            "Epoch 39: val_accuracy did not improve from 0.64506\n",
            "58/58 [==============================] - 16s 275ms/step - loss: 0.6494 - accuracy: 0.6382 - val_loss: 0.7293 - val_accuracy: 0.6383\n",
            "Epoch 40/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6494 - accuracy: 0.6382\n",
            "Epoch 40: val_accuracy did not improve from 0.64506\n",
            "58/58 [==============================] - 16s 273ms/step - loss: 0.6494 - accuracy: 0.6382 - val_loss: 0.7292 - val_accuracy: 0.6386\n",
            "Epoch 41/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6494 - accuracy: 0.6382\n",
            "Epoch 41: val_accuracy did not improve from 0.64506\n",
            "58/58 [==============================] - 16s 277ms/step - loss: 0.6494 - accuracy: 0.6382 - val_loss: 0.7294 - val_accuracy: 0.6379\n",
            "Epoch 42/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6494 - accuracy: 0.6382\n",
            "Epoch 42: val_accuracy did not improve from 0.64506\n",
            "58/58 [==============================] - 16s 274ms/step - loss: 0.6494 - accuracy: 0.6382 - val_loss: 0.7298 - val_accuracy: 0.6373\n",
            "Epoch 43/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6494 - accuracy: 0.6382\n",
            "Epoch 43: val_accuracy did not improve from 0.64506\n",
            "58/58 [==============================] - 16s 282ms/step - loss: 0.6494 - accuracy: 0.6382 - val_loss: 0.7307 - val_accuracy: 0.6369\n",
            "Epoch 44/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6494 - accuracy: 0.6382\n",
            "Epoch 44: val_accuracy did not improve from 0.64506\n",
            "58/58 [==============================] - 17s 286ms/step - loss: 0.6494 - accuracy: 0.6382 - val_loss: 0.7304 - val_accuracy: 0.6376\n",
            "Epoch 45/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6494 - accuracy: 0.6382\n",
            "Epoch 45: val_accuracy did not improve from 0.64506\n",
            "58/58 [==============================] - 16s 275ms/step - loss: 0.6494 - accuracy: 0.6382 - val_loss: 0.7310 - val_accuracy: 0.6369\n",
            "Epoch 46/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6494 - accuracy: 0.6382\n",
            "Epoch 46: val_accuracy did not improve from 0.64506\n",
            "58/58 [==============================] - 16s 274ms/step - loss: 0.6494 - accuracy: 0.6382 - val_loss: 0.7301 - val_accuracy: 0.6362\n",
            "Epoch 47/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6494 - accuracy: 0.6382\n",
            "Epoch 47: val_accuracy did not improve from 0.64506\n",
            "58/58 [==============================] - 16s 274ms/step - loss: 0.6494 - accuracy: 0.6382 - val_loss: 0.7331 - val_accuracy: 0.6362\n",
            "Epoch 48/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6494 - accuracy: 0.6382\n",
            "Epoch 48: val_accuracy did not improve from 0.64506\n",
            "58/58 [==============================] - 16s 276ms/step - loss: 0.6494 - accuracy: 0.6382 - val_loss: 0.7302 - val_accuracy: 0.6376\n",
            "Epoch 49/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.7678 - accuracy: 0.5983\n",
            "Epoch 49: val_accuracy did not improve from 0.64506\n",
            "58/58 [==============================] - 16s 275ms/step - loss: 0.7678 - accuracy: 0.5983 - val_loss: 1.3195 - val_accuracy: 0.4350\n",
            "Epoch 50/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.6982 - accuracy: 0.6371\n",
            "Epoch 50: val_accuracy improved from 0.64506 to 0.70920, saving model to CNN_LSTM_Capsule_weights.h5\n",
            "58/58 [==============================] - 19s 330ms/step - loss: 0.6982 - accuracy: 0.6371 - val_loss: 0.6174 - val_accuracy: 0.7092\n"
          ]
        }
      ],
      "source": [
        "n_steps = 4\n",
        "n_length = 32\n",
        "X_train = X_train.reshape((X_train.shape[0], n_steps, n_length, n_features))\n",
        "X_test = X_test.reshape((X_test.shape[0], n_steps, n_length, n_features))\n",
        "\n",
        "inputs = layers.Input(shape=(None,32,9))\n",
        "conv1 = TimeDistributed(Conv1D(filters=128, kernel_size=4, activation='relu', input_shape=(None,n_length,n_features)))(inputs)\n",
        "maxpool1 = TimeDistributed(MaxPooling1D(pool_size=2))(conv1)\n",
        "conv2 = TimeDistributed(Conv1D(filters=128, kernel_size=4, activation='relu', padding = 'same'))(maxpool1)\n",
        "maxpool2 = TimeDistributed(MaxPooling1D(pool_size=2))(conv2)\n",
        "flatten1 = TimeDistributed(Flatten())(maxpool2)\n",
        "lstm_main = LSTM(128*8)(flatten1)\n",
        "reshaped = Reshape((128,8))(lstm_main)\n",
        "\n",
        "def squash(inputs):\n",
        "    # take norm of input vectors\n",
        "    squared_norm = K.sum(K.square(inputs), axis = -1, keepdims = True)\n",
        "    # use the formula for non-linear function to return squashed output\n",
        "    return ((squared_norm/(1+squared_norm))/(K.sqrt(squared_norm+K.epsilon())))*inputs\n",
        "\n",
        "# squash the reshaped output to make length of vector b/w 0 and 1\n",
        "squashed_output = Lambda(squash)(reshaped)\n",
        "\n",
        "class DigitCapsuleLayer(Layer):\n",
        "    # creating a layer class in keras\n",
        "    def __init__(self, **kwargs):\n",
        "        super(DigitCapsuleLayer, self).__init__(**kwargs)\n",
        "        self.kernel_initializer = initializers.get('glorot_uniform')\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # initialize weight matrix for each capsule in lower layer\n",
        "        self.W = self.add_weight(shape = [6, 128, 16, 8], initializer = self.kernel_initializer, name = 'weights')\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inputs = K.expand_dims(inputs, 1)\n",
        "        inputs = K.tile(inputs, [1, 6, 1, 1])\n",
        "        # matrix multiplication b/w previous layer output and weight matrix\n",
        "        inputs = K.map_fn(lambda x: own_batch_dot(x, self.W, [2, 3]), elems=inputs)\n",
        "        b = tf.zeros(shape = [K.shape(inputs)[0], 6, 128])\n",
        "\n",
        "\t\t# routing algorithm with updating coupling coefficient c, using scalar product b/w input capsule and output capsule\n",
        "        for i in range(3-1):\n",
        "            # print(b)\n",
        "            c = tf.nn.softmax(b, axis=1)\n",
        "            print(c)\n",
        "            s = own_batch_dot(c, inputs, [2, 2])\n",
        "            print(s)\n",
        "            v = squash(s)\n",
        "            print(v)\n",
        "            b = b + own_batch_dot(v, inputs, [2,3])\n",
        "            \n",
        "        return v\n",
        "    \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return tuple([None, 6, 16])\n",
        "\n",
        "def output_layer(inputs):\n",
        "    return K.sqrt(K.sum(K.square(inputs), -1) + K.epsilon())\n",
        "\n",
        "digit_caps = DigitCapsuleLayer()(squashed_output)\n",
        "outputs = Lambda(output_layer)(digit_caps)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test),\n",
        "                    epochs=epochs, batch_size=batch_size, verbose=verbose, callbacks=[checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "SXzSMgFbKgfh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Validation Accuracy: 0.7091957926750183\n"
          ]
        }
      ],
      "source": [
        "# Maximum validation Accuracy\n",
        "val_accuracy_tensor = history.history['val_accuracy']\n",
        "best_val_accuracy = np.max(val_accuracy_tensor)\n",
        "print(\"Best Validation Accuracy:\", best_val_accuracy)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
