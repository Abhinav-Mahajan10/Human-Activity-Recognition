{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9ECDQJJyKgfc"
      },
      "outputs": [],
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import dstack\n",
        "from pandas import read_csv\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from matplotlib import pyplot as plt\n",
        "import keras\n",
        "from keras import initializers\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras.backend import *\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import Dense, Layer, Lambda, Input, Flatten, Dropout, Conv2D, MaxPooling2D, Conv1D, MaxPooling1D, LSTM, TimeDistributed, ConvLSTM2D, Permute, Reshape, Conv2D\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.python.ops import array_ops\n",
        "from tensorflow.python.ops import math_ops\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping\n",
        "\n",
        "def own_batch_dot(x, y, axes=None):\n",
        "\t\"\"\"Batchwise dot product.\n",
        "\t`batch_dot` is used to compute dot product of `x` and `y` when\n",
        "\t`x` and `y` are data in batch, i.e. in a shape of\n",
        "\t`(batch_size, :)`.\n",
        "\t`batch_dot` results in a tensor or variable with less dimensions\n",
        "\tthan the input. If the number of dimensions is reduced to 1,\n",
        "\twe use `expand_dims` to make sure that ndim is at least 2.\n",
        "\tArguments:\n",
        "\t\tx: Keras tensor or variable with `ndim >= 2`.\n",
        "\t\ty: Keras tensor or variable with `ndim >= 2`.\n",
        "\t\taxes: list of (or single) int with target dimensions.\n",
        "\t\t\tThe lengths of `axes[0]` and `axes[1]` should be the same.\n",
        "\tReturns:\n",
        "\t\tA tensor with shape equal to the concatenation of `x`'s shape\n",
        "\t\t(less the dimension that was summed over) and `y`'s shape\n",
        "\t\t(less the batch dimension and the dimension that was summed over).\n",
        "\t\tIf the final rank is 1, we reshape it to `(batch_size, 1)`.\n",
        "\tExamples:\n",
        "\t\tAssume `x = [[1, 2], [3, 4]]` and `y = [[5, 6], [7, 8]]`\n",
        "\t\t`batch_dot(x, y, axes=1) = [[17, 53]]` which is the main diagonal\n",
        "\t\tof `x.dot(y.T)`, although we never have to calculate the off-diagonal\n",
        "\t\telements.\n",
        "\t\tShape inference:\n",
        "\t\tLet `x`'s shape be `(100, 20)` and `y`'s shape be `(100, 30, 20)`.\n",
        "\t\tIf `axes` is (1, 2), to find the output shape of resultant tensor,\n",
        "\t\t\tloop through each dimension in `x`'s shape and `y`'s shape:\n",
        "\t\t* `x.shape[0]` : 100 : append to output shape\n",
        "\t\t* `x.shape[1]` : 20 : do not append to output shape,\n",
        "\t\t\tdimension 1 of `x` has been summed over. (`dot_axes[0]` = 1)\n",
        "\t\t* `y.shape[0]` : 100 : do not append to output shape,\n",
        "\t\t\talways ignore first dimension of `y`\n",
        "\t\t* `y.shape[1]` : 30 : append to output shape\n",
        "\t\t* `y.shape[2]` : 20 : do not append to output shape,\n",
        "\t\t\tdimension 2 of `y` has been summed over. (`dot_axes[1]` = 2)\n",
        "\t\t`output_shape` = `(100, 30)`\n",
        "\t```python\n",
        "\t\t>>> x_batch = K.ones(shape=(32, 20, 1))\n",
        "\t\t>>> y_batch = K.ones(shape=(32, 30, 20))\n",
        "\t\t>>> xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=[1, 2])\n",
        "\t\t>>> K.int_shape(xy_batch_dot)\n",
        "\t\t(32, 1, 30)\n",
        "\t```\n",
        "\t\"\"\"\n",
        "\tif isinstance(axes, int):\n",
        "\t\taxes = (axes, axes)\n",
        "\tx_ndim = ndim(x)\n",
        "\ty_ndim = ndim(y)\n",
        "\tif axes is None:\n",
        "\t\t# behaves like tf.batch_matmul as default\n",
        "\t\taxes = [x_ndim - 1, y_ndim - 2]\n",
        "\tif x_ndim > y_ndim:\n",
        "\t\tdiff = x_ndim - y_ndim\n",
        "\t\ty = array_ops.reshape(y,\n",
        "\t\t\t\t\t\t\tarray_ops.concat(\n",
        "\t\t\t\t\t\t\t\t[array_ops.shape(y), [1] * (diff)], axis=0))\n",
        "\telif y_ndim > x_ndim:\n",
        "\t\tdiff = y_ndim - x_ndim\n",
        "\t\tx = array_ops.reshape(x,\n",
        "\t\t\t\t\t\t\tarray_ops.concat(\n",
        "\t\t\t\t\t\t\t\t[array_ops.shape(x), [1] * (diff)], axis=0))\n",
        "\telse:\n",
        "\t\tdiff = 0\n",
        "\tif ndim(x) == 2 and ndim(y) == 2:\n",
        "\t\tif axes[0] == axes[1]:\n",
        "\t\t\tout = math_ops.reduce_sum(math_ops.multiply(x, y), axes[0])\n",
        "\t\telse:\n",
        "\t\t\tout = math_ops.reduce_sum(\n",
        "\t\t\tmath_ops.multiply(array_ops.transpose(x, [1, 0]), y), axes[1])\n",
        "\telse:\n",
        "\t\tadj_x = None if axes[0] == ndim(x) - 1 else True\n",
        "\t\tadj_y = True if axes[1] == ndim(y) - 1 else None\n",
        "\t\tout = math_ops.matmul(x, y, adjoint_a=adj_x, adjoint_b=adj_y)\n",
        "\tif diff:\n",
        "\t\tif x_ndim > y_ndim:\n",
        "\t\t\tidx = x_ndim + y_ndim - 3\n",
        "\t\telse:\n",
        "\t\t\tidx = x_ndim - 1\n",
        "\t\t\tout = array_ops.squeeze(out, list(range(idx, idx + diff)))\n",
        "\tif ndim(out) == 1:\n",
        "\t\tout = expand_dims(out, 1)\n",
        "\treturn out\n",
        "\n",
        "randomSeed = 1\n",
        "\n",
        "class DropPath(layers.Layer):\n",
        "    def __init__(self, drop_prob=0.0, **kwargs):\n",
        "        super(DropPath, self).__init__(**kwargs)\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def call(self, x,training=None):\n",
        "        if(training):\n",
        "            input_shape = tf.shape(x)\n",
        "            batch_size = input_shape[0]\n",
        "            rank = x.shape.rank\n",
        "            shape = (batch_size,) + (1,) * (rank - 1)\n",
        "            random_tensor = (1 - self.drop_prob) + tf.random.uniform(shape, dtype=x.dtype)\n",
        "            path_mask = tf.floor(random_tensor)\n",
        "            output = tf.math.divide(x, 1 - self.drop_prob) * path_mask\n",
        "            return output\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'drop_prob': self.drop_prob,})\n",
        "        return config\n",
        "\n",
        "class GatedLinearUnit(layers.Layer):\n",
        "    def __init__(self,units,**kwargs):\n",
        "        super(GatedLinearUnit, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.linear = layers.Dense(units * 2)\n",
        "        self.sigmoid = tf.keras.activations.sigmoid\n",
        "    def call(self, inputs):\n",
        "        linearProjection = self.linear(inputs)\n",
        "        softMaxProjection = self.sigmoid(linearProjection[:,:,self.units:])\n",
        "        return tf.multiply(linearProjection[:,:,:self.units],softMaxProjection)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'units': self.units,})\n",
        "        return config\n",
        "\n",
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim,**kwargs):\n",
        "        super(PatchEncoder, self).__init__(**kwargs)\n",
        "        self.num_patches = num_patches\n",
        "        self.projection_dim = projection_dim\n",
        "        self.position_embedding = layers.Embedding(input_dim=num_patches, output_dim=projection_dim)\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = patch + self.position_embedding(positions)\n",
        "        return encoded\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'num_patches': self.num_patches,\n",
        "            'projection_dim': self.projection_dim,})\n",
        "        return config\n",
        "\n",
        "class ClassToken(layers.Layer):\n",
        "    def __init__(self, hidden_size,**kwargs):\n",
        "        super(ClassToken, self).__init__(**kwargs)\n",
        "        self.cls_init = tf.random.normal\n",
        "        self.hidden_size = hidden_size\n",
        "        self.cls = tf.Variable(\n",
        "            name=\"cls\",\n",
        "            initial_value=self.cls_init(shape=(1, 1, self.hidden_size), seed=randomSeed, dtype=\"float32\"),\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        cls_broadcasted = tf.cast(\n",
        "            tf.broadcast_to(self.cls, [batch_size, 1, self.hidden_size]),\n",
        "            dtype=inputs.dtype,\n",
        "        )\n",
        "        return tf.concat([cls_broadcasted, inputs], 1)\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'hidden_size': self.hidden_size,})\n",
        "        return config\n",
        "\n",
        "class Prompts(layers.Layer):\n",
        "    def __init__(self, projectionDims,promptCount = 1,**kwargs):\n",
        "        super(Prompts, self).__init__(**kwargs)\n",
        "        self.cls_init = tf.random.normal\n",
        "        self.projectionDims = projectionDims\n",
        "        self.promptCount = promptCount\n",
        "        self.prompts = [tf.Variable(\n",
        "            name=\"prompt\"+str(_),\n",
        "            initial_value=self.cls_init(shape=(1, 1, self.projectionDims), seed=randomSeed, dtype=\"float32\"),\n",
        "            trainable=True,\n",
        "        )  for _ in range(promptCount)]\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        prompt_broadcasted = tf.concat([tf.cast(tf.broadcast_to(promptInits, [batch_size, 1, self.projectionDims]),dtype=inputs.dtype,)for promptInits in self.prompts],1)\n",
        "        return tf.concat([inputs,prompt_broadcasted], 1)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'projectionDims': self.projectionDims,\n",
        "            'promptCount': self.promptCount,})\n",
        "        return config\n",
        "\n",
        "class SensorWiseMHA(layers.Layer):\n",
        "    def __init__(self, projectionQuarter, num_heads,startIndex,stopIndex,dropout_rate = 0.0,dropPathRate = 0.0, **kwargs):\n",
        "        super(SensorWiseMHA, self).__init__(**kwargs)\n",
        "        self.projectionQuarter = projectionQuarter\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.MHA = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.projectionQuarter, dropout = dropout_rate )\n",
        "        self.startIndex = startIndex\n",
        "        self.stopIndex = stopIndex\n",
        "        self.dropPathRate = dropPathRate\n",
        "        self.DropPath = DropPath(dropPathRate)\n",
        "    def call(self, inputData, training=None, return_attention_scores = False):\n",
        "        extractedInput = inputData[:,:,self.startIndex:self.stopIndex]\n",
        "        if(return_attention_scores):\n",
        "            MHA_Outputs, attentionScores = self.MHA(extractedInput,extractedInput,return_attention_scores = True )\n",
        "            return MHA_Outputs , attentionScores\n",
        "        else:\n",
        "            MHA_Outputs = self.MHA(extractedInput,extractedInput)\n",
        "            MHA_Outputs = self.DropPath(MHA_Outputs)\n",
        "            return MHA_Outputs\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'projectionQuarter': self.projectionQuarter,\n",
        "            'num_heads': self.num_heads,\n",
        "            'startIndex': self.startIndex,\n",
        "            'dropout_rate': self.dropout_rate,\n",
        "            'stopIndex': self.stopIndex,\n",
        "            'dropPathRate': self.dropPathRate,})\n",
        "        return config\n",
        "def softDepthConv(inputs):\n",
        "    kernel = inputs[0]\n",
        "    inputData = inputs[1]\n",
        "    convOutputs = tf.nn.conv1d(\n",
        "    inputData,\n",
        "    kernel,\n",
        "    stride = 1,\n",
        "    padding = 'SAME',\n",
        "    data_format='NCW',)\n",
        "    return convOutputs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class liteFormer(layers.Layer):\n",
        "    def __init__(self,startIndex,stopIndex, projectionSize, kernelSize = 16, attentionHead = 3, use_bias=False, dropPathRate = 0.0,dropout_rate = 0,**kwargs):\n",
        "        super(liteFormer, self).__init__(**kwargs)\n",
        "        self.use_bias = use_bias\n",
        "        self.startIndex = startIndex\n",
        "        self.stopIndex = stopIndex\n",
        "        self.kernelSize = kernelSize\n",
        "        self.softmax = tf.nn.softmax\n",
        "        self.projectionSize = projectionSize\n",
        "        self.attentionHead = attentionHead\n",
        "        self.DropPathLayer = DropPath(dropPathRate)\n",
        "        self.projectionHalf = projectionSize // 2\n",
        "    def build(self,inputShape):\n",
        "        self.depthwise_kernel = [self.add_weight(\n",
        "            shape=(self.kernelSize,1,1),\n",
        "            initializer=\"glorot_uniform\",\n",
        "            trainable=True,\n",
        "            name=\"convWeights\"+str(_),\n",
        "            dtype=\"float32\") for _ in range(self.attentionHead)]\n",
        "        if self.use_bias:\n",
        "            self.convBias = self.add_weight(\n",
        "                shape=(self.attentionHead,),\n",
        "                initializer=\"glorot_uniform\",\n",
        "                trainable=True,\n",
        "                name=\"biasWeights\",\n",
        "                dtype=\"float32\"\n",
        "            )\n",
        "\n",
        "    def call(self, inputs,training=None):\n",
        "        formattedInputs = inputs[:,:,self.startIndex:self.stopIndex]\n",
        "        inputShape = tf.shape(formattedInputs)\n",
        "        reshapedInputs = tf.reshape(formattedInputs,(-1,self.attentionHead,inputShape[1]))\n",
        "        if(training):\n",
        "            for convIndex in range(self.attentionHead):\n",
        "                self.depthwise_kernel[convIndex].assign(self.softmax(self.depthwise_kernel[convIndex], axis=0))\n",
        "        convOutputs = tf.convert_to_tensor([tf.nn.conv1d(\n",
        "            reshapedInputs[:,convIndex:convIndex+1,:],\n",
        "            self.depthwise_kernel[convIndex],\n",
        "            stride = 1,\n",
        "            padding = 'SAME',\n",
        "            data_format='NCW',) for convIndex in range(self.attentionHead) ])\n",
        "        convOutputsDropPath = self.DropPathLayer(convOutputs)\n",
        "        localAttention = tf.reshape(convOutputsDropPath,(-1,inputShape[1],self.projectionSize))\n",
        "        return localAttention\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'use_bias': self.use_bias,\n",
        "            'kernelSize': self.kernelSize,\n",
        "            'startIndex': self.startIndex,\n",
        "            'stopIndex': self.stopIndex,\n",
        "            'projectionSize': self.projectionSize,\n",
        "            'attentionHead': self.attentionHead,})\n",
        "        return config\n",
        "\n",
        "class mixAccGyro(layers.Layer):\n",
        "    def __init__(self,projectionQuarter,projectionHalf,projection_dim,**kwargs):\n",
        "        super(mixAccGyro, self).__init__(**kwargs)\n",
        "        self.projectionQuarter = projectionQuarter\n",
        "        self.projectionHalf = projectionHalf\n",
        "        self.projection_dim = projection_dim\n",
        "        self.projectionThreeFourth = self.projectionHalf+self.projectionQuarter\n",
        "        self.mixedAccGyroIndex = tf.reshape(tf.transpose(tf.stack(\n",
        "            [np.arange(projectionQuarter,projectionHalf), np.arange(projectionHalf,projectionHalf + projectionQuarter)])),[-1])\n",
        "        self.newArrangement = tf.concat((np.arange(0,projectionQuarter),self.mixedAccGyroIndex,np.arange(self.projectionThreeFourth,projection_dim)),axis = 0)\n",
        "    def call(self, inputs):\n",
        "        return tf.gather(inputs,self.newArrangement,axis= 2)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'projectionQuarter': self.projectionQuarter,\n",
        "            'projectionHalf': self.projectionHalf,\n",
        "            'projection_dim': self.projection_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.swish)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x\n",
        "\n",
        "def mlp2(x, hidden_units, dropout_rate):\n",
        "    x = layers.Dense(hidden_units[0],activation=tf.nn.swish)(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "    x = layers.Dense(hidden_units[1])(x)\n",
        "    return x\n",
        "\n",
        "def depthMLP(x, hidden_units, dropout_rate):\n",
        "    x = layers.Dense(hidden_units[0])(x)\n",
        "    x = layers.DepthwiseConv1D(3,data_format='channels_first',activation=tf.nn.swish)(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "    x = layers.Dense(hidden_units[1])(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "    return x\n",
        "\n",
        "class SensorPatchesTimeDistributed(layers.Layer):\n",
        "    def __init__(self, projection_dim,filterCount,patchCount,frameSize = 128, channelsCount = 6,**kwargs):\n",
        "        super(SensorPatchesTimeDistributed, self).__init__(**kwargs)\n",
        "        self.projection_dim = projection_dim\n",
        "        self.frameSize = frameSize\n",
        "        self.channelsCount = channelsCount\n",
        "        self.patchCount = patchCount\n",
        "        self.filterCount = filterCount\n",
        "        self.reshapeInputs = layers.Reshape((patchCount, frameSize // patchCount, channelsCount))\n",
        "        self.kernelSize = (projection_dim//2 + filterCount) // filterCount\n",
        "        self.accProjection = layers.TimeDistributed(layers.Conv1D(filters = filterCount,kernel_size = self.kernelSize,strides = 1, data_format = \"channels_last\"))\n",
        "        self.gyroProjection = layers.TimeDistributed(layers.Conv1D(filters = filterCount,kernel_size = self.kernelSize,strides = 1, data_format = \"channels_last\"))\n",
        "        self.flattenTime = layers.TimeDistributed(layers.Flatten())\n",
        "        assert (projection_dim//2 + filterCount) / filterCount % self.kernelSize == 0\n",
        "        print(\"Kernel Size is \"+str((projection_dim//2 + filterCount) / filterCount))\n",
        "#         assert\n",
        "    def call(self, inputData):\n",
        "        inputData = self.reshapeInputs(inputData)\n",
        "        accProjections = self.flattenTime(self.accProjection(inputData[:,:,:,:3]))\n",
        "        gyroProjections = self.flattenTime(self.gyroProjection(inputData[:,:,:,3:]))\n",
        "        Projections = tf.concat((accProjections,gyroProjections),axis=2)\n",
        "        return Projections\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'projection_dim': self.projection_dim,\n",
        "            'filterCount': self.filterCount,\n",
        "            'patchCount': self.patchCount,\n",
        "            'frameSize': self.frameSize,\n",
        "            'channelsCount': self.channelsCount,})\n",
        "        return config\n",
        "\n",
        "class SensorPatches(layers.Layer):\n",
        "    def __init__(self, projection_dim, patchSize,timeStep, **kwargs):\n",
        "        super(SensorPatches, self).__init__(**kwargs)\n",
        "        self.patchSize = patchSize\n",
        "        self.timeStep = timeStep\n",
        "        self.projection_dim = projection_dim\n",
        "        self.accProjection = layers.Conv1D(filters = int(projection_dim/3),kernel_size = patchSize,strides = timeStep, data_format = \"channels_last\")\n",
        "        self.gyroProjection = layers.Conv1D(filters = int(projection_dim/3),kernel_size = patchSize,strides = timeStep, data_format = \"channels_last\")\n",
        "        self.totalaccProjection = layers.Conv1D(filters = int(projection_dim/3),kernel_size = patchSize,strides = timeStep, data_format = \"channels_last\")\n",
        "    def call(self, inputData):\n",
        "\n",
        "        accProjections = self.accProjection(inputData[:,:,:3])\n",
        "        gyroProjections = self.gyroProjection(inputData[:,:,3:6])\n",
        "        totalaccProjection = self.totalaccProjection(inputData[:,:,6:9])\n",
        "        Projections = tf.concat((accProjections,gyroProjections,totalaccProjection),axis=2)\n",
        "        return Projections\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'patchSize': self.patchSize,\n",
        "            'projection_dim': self.projection_dim,\n",
        "            'timeStep': self.timeStep,})\n",
        "        return config\n",
        "\n",
        "\n",
        "class threeSensorPatches(layers.Layer):\n",
        "    def __init__(self, projection_dim, patchSize,timeStep, **kwargs):\n",
        "        super(threeSensorPatches, self).__init__(**kwargs)\n",
        "        self.patchSize = patchSize\n",
        "        self.timeStep = timeStep\n",
        "        self.projection_dim = projection_dim\n",
        "        self.accProjection = layers.Conv1D(filters = int(projection_dim//3),kernel_size = patchSize,strides = timeStep, data_format = \"channels_last\")\n",
        "        self.gyroProjection = layers.Conv1D(filters = int(projection_dim//3),kernel_size = patchSize,strides = timeStep, data_format = \"channels_last\")\n",
        "        self.magProjection = layers.Conv1D(filters = int(projection_dim//3),kernel_size = patchSize,strides = timeStep, data_format = \"channels_last\")\n",
        "\n",
        "    def call(self, inputData):\n",
        "\n",
        "        accProjections = self.accProjection(inputData[:,:,:3])\n",
        "        gyroProjections = self.gyroProjection(inputData[:,:,3:6])\n",
        "        magProjections = self.magProjection(inputData[:,:,6:])\n",
        "\n",
        "        Projections = tf.concat((accProjections,gyroProjections,magProjections),axis=2)\n",
        "        return Projections\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'patchSize': self.patchSize,\n",
        "            'projection_dim': self.projection_dim,\n",
        "            'timeStep': self.timeStep,})\n",
        "        return config\n",
        "\n",
        "\n",
        "class fourSensorPatches(layers.Layer):\n",
        "    def __init__(self, projection_dim, patchSize,timeStep, **kwargs):\n",
        "        super(fourSensorPatches, self).__init__(**kwargs)\n",
        "        self.patchSize = patchSize\n",
        "        self.timeStep = timeStep\n",
        "        self.projection_dim = projection_dim\n",
        "        self.accProjection = layers.Conv1D(filters = int(projection_dim/4),kernel_size = patchSize,strides = timeStep, data_format = \"channels_last\")\n",
        "        self.gyroProjection = layers.Conv1D(filters = int(projection_dim/4),kernel_size = patchSize,strides = timeStep, data_format = \"channels_last\")\n",
        "        self.magProjection = layers.Conv1D(filters = int(projection_dim/4),kernel_size = patchSize,strides = timeStep, data_format = \"channels_last\")\n",
        "        self.altProjection = layers.Conv1D(filters = int(projection_dim/4),kernel_size = patchSize,strides = timeStep, data_format = \"channels_last\")\n",
        "\n",
        "    def call(self, inputData):\n",
        "\n",
        "        accProjections = self.accProjection(inputData[:,:,:3])\n",
        "        gyroProjections = self.gyroProjection(inputData[:,:,3:6])\n",
        "        magProjection = self.magProjection(inputData[:,:,6:9])\n",
        "        altProjection = self.altProjection(inputData[:,:,9:])\n",
        "\n",
        "        Projections = tf.concat((accProjections,gyroProjections,magProjection,altProjection),axis=2)\n",
        "        return Projections\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'patchSize': self.patchSize,\n",
        "            'projection_dim': self.projection_dim,\n",
        "            'timeStep': self.timeStep,})\n",
        "        return config\n",
        "\n",
        "def extract_intermediate_model_from_base_model(base_model, intermediate_layer=4):\n",
        "    model = tf.keras.Model(inputs=base_model.inputs, outputs=base_model.layers[intermediate_layer].output, name=base_model.name + \"_layer_\" + str(intermediate_layer))\n",
        "    return model\n",
        "\n",
        "def HART(input_shape,activityCount, projection_dim = 192,patchSize = 16,timeStep = 16,num_heads = 3,filterAttentionHead = 4, convKernels = [3, 7, 15, 31, 31, 31], mlp_head_units = [1024],dropout_rate = 0.3,useTokens = False):\n",
        "    projectionHalf = projection_dim//2\n",
        "    projectionQuarter = projection_dim//4\n",
        "    projectionThird = projection_dim//3\n",
        "    projectionSixth = projection_dim//6\n",
        "    projectionFiveSixth = projection_dim*5//6\n",
        "    projectionTwoThird = projection_dim*2//3\n",
        "    dropPathRate = np.linspace(0, dropout_rate* 10, len(convKernels)) * 0.1\n",
        "    transformer_units = [\n",
        "    projection_dim * 2,\n",
        "    projection_dim,]\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # print(input)\n",
        "    try:\n",
        "        patches = SensorPatches(projection_dim,patchSize,timeStep)(inputs)\n",
        "    except:\n",
        "        print(\"Sensor patches\")\n",
        "    # print(\"No problem with Sensor Patches\")\n",
        "    if(useTokens):\n",
        "        try:\n",
        "            patches = ClassToken(projection_dim)(patches)\n",
        "        except:\n",
        "            print(\"ClassToken\")\n",
        "    # print(\"No problem with Class Token\")\n",
        "    patchCount = patches.shape[1]\n",
        "    try:\n",
        "        encoded_patches = PatchEncoder(patchCount, projection_dim)(patches)\n",
        "    except:\n",
        "        print(\"Problem with Patch Encoder\")\n",
        "    # print(\"No problem with Patch Encoder\")\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for layerIndex, kernelLength in enumerate(convKernels):\n",
        "        try:\n",
        "            x1 = layers.LayerNormalization(epsilon=1e-6 , name = \"normalizedInputs_\"+str(layerIndex))(encoded_patches)\n",
        "            x1_a1 = x1[:,:,:projectionSixth]\n",
        "            x1_a2 = x1[:,:,projectionSixth:projectionThird]\n",
        "            x1_b1 = x1[:,:,projectionThird:projectionHalf]\n",
        "            x1_b2 = x1[:,:,projectionHalf:projectionTwoThird]\n",
        "            x1_c1 = x1[:,:,projectionTwoThird:projectionFiveSixth]\n",
        "            x1_c2 = x1[:,:,projectionFiveSixth:]\n",
        "            x1_multihead = tf.concat([x1_a1, x1_b1, x1_c1], axis=-1)\n",
        "            x1_liteformer = tf.concat([x1_a2, x1_b2, x1_c2], axis=-1)\n",
        "\n",
        "        except:\n",
        "            print(\"Problem with LayerNormalization\")\n",
        "        # print(\"No problem with LayerNormalisation\")\n",
        "        try:\n",
        "            branch1 = liteFormer(\n",
        "                            startIndex = 0,\n",
        "                            stopIndex = projectionHalf,\n",
        "                            projectionSize = projectionHalf,\n",
        "                            attentionHead =  filterAttentionHead,\n",
        "                            kernelSize = kernelLength,\n",
        "                            dropPathRate = dropPathRate[layerIndex],\n",
        "                            dropout_rate = dropout_rate,\n",
        "                            name = \"liteFormer_\"+str(layerIndex))(x1_liteformer)\n",
        "        except:\n",
        "            print(\"Problem with Lite Transformer\")\n",
        "        # print(\"No problem with Lite Tranformer\")\n",
        "\n",
        "        try:\n",
        "            branch2Acc = SensorWiseMHA(projectionSixth,num_heads,0,projectionSixth,dropPathRate = dropPathRate[layerIndex],dropout_rate = dropout_rate,name = \"AccMHA_\"+str(layerIndex))(x1_multihead)\n",
        "        except:\n",
        "            print(\"Problem with Acceleration Barnch\")\n",
        "        # print(\"No problem with Acceleration branch\")\n",
        "        try:\n",
        "            branch2Gyro = SensorWiseMHA(projectionSixth,num_heads,projectionSixth,projectionThird,dropPathRate = dropPathRate[layerIndex],dropout_rate = dropout_rate, name = \"GyroMHA_\"+str(layerIndex))(x1_multihead)\n",
        "        except:\n",
        "            print(\"Problem with Gyro Branch\")\n",
        "        # print(\"No problem with Gyro branch\")\n",
        "        try:\n",
        "            branch2TotalAcc = SensorWiseMHA(projectionSixth,num_heads,projectionThird,projectionHalf,dropPathRate = dropPathRate[layerIndex],dropout_rate = dropout_rate,name = \"TotalAccMHA_\"+str(layerIndex))(x1_multihead)\n",
        "        except:\n",
        "            print(\"Problem with Total Acceleration Branch\")\n",
        "        # print(\"No problem with Total Acceleration branch\")\n",
        "        concatAttention = tf.concat((branch2Acc,branch1,branch2Gyro,branch2TotalAcc),axis= 2)\n",
        "        # print(tf.size(concatAttention))\n",
        "        # print(tf.size(encoded_patches))\n",
        "        # try:\n",
        "        x2 = layers.Add()([concatAttention, encoded_patches])\n",
        "        # except:\n",
        "        #     print(\"Problem with concatAttention and encoded patches\")\n",
        "        # print(\"No problem with concatAttention and encoded patches\")\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        x3 = mlp2(x3, hidden_units=transformer_units, dropout_rate=dropout_rate)\n",
        "        x3 = DropPath(dropPathRate[layerIndex])(x3)\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    if(useTokens):\n",
        "        representation = layers.Lambda(lambda v: v[:, 0], name=\"ExtractToken\")(representation)\n",
        "    else:\n",
        "        representation = layers.GlobalAveragePooling1D()(representation)\n",
        "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=dropout_rate)\n",
        "\n",
        "    reshaped = Reshape((128,8))(features)\n",
        "    def squash(inputs):\n",
        "        # take norm of input vectors\n",
        "        squared_norm = K.sum(K.square(inputs), axis = -1, keepdims = True)\n",
        "        # use the formula for non-linear function to return squashed output\n",
        "        return ((squared_norm/(1+squared_norm))/(K.sqrt(squared_norm+K.epsilon())))*inputs\n",
        "    # squash the reshaped output to make length of vector b/w 0 and 1\n",
        "    squashed_output = Lambda(squash)(reshaped)\n",
        "    class DigitCapsuleLayer(Layer):\n",
        "        # creating a layer class in keras\n",
        "        def __init__(self, **kwargs):\n",
        "            super(DigitCapsuleLayer, self).__init__(**kwargs)\n",
        "            self.kernel_initializer = initializers.get('glorot_uniform')\n",
        "\n",
        "        def build(self, input_shape):\n",
        "            # initialize weight matrix for each capsule in lower layer\n",
        "            self.W = self.add_weight(shape = [6, 128, 16, 8], initializer = self.kernel_initializer, name = 'weights')\n",
        "            self.built = True\n",
        "\n",
        "        def call(self, inputs):\n",
        "            inputs = K.expand_dims(inputs, 1)\n",
        "            inputs = K.tile(inputs, [1, 6, 1, 1])\n",
        "            # matrix multiplication b/w previous layer output and weight matrix\n",
        "            inputs = K.map_fn(lambda x: own_batch_dot(x, self.W, [2, 3]), elems=inputs)\n",
        "            b = tf.zeros(shape = [K.shape(inputs)[0], 6, 128])\n",
        "\n",
        "    # routing algorithm with updating coupling coefficient c, using scalar product b/w input capsule and output capsule\n",
        "            for i in range(3-1):\n",
        "                # print(b)\n",
        "                c = tf.nn.softmax(b, axis=1)\n",
        "                print(c)\n",
        "                s = own_batch_dot(c, inputs, [2, 2])\n",
        "                print(s)\n",
        "                v = squash(s)\n",
        "                print(v)\n",
        "                b = b + own_batch_dot(v, inputs, [2,3])\n",
        "\n",
        "            return v\n",
        "        def compute_output_shape(self, input_shape):\n",
        "            return tuple([None, 6, 16])\n",
        "    def output_layer(inputs):\n",
        "        return K.sqrt(K.sum(K.square(inputs), -1) + K.epsilon())\n",
        "    digit_caps = DigitCapsuleLayer()(squashed_output)\n",
        "    # flatten1 = Flatten()(digit_caps)\n",
        "    # fc_inference = Dense(activityCount, activation='softmax')(flatten1)\n",
        "    outputs = Lambda(output_layer)(digit_caps)\n",
        "\n",
        "    # logits = layers.Dense(activityCount,  activation='softmax')(features)\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "# ------------------------------specific module for MobileHART------------------------------\n",
        "\n",
        "def conv_block(x, filters=16, kernel_size=3, strides=2):\n",
        "    conv_layer = layers.Conv1D(\n",
        "        filters, kernel_size, strides=strides, activation=tf.nn.swish, padding=\"same\"\n",
        "    )\n",
        "    return conv_layer(x)\n",
        "\n",
        "def inverted_residual_block(x, expanded_channels, output_channels, strides=1):\n",
        "    m = layers.Conv1D(expanded_channels, 1, padding=\"same\", use_bias=False)(x)\n",
        "    m = layers.BatchNormalization()(m)\n",
        "    m = tf.nn.swish(m)\n",
        "\n",
        "    if strides == 2:\n",
        "        m = layers.ZeroPadding1D(padding=1)(m)\n",
        "    m = layers.DepthwiseConv1D(\n",
        "        3, strides=strides, padding=\"same\" if strides == 1 else \"valid\", use_bias=False\n",
        "    )(m)\n",
        "    m = layers.BatchNormalization()(m)\n",
        "    m = tf.nn.swish(m)\n",
        "\n",
        "    m = layers.Conv1D(output_channels, 1, padding=\"same\", use_bias=False)(m)\n",
        "    m = layers.BatchNormalization()(m)\n",
        "\n",
        "    if tf.math.equal(x.shape[-1], output_channels) and strides == 1:\n",
        "        return layers.Add()([m, x])\n",
        "    return m\n",
        "\n",
        "def transformer_block(x, transformer_layers, projection_dim, dropout_rate = 0.3,num_heads=2):\n",
        "\n",
        "    dropPathRate = np.linspace(0, dropout_rate* 10,transformer_layers) * 0.1\n",
        "\n",
        "\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=dropout_rate\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, x])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp2(\n",
        "            x3,\n",
        "            hidden_units=[x.shape[-1] * 2, x.shape[-1]],\n",
        "            dropout_rate=dropout_rate,\n",
        "        )\n",
        "        # Skip connection 2.\n",
        "        x = layers.Add()([x3, x2])\n",
        "\n",
        "    return x\n",
        "\n",
        "def mobilevit_block(x, num_blocks, projection_dim, strides=1):\n",
        "    # Local projection with convolutions.\n",
        "    local_features = conv_block(x, filters=projection_dim, strides=strides)\n",
        "    local_features = conv_block(\n",
        "        local_features, filters=projection_dim, kernel_size=1, strides=strides\n",
        "    )\n",
        "    global_features = transformer_block(\n",
        "        local_features, num_blocks, projection_dim\n",
        "    )\n",
        "\n",
        "    # Apply point-wise conv -> concatenate with the input features.\n",
        "    folded_feature_map = conv_block(\n",
        "        global_features, filters=x.shape[-1], kernel_size=1, strides=strides\n",
        "    )\n",
        "    local_global_features = layers.Concatenate(axis=-1)([x, folded_feature_map])\n",
        "\n",
        "    # Fuse the local and global features using a convoluion layer.\n",
        "    local_global_features = conv_block(\n",
        "        local_global_features, filters=projection_dim, strides=strides\n",
        "    )\n",
        "\n",
        "    return local_global_features\n",
        "\n",
        "\n",
        "def sensorWiseTransformer_block(xAcc, xGyro, patchCount,transformer_layers, projection_dim,kernelSize = 4,  dropout_rate = 0.3,num_heads=2):\n",
        "    projectionQuarter = projection_dim // 4\n",
        "    projectionHalf = projection_dim // 2\n",
        "    dropPathRate = np.linspace(0, dropout_rate* 10,transformer_layers) * 0.1\n",
        "\n",
        "    x = tf.concat((xAcc,xGyro),axis= 2 )\n",
        "    for layerIndex in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6, name = \"normalizedInputs_\"+str(layerIndex))(x)\n",
        "\n",
        "        branch1 = liteFormer(\n",
        "                            startIndex = projectionQuarter,\n",
        "                            stopIndex = projectionQuarter + projectionHalf,\n",
        "                            projectionSize = projectionHalf,\n",
        "                            attentionHead =  num_heads,\n",
        "                            kernelSize = kernelSize,\n",
        "                            dropPathRate = dropPathRate[layerIndex],\n",
        "                            name = \"liteFormer_\"+str(layerIndex))(x1)\n",
        "\n",
        "        branch2Acc = SensorWiseMHA(projectionQuarter,num_heads,0,projectionQuarter,dropPathRate = dropPathRate[layerIndex],dropout_rate = dropout_rate,name = \"AccMHA_\"+str(layerIndex))(x1)\n",
        "        branch2Gyro = SensorWiseMHA(projectionQuarter,num_heads,projectionQuarter + projectionHalf ,projection_dim,dropPathRate = dropPathRate[layerIndex],dropout_rate = dropout_rate,name = \"GyroMHA_\"+str(layerIndex))(x1)\n",
        "        concatAttention = tf.concat((branch2Acc,branch1,branch2Gyro),axis= 2 )\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([concatAttention, x])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp2(\n",
        "            x3,\n",
        "            hidden_units=[x.shape[-1] * 2, x.shape[-1]],\n",
        "            dropout_rate=dropout_rate,\n",
        "        )\n",
        "        x3 = DropPath(dropPathRate[layerIndex])(x3)\n",
        "        # Skip connection 2.\n",
        "        x = layers.Add()([x3, x2])\n",
        "\n",
        "    return x\n",
        "def sensorWiseHART(xAcc,xGyro, num_blocks, projection_dim, kernelSize = 4, strides=1):\n",
        "    # Local projection with convolutions.\n",
        "#     ---------------acc--------------\n",
        "    local_featuresAcc = conv_block(xAcc, filters=projection_dim//2, strides=strides)\n",
        "    local_featuresAcc = conv_block(\n",
        "        local_featuresAcc, filters=projection_dim//2, kernel_size=1, strides=strides\n",
        "    )\n",
        "\n",
        "#     ---------------gyro--------------\n",
        "\n",
        "    local_featuresGyro = conv_block(xGyro, filters=projection_dim//2, strides=strides)\n",
        "    local_featuresGyro = conv_block(\n",
        "        local_featuresGyro, filters=projection_dim//2, kernel_size=1, strides=strides\n",
        "    )\n",
        "    global_features = sensorWiseTransformer_block(local_featuresAcc,\n",
        "        local_featuresGyro, local_featuresGyro.shape[1], num_blocks, projection_dim, kernelSize = kernelSize\n",
        "    )\n",
        "\n",
        "    folded_feature_map_acc = conv_block(\n",
        "        global_features[:,:,:projection_dim//2], filters=xAcc.shape[-1], kernel_size=1, strides=strides\n",
        "    )\n",
        "    local_global_features_acc = layers.Concatenate(axis=-1)([xAcc, folded_feature_map_acc])\n",
        "        # Fuse the local and global features using a convoluion layer.\n",
        "    local_global_features_acc = conv_block(\n",
        "        local_global_features_acc, filters=projection_dim//2, strides=strides\n",
        "    )\n",
        "\n",
        "    folded_feature_map_gyro = conv_block(\n",
        "        global_features[:,:,projection_dim//2:], filters=xGyro.shape[-1], kernel_size=1, strides=strides\n",
        "    )\n",
        "    local_global_features_gyro = layers.Concatenate(axis=-1)([xGyro, folded_feature_map_gyro])\n",
        "\n",
        "    local_global_features_gyro = conv_block(\n",
        "        local_global_features_gyro, filters=projection_dim//2, strides=strides\n",
        "    )\n",
        "\n",
        "    return local_global_features_acc, local_global_features_gyro\n",
        "def mv2Block(x,expansion_factor,filterCount):\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=filterCount[0] * expansion_factor, output_channels=filterCount[1]\n",
        "    )\n",
        "    # Downsampling with MV2 block.\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=filterCount[1] * expansion_factor, output_channels=filterCount[2], strides=2\n",
        "    )\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=filterCount[2] * expansion_factor, output_channels=filterCount[2]\n",
        "    )\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=filterCount[2] * expansion_factor, output_channels=filterCount[2]\n",
        "    )\n",
        "    # First MV2 -> MobileViT block.\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=filterCount[2] * expansion_factor, output_channels=filterCount[3], strides=2\n",
        "    )\n",
        "    return x\n",
        "    # def hartModel(input_shape,activityCount, projection_dim,patchSize,timeStep,num_heads,filterAttentionHead, convKernels = [3, 7, 15, 31, 31, 31], mlp_head_units = [1024],dropout_rate = 0.3,useTokens = True):\n",
        "\n",
        "def mobileHART_XS(input_shape,activityCount,projectionDims = [96,120,144],filterCount = [16//2,32//2,48//2,64//2,80,96,384],expansion_factor=4,mlp_head_units = [1024],dropout_rate = 0.3):\n",
        "\n",
        "    # inputs = keras.Input((segment_size, num_input_channels))\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Initial conv-stem -> MV2 block.\n",
        "    accX = conv_block(inputs[:,:,:3],filters=filterCount[0])\n",
        "    gyroX = conv_block(inputs[:,:,3:],filters=filterCount[0])\n",
        "    accX = mv2Block(accX,expansion_factor,filterCount)\n",
        "    gyroX = mv2Block(gyroX,expansion_factor,filterCount)\n",
        "    accX, gyroX  = sensorWiseHART(accX,gyroX, num_blocks=2, projection_dim=projectionDims[0])\n",
        "    x = tf.concat((accX,gyroX), axis = 2)\n",
        "    x = layers.Dense(projectionDims[0],activation=tf.nn.swish)(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    # Second MV2 -> MobileViT block.\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=projectionDims[0] * expansion_factor, output_channels=filterCount[4], strides=2\n",
        "    )\n",
        "    x = mobilevit_block(x, num_blocks=4, projection_dim=projectionDims[1])\n",
        "\n",
        "    # Third MV2 -> MobileViT block.\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=projectionDims[1] * expansion_factor, output_channels=filterCount[5], strides=2\n",
        "    )\n",
        "    x = mobilevit_block(x, num_blocks=3, projection_dim=projectionDims[2])\n",
        "    x = conv_block(x, filters=filterCount[6], kernel_size=1, strides=1)\n",
        "    # Classification head.\n",
        "    x = layers.GlobalAvgPool1D(name = \"GAP\")(x)\n",
        "\n",
        "    x = mlp(x, hidden_units=mlp_head_units, dropout_rate=dropout_rate)\n",
        "\n",
        "    outputs = layers.Dense(activityCount, activation=\"softmax\")(x)\n",
        "# f.keras.Model(inputs=inputs, outputs=logits)\n",
        "    return tf.keras.Model(inputs, outputs)\n",
        "\n",
        "def mobileHART_XXS(input_shape,activityCount,projectionDims = [64,80,96],filterCount = [16//2,16//2,24//2,48//2,64,80,320],expansion_factor=2,mlp_head_units = [1024],dropout_rate = 0.3):\n",
        "\n",
        "    # inputs = keras.Input((segment_size, num_input_channels))\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Initial conv-stem -> MV2 block.\n",
        "    accX = conv_block(inputs[:,:,:3],filters=filterCount[0])\n",
        "    gyroX = conv_block(inputs[:,:,3:],filters=filterCount[0])\n",
        "    accX = mv2Block(accX,expansion_factor,filterCount)\n",
        "    gyroX = mv2Block(gyroX,expansion_factor,filterCount)\n",
        "    accX, gyroX  = sensorWiseHART(accX,gyroX, num_blocks=2, projection_dim=projectionDims[0])\n",
        "    x = tf.concat((accX,gyroX), axis = 2)\n",
        "    x = layers.Dense(projectionDims[0],activation=tf.nn.swish)(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    # Second MV2 -> MobileViT block.\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=projectionDims[0] * expansion_factor, output_channels=filterCount[4], strides=2\n",
        "    )\n",
        "    x = mobilevit_block(x, num_blocks=4, projection_dim=projectionDims[1])\n",
        "\n",
        "    # Third MV2 -> MobileViT block.\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=projectionDims[1] * expansion_factor, output_channels=filterCount[5], strides=2\n",
        "    )\n",
        "    x = mobilevit_block(x, num_blocks=3, projection_dim=projectionDims[2])\n",
        "    x = conv_block(x, filters=filterCount[6], kernel_size=1, strides=1)\n",
        "    # Classification head.\n",
        "    x = layers.GlobalAvgPool1D(name = \"GAP\")(x)\n",
        "\n",
        "    x = mlp(x, hidden_units=mlp_head_units, dropout_rate=dropout_rate)\n",
        "\n",
        "    outputs = layers.Dense(activityCount, activation=\"softmax\")(x)\n",
        "# f.keras.Model(inputs=inputs, outputs=logits)\n",
        "    return tf.keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayIEcaBiK446",
        "outputId": "42666ff2-13c8-4b15-a7bb-3ab02edf5c19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "V-9Tyf2WKgfe"
      },
      "outputs": [],
      "source": [
        "# load a single file as a numpy array\n",
        "def load_file(filepath):\n",
        "    dataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
        "    return dataframe.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RFtsLcBTKgff"
      },
      "outputs": [],
      "source": [
        "# load a list of files and return as a 3d numpy array\n",
        "def load_group(filenames, prefix=''):\n",
        "    loaded = list()\n",
        "    for name in filenames:\n",
        "        data = load_file(prefix + name)\n",
        "        loaded.append(data)\n",
        "    # stack group so that features are the 3rd dimension\n",
        "    loaded = dstack(loaded)\n",
        "    return loaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IDNdqOsIKgff"
      },
      "outputs": [],
      "source": [
        "# load a dataset group, such as train or test\n",
        "def load_dataset_group(group, prefix=''):\n",
        "    filepath = prefix + group + '/Inertial Signals/'\n",
        "    print('File Path : ',filepath)\n",
        "    # load all 9 files as a single array\n",
        "    filenames = list()\n",
        "    # total acceleration\n",
        "    filenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
        "    # body acceleration\n",
        "    filenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
        "    # body gyroscope\n",
        "    filenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
        "    # load input data\n",
        "    X = load_group(filenames, filepath)\n",
        "    # load class output\n",
        "    y = load_file(prefix + group + '/y_'+group+'.txt')\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pGNunmWKgfg",
        "outputId": "0c135f84-a909-4936-9b6d-5fe3e99cb3e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File Path :  /content/drive/MyDrive/HARCNNLSTM/UCIDataset/train/Inertial Signals/\n",
            "File Path :  /content/drive/MyDrive/HARCNNLSTM/UCIDataset/test/Inertial Signals/\n",
            "X_train.shape :  (7352, 128, 9)\n",
            "Y_train.shape :  (7352, 6)\n",
            "X_test.shape :  (2947, 128, 9)\n",
            "Y_test.shape :  (2947, 6)\n"
          ]
        }
      ],
      "source": [
        "# load all train\n",
        "X_train, Y_train = load_dataset_group('train', '/content/drive/MyDrive/HARCNNLSTM/UCIDataset/')\n",
        "# load all test\n",
        "X_test, Y_test = load_dataset_group('test', '/content/drive/MyDrive/HARCNNLSTM/UCIDataset/')\n",
        "\n",
        "# zero-offset class values\n",
        "Y_train = Y_train - 1\n",
        "Y_test = Y_test - 1\n",
        "# one hot encode y\n",
        "Y_train = to_categorical(Y_train)\n",
        "Y_test = to_categorical(Y_test)\n",
        "\n",
        "print('X_train.shape : ', X_train.shape)\n",
        "print('Y_train.shape : ', Y_train.shape)\n",
        "print('X_test.shape : ', X_test.shape)\n",
        "print('Y_test.shape : ', Y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_zhoqoAJKgfg"
      },
      "outputs": [],
      "source": [
        "verbose = 1\n",
        "epochs = 500\n",
        "batch_size = 128\n",
        "\n",
        "n_timesteps = X_train.shape[1]\n",
        "n_features = X_train.shape[2]\n",
        "n_outputs = Y_train.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SCAD5m6UKgfg"
      },
      "outputs": [],
      "source": [
        "checkpoint = ModelCheckpoint(\"HART_Capsule.h5\", monitor='val_accuracy', verbose=1,\n",
        "                             save_best_only=True, save_weights_only=False, mode='auto')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tn19lw-rCUI-",
        "outputId": "1d890813-41b9-4c35-9040-fee583529440"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensor(\"digit_capsule_layer_1/transpose_1:0\", shape=(None, 6, 128), dtype=float32)\n",
            "Tensor(\"digit_capsule_layer_1/Squeeze:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Tensor(\"digit_capsule_layer_1/mul:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Tensor(\"digit_capsule_layer_1/transpose_3:0\", shape=(None, 6, 128), dtype=float32)\n",
            "Tensor(\"digit_capsule_layer_1/Squeeze_2:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Tensor(\"digit_capsule_layer_1/mul_1:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Epoch 1/500\n",
            "Tensor(\"model_3/digit_capsule_layer_1/transpose_1:0\", shape=(None, 6, 128), dtype=float32)\n",
            "Tensor(\"model_3/digit_capsule_layer_1/Squeeze:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Tensor(\"model_3/digit_capsule_layer_1/mul:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Tensor(\"model_3/digit_capsule_layer_1/transpose_3:0\", shape=(None, 6, 128), dtype=float32)\n",
            "Tensor(\"model_3/digit_capsule_layer_1/Squeeze_2:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Tensor(\"model_3/digit_capsule_layer_1/mul_1:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Tensor(\"model_3/digit_capsule_layer_1/transpose_1:0\", shape=(None, 6, 128), dtype=float32)\n",
            "Tensor(\"model_3/digit_capsule_layer_1/Squeeze:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Tensor(\"model_3/digit_capsule_layer_1/mul:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Tensor(\"model_3/digit_capsule_layer_1/transpose_3:0\", shape=(None, 6, 128), dtype=float32)\n",
            "Tensor(\"model_3/digit_capsule_layer_1/Squeeze_2:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Tensor(\"model_3/digit_capsule_layer_1/mul_1:0\", shape=(None, 6, 16), dtype=float32)\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.9783 - accuracy: 0.5939Tensor(\"model_3/digit_capsule_layer_1/transpose_1:0\", shape=(None, 6, 128), dtype=float32)\n",
            "Tensor(\"model_3/digit_capsule_layer_1/Squeeze:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Tensor(\"model_3/digit_capsule_layer_1/mul:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Tensor(\"model_3/digit_capsule_layer_1/transpose_3:0\", shape=(None, 6, 128), dtype=float32)\n",
            "Tensor(\"model_3/digit_capsule_layer_1/Squeeze_2:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Tensor(\"model_3/digit_capsule_layer_1/mul_1:0\", shape=(None, 6, 16), dtype=float32)\n",
            "58/58 [==============================] - 76s 245ms/step - loss: 0.9783 - accuracy: 0.5939 - val_loss: 0.6878 - val_accuracy: 0.7367\n",
            "Epoch 2/500\n",
            "58/58 [==============================] - 11s 188ms/step - loss: 0.4268 - accuracy: 0.8538 - val_loss: 0.4595 - val_accuracy: 0.8459\n",
            "Epoch 3/500\n",
            "58/58 [==============================] - 9s 162ms/step - loss: 0.2534 - accuracy: 0.9161 - val_loss: 0.3512 - val_accuracy: 0.8687\n",
            "Epoch 4/500\n",
            "58/58 [==============================] - 11s 189ms/step - loss: 0.1882 - accuracy: 0.9339 - val_loss: 0.3318 - val_accuracy: 0.8938\n",
            "Epoch 5/500\n",
            "58/58 [==============================] - 11s 189ms/step - loss: 0.1692 - accuracy: 0.9392 - val_loss: 0.2913 - val_accuracy: 0.8992\n",
            "Epoch 6/500\n",
            "58/58 [==============================] - 11s 189ms/step - loss: 0.1495 - accuracy: 0.9445 - val_loss: 0.3105 - val_accuracy: 0.9033\n",
            "Epoch 7/500\n",
            "58/58 [==============================] - 11s 182ms/step - loss: 0.1527 - accuracy: 0.9460 - val_loss: 0.3420 - val_accuracy: 0.8941\n",
            "Epoch 8/500\n",
            "58/58 [==============================] - 10s 165ms/step - loss: 0.1553 - accuracy: 0.9426 - val_loss: 0.3402 - val_accuracy: 0.8955\n",
            "Epoch 9/500\n",
            "58/58 [==============================] - 11s 192ms/step - loss: 0.1306 - accuracy: 0.9509 - val_loss: 0.3080 - val_accuracy: 0.8982\n",
            "Epoch 10/500\n",
            "58/58 [==============================] - 11s 189ms/step - loss: 0.1268 - accuracy: 0.9518 - val_loss: 0.3108 - val_accuracy: 0.8999\n",
            "Epoch 11/500\n",
            "58/58 [==============================] - 11s 187ms/step - loss: 0.1693 - accuracy: 0.9414 - val_loss: 0.3949 - val_accuracy: 0.8877\n",
            "Epoch 12/500\n",
            "58/58 [==============================] - 11s 184ms/step - loss: 0.1661 - accuracy: 0.9412 - val_loss: 0.3047 - val_accuracy: 0.9006\n",
            "Epoch 13/500\n",
            "58/58 [==============================] - 10s 164ms/step - loss: 0.1224 - accuracy: 0.9510 - val_loss: 0.3169 - val_accuracy: 0.9036\n",
            "Epoch 14/500\n",
            "58/58 [==============================] - 11s 187ms/step - loss: 0.1189 - accuracy: 0.9524 - val_loss: 0.3258 - val_accuracy: 0.9057\n",
            "Epoch 15/500\n",
            "58/58 [==============================] - 11s 189ms/step - loss: 0.1223 - accuracy: 0.9517 - val_loss: 0.4484 - val_accuracy: 0.8772\n",
            "Epoch 16/500\n",
            "58/58 [==============================] - 11s 188ms/step - loss: 0.1433 - accuracy: 0.9478 - val_loss: 0.3121 - val_accuracy: 0.9162\n",
            "Epoch 17/500\n",
            "58/58 [==============================] - 11s 186ms/step - loss: 0.1214 - accuracy: 0.9529 - val_loss: 0.2941 - val_accuracy: 0.9158\n",
            "Epoch 18/500\n",
            "58/58 [==============================] - 9s 161ms/step - loss: 0.1319 - accuracy: 0.9484 - val_loss: 0.2885 - val_accuracy: 0.9175\n",
            "Epoch 19/500\n",
            "58/58 [==============================] - 11s 189ms/step - loss: 0.1146 - accuracy: 0.9505 - val_loss: 0.2794 - val_accuracy: 0.9145\n",
            "Epoch 20/500\n",
            "58/58 [==============================] - 11s 187ms/step - loss: 0.1102 - accuracy: 0.9555 - val_loss: 0.2724 - val_accuracy: 0.9186\n",
            "Epoch 21/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.1018 - accuracy: 0.9588 - val_loss: 0.2792 - val_accuracy: 0.9189\n",
            "Epoch 22/500\n",
            "58/58 [==============================] - 10s 178ms/step - loss: 0.1089 - accuracy: 0.9559 - val_loss: 0.3810 - val_accuracy: 0.8860\n",
            "Epoch 23/500\n",
            "58/58 [==============================] - 10s 166ms/step - loss: 0.1254 - accuracy: 0.9555 - val_loss: 0.2877 - val_accuracy: 0.9155\n",
            "Epoch 24/500\n",
            "58/58 [==============================] - 11s 186ms/step - loss: 0.1218 - accuracy: 0.9495 - val_loss: 0.2263 - val_accuracy: 0.9328\n",
            "Epoch 25/500\n",
            "58/58 [==============================] - 11s 191ms/step - loss: 0.1238 - accuracy: 0.9535 - val_loss: 0.2831 - val_accuracy: 0.9243\n",
            "Epoch 26/500\n",
            "58/58 [==============================] - 11s 188ms/step - loss: 0.1179 - accuracy: 0.9525 - val_loss: 0.4722 - val_accuracy: 0.8646\n",
            "Epoch 27/500\n",
            "58/58 [==============================] - 11s 186ms/step - loss: 0.1107 - accuracy: 0.9550 - val_loss: 0.2878 - val_accuracy: 0.9148\n",
            "Epoch 28/500\n",
            "58/58 [==============================] - 10s 164ms/step - loss: 0.1096 - accuracy: 0.9588 - val_loss: 0.2799 - val_accuracy: 0.9125\n",
            "Epoch 29/500\n",
            "58/58 [==============================] - 11s 191ms/step - loss: 0.1325 - accuracy: 0.9493 - val_loss: 0.2544 - val_accuracy: 0.9152\n",
            "Epoch 30/500\n",
            "58/58 [==============================] - 11s 192ms/step - loss: 0.1131 - accuracy: 0.9563 - val_loss: 0.3309 - val_accuracy: 0.9030\n",
            "Epoch 31/500\n",
            "58/58 [==============================] - 11s 192ms/step - loss: 0.1069 - accuracy: 0.9554 - val_loss: 0.3381 - val_accuracy: 0.9060\n",
            "Epoch 32/500\n",
            "58/58 [==============================] - 11s 196ms/step - loss: 0.1055 - accuracy: 0.9567 - val_loss: 0.2725 - val_accuracy: 0.9240\n",
            "Epoch 33/500\n",
            "58/58 [==============================] - 9s 163ms/step - loss: 0.0969 - accuracy: 0.9595 - val_loss: 0.3183 - val_accuracy: 0.9145\n",
            "Epoch 34/500\n",
            "58/58 [==============================] - 11s 188ms/step - loss: 0.1184 - accuracy: 0.9539 - val_loss: 0.2886 - val_accuracy: 0.9203\n",
            "Epoch 35/500\n",
            "58/58 [==============================] - 11s 191ms/step - loss: 0.1017 - accuracy: 0.9591 - val_loss: 0.3033 - val_accuracy: 0.9108\n",
            "Epoch 36/500\n",
            "58/58 [==============================] - 11s 189ms/step - loss: 0.0981 - accuracy: 0.9591 - val_loss: 0.2867 - val_accuracy: 0.9257\n",
            "Epoch 37/500\n",
            "58/58 [==============================] - 11s 187ms/step - loss: 0.0933 - accuracy: 0.9625 - val_loss: 0.3382 - val_accuracy: 0.9040\n",
            "Epoch 38/500\n",
            "58/58 [==============================] - 10s 167ms/step - loss: 0.1288 - accuracy: 0.9527 - val_loss: 0.2850 - val_accuracy: 0.9169\n",
            "Epoch 39/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.1037 - accuracy: 0.9592 - val_loss: 0.3036 - val_accuracy: 0.9186\n",
            "Epoch 40/500\n",
            "58/58 [==============================] - 11s 196ms/step - loss: 0.1080 - accuracy: 0.9612 - val_loss: 0.2854 - val_accuracy: 0.9209\n",
            "Epoch 41/500\n",
            "58/58 [==============================] - 11s 191ms/step - loss: 0.0985 - accuracy: 0.9616 - val_loss: 0.3097 - val_accuracy: 0.9264\n",
            "Epoch 42/500\n",
            "58/58 [==============================] - 11s 191ms/step - loss: 0.1990 - accuracy: 0.9215 - val_loss: 0.3002 - val_accuracy: 0.9094\n",
            "Epoch 43/500\n",
            "58/58 [==============================] - 11s 188ms/step - loss: 0.1465 - accuracy: 0.9493 - val_loss: 0.2989 - val_accuracy: 0.9121\n",
            "Epoch 44/500\n",
            "58/58 [==============================] - 10s 166ms/step - loss: 0.1311 - accuracy: 0.9546 - val_loss: 0.3069 - val_accuracy: 0.9084\n",
            "Epoch 45/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.1098 - accuracy: 0.9593 - val_loss: 0.3095 - val_accuracy: 0.9118\n",
            "Epoch 46/500\n",
            "58/58 [==============================] - 11s 192ms/step - loss: 0.1071 - accuracy: 0.9606 - val_loss: 0.3037 - val_accuracy: 0.9281\n",
            "Epoch 47/500\n",
            "58/58 [==============================] - 11s 194ms/step - loss: 0.1025 - accuracy: 0.9640 - val_loss: 0.3152 - val_accuracy: 0.9172\n",
            "Epoch 48/500\n",
            "58/58 [==============================] - 11s 199ms/step - loss: 0.1438 - accuracy: 0.9502 - val_loss: 0.2675 - val_accuracy: 0.9230\n",
            "Epoch 49/500\n",
            "58/58 [==============================] - 11s 189ms/step - loss: 0.1088 - accuracy: 0.9569 - val_loss: 0.2927 - val_accuracy: 0.9179\n",
            "Epoch 50/500\n",
            "58/58 [==============================] - 10s 170ms/step - loss: 0.0983 - accuracy: 0.9629 - val_loss: 0.3041 - val_accuracy: 0.9216\n",
            "Epoch 51/500\n",
            "58/58 [==============================] - 11s 197ms/step - loss: 0.0989 - accuracy: 0.9644 - val_loss: 0.2559 - val_accuracy: 0.9230\n",
            "Epoch 52/500\n",
            "58/58 [==============================] - 11s 195ms/step - loss: 0.0886 - accuracy: 0.9676 - val_loss: 0.2860 - val_accuracy: 0.9338\n",
            "Epoch 53/500\n",
            "58/58 [==============================] - 11s 191ms/step - loss: 0.0888 - accuracy: 0.9668 - val_loss: 0.3226 - val_accuracy: 0.8924\n",
            "Epoch 54/500\n",
            "58/58 [==============================] - 12s 204ms/step - loss: 0.1245 - accuracy: 0.9576 - val_loss: 0.2851 - val_accuracy: 0.9162\n",
            "Epoch 55/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.1075 - accuracy: 0.9623 - val_loss: 0.2746 - val_accuracy: 0.9287\n",
            "Epoch 56/500\n",
            "58/58 [==============================] - 10s 162ms/step - loss: 0.0877 - accuracy: 0.9682 - val_loss: 0.2742 - val_accuracy: 0.9240\n",
            "Epoch 57/500\n",
            "58/58 [==============================] - 11s 191ms/step - loss: 0.0812 - accuracy: 0.9690 - val_loss: 0.2197 - val_accuracy: 0.9325\n",
            "Epoch 58/500\n",
            "58/58 [==============================] - 11s 189ms/step - loss: 0.0889 - accuracy: 0.9690 - val_loss: 0.2780 - val_accuracy: 0.9294\n",
            "Epoch 59/500\n",
            "58/58 [==============================] - 11s 188ms/step - loss: 0.0827 - accuracy: 0.9695 - val_loss: 0.2885 - val_accuracy: 0.9243\n",
            "Epoch 60/500\n",
            "58/58 [==============================] - 11s 194ms/step - loss: 0.0886 - accuracy: 0.9652 - val_loss: 0.2886 - val_accuracy: 0.9237\n",
            "Epoch 61/500\n",
            "58/58 [==============================] - 10s 167ms/step - loss: 0.0807 - accuracy: 0.9686 - val_loss: 0.3269 - val_accuracy: 0.9135\n",
            "Epoch 62/500\n",
            "58/58 [==============================] - 11s 194ms/step - loss: 0.0816 - accuracy: 0.9712 - val_loss: 0.2558 - val_accuracy: 0.9304\n",
            "Epoch 63/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0793 - accuracy: 0.9713 - val_loss: 0.2555 - val_accuracy: 0.9311\n",
            "Epoch 64/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0782 - accuracy: 0.9725 - val_loss: 0.2397 - val_accuracy: 0.9369\n",
            "Epoch 65/500\n",
            "58/58 [==============================] - 12s 204ms/step - loss: 0.0789 - accuracy: 0.9731 - val_loss: 0.2763 - val_accuracy: 0.9257\n",
            "Epoch 66/500\n",
            "58/58 [==============================] - 10s 175ms/step - loss: 0.0783 - accuracy: 0.9717 - val_loss: 0.2952 - val_accuracy: 0.9318\n",
            "Epoch 67/500\n",
            "58/58 [==============================] - 10s 177ms/step - loss: 0.0702 - accuracy: 0.9758 - val_loss: 0.2613 - val_accuracy: 0.9325\n",
            "Epoch 68/500\n",
            "58/58 [==============================] - 11s 189ms/step - loss: 0.0611 - accuracy: 0.9781 - val_loss: 0.2966 - val_accuracy: 0.9281\n",
            "Epoch 69/500\n",
            "58/58 [==============================] - 11s 192ms/step - loss: 0.0729 - accuracy: 0.9757 - val_loss: 0.2515 - val_accuracy: 0.9304\n",
            "Epoch 70/500\n",
            "58/58 [==============================] - 11s 196ms/step - loss: 0.0733 - accuracy: 0.9743 - val_loss: 0.2654 - val_accuracy: 0.9318\n",
            "Epoch 71/500\n",
            "58/58 [==============================] - 11s 188ms/step - loss: 0.0639 - accuracy: 0.9767 - val_loss: 0.2557 - val_accuracy: 0.9301\n",
            "Epoch 72/500\n",
            "58/58 [==============================] - 10s 169ms/step - loss: 0.0732 - accuracy: 0.9743 - val_loss: 0.2566 - val_accuracy: 0.9359\n",
            "Epoch 73/500\n",
            "58/58 [==============================] - 11s 194ms/step - loss: 0.0600 - accuracy: 0.9792 - val_loss: 0.2397 - val_accuracy: 0.9291\n",
            "Epoch 74/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0556 - accuracy: 0.9782 - val_loss: 0.3028 - val_accuracy: 0.9362\n",
            "Epoch 75/500\n",
            "58/58 [==============================] - 11s 191ms/step - loss: 0.0654 - accuracy: 0.9752 - val_loss: 0.2950 - val_accuracy: 0.9220\n",
            "Epoch 76/500\n",
            "58/58 [==============================] - 10s 180ms/step - loss: 0.1032 - accuracy: 0.9656 - val_loss: 0.2572 - val_accuracy: 0.9325\n",
            "Epoch 77/500\n",
            "58/58 [==============================] - 10s 163ms/step - loss: 0.0639 - accuracy: 0.9770 - val_loss: 0.2637 - val_accuracy: 0.9342\n",
            "Epoch 78/500\n",
            "58/58 [==============================] - 11s 189ms/step - loss: 0.0739 - accuracy: 0.9746 - val_loss: 0.2210 - val_accuracy: 0.9348\n",
            "Epoch 79/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0537 - accuracy: 0.9797 - val_loss: 0.2949 - val_accuracy: 0.9291\n",
            "Epoch 80/500\n",
            "58/58 [==============================] - 11s 188ms/step - loss: 0.0610 - accuracy: 0.9788 - val_loss: 0.2668 - val_accuracy: 0.9386\n",
            "Epoch 81/500\n",
            "58/58 [==============================] - 10s 180ms/step - loss: 0.0574 - accuracy: 0.9791 - val_loss: 0.2904 - val_accuracy: 0.9315\n",
            "Epoch 82/500\n",
            "58/58 [==============================] - 10s 171ms/step - loss: 0.0563 - accuracy: 0.9782 - val_loss: 0.2868 - val_accuracy: 0.9352\n",
            "Epoch 83/500\n",
            "58/58 [==============================] - 11s 194ms/step - loss: 0.0482 - accuracy: 0.9826 - val_loss: 0.2673 - val_accuracy: 0.9315\n",
            "Epoch 84/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0447 - accuracy: 0.9838 - val_loss: 0.3203 - val_accuracy: 0.9270\n",
            "Epoch 85/500\n",
            "58/58 [==============================] - 11s 188ms/step - loss: 0.0478 - accuracy: 0.9834 - val_loss: 0.3195 - val_accuracy: 0.9233\n",
            "Epoch 86/500\n",
            "58/58 [==============================] - 11s 185ms/step - loss: 0.0470 - accuracy: 0.9838 - val_loss: 0.2823 - val_accuracy: 0.9338\n",
            "Epoch 87/500\n",
            "58/58 [==============================] - 9s 162ms/step - loss: 0.0595 - accuracy: 0.9795 - val_loss: 0.3092 - val_accuracy: 0.9301\n",
            "Epoch 88/500\n",
            "58/58 [==============================] - 11s 186ms/step - loss: 0.0441 - accuracy: 0.9816 - val_loss: 0.3084 - val_accuracy: 0.9287\n",
            "Epoch 89/500\n",
            "58/58 [==============================] - 11s 191ms/step - loss: 0.0629 - accuracy: 0.9774 - val_loss: 0.3023 - val_accuracy: 0.9274\n",
            "Epoch 90/500\n",
            "58/58 [==============================] - 11s 195ms/step - loss: 0.0642 - accuracy: 0.9801 - val_loss: 0.2862 - val_accuracy: 0.9318\n",
            "Epoch 91/500\n",
            "58/58 [==============================] - 11s 196ms/step - loss: 0.0432 - accuracy: 0.9841 - val_loss: 0.3161 - val_accuracy: 0.9315\n",
            "Epoch 92/500\n",
            "58/58 [==============================] - 9s 164ms/step - loss: 0.0467 - accuracy: 0.9826 - val_loss: 0.3536 - val_accuracy: 0.9111\n",
            "Epoch 93/500\n",
            "58/58 [==============================] - 11s 185ms/step - loss: 0.0600 - accuracy: 0.9785 - val_loss: 0.2713 - val_accuracy: 0.9362\n",
            "Epoch 94/500\n",
            "58/58 [==============================] - 11s 186ms/step - loss: 0.0416 - accuracy: 0.9844 - val_loss: 0.2896 - val_accuracy: 0.9355\n",
            "Epoch 95/500\n",
            "58/58 [==============================] - 11s 187ms/step - loss: 0.0433 - accuracy: 0.9852 - val_loss: 0.2838 - val_accuracy: 0.9362\n",
            "Epoch 96/500\n",
            "58/58 [==============================] - 12s 201ms/step - loss: 0.0391 - accuracy: 0.9864 - val_loss: 0.2728 - val_accuracy: 0.9406\n",
            "Epoch 97/500\n",
            "58/58 [==============================] - 10s 170ms/step - loss: 0.0443 - accuracy: 0.9841 - val_loss: 0.2847 - val_accuracy: 0.9376\n",
            "Epoch 98/500\n",
            "58/58 [==============================] - 11s 186ms/step - loss: 0.0420 - accuracy: 0.9854 - val_loss: 0.2824 - val_accuracy: 0.9399\n",
            "Epoch 99/500\n",
            "58/58 [==============================] - 11s 193ms/step - loss: 0.0436 - accuracy: 0.9827 - val_loss: 0.2943 - val_accuracy: 0.9321\n",
            "Epoch 100/500\n",
            "58/58 [==============================] - 11s 189ms/step - loss: 0.0444 - accuracy: 0.9842 - val_loss: 0.3023 - val_accuracy: 0.9345\n",
            "Epoch 101/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0475 - accuracy: 0.9826 - val_loss: 0.3117 - val_accuracy: 0.9298\n",
            "Epoch 102/500\n",
            "58/58 [==============================] - 11s 186ms/step - loss: 0.0484 - accuracy: 0.9838 - val_loss: 0.3063 - val_accuracy: 0.9325\n",
            "Epoch 103/500\n",
            "58/58 [==============================] - 10s 164ms/step - loss: 0.0592 - accuracy: 0.9803 - val_loss: 0.2636 - val_accuracy: 0.9382\n",
            "Epoch 104/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0533 - accuracy: 0.9810 - val_loss: 0.3031 - val_accuracy: 0.9325\n",
            "Epoch 105/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0396 - accuracy: 0.9859 - val_loss: 0.2968 - val_accuracy: 0.9328\n",
            "Epoch 106/500\n",
            "58/58 [==============================] - 13s 222ms/step - loss: 0.0412 - accuracy: 0.9841 - val_loss: 0.2709 - val_accuracy: 0.9335\n",
            "Epoch 107/500\n",
            "58/58 [==============================] - 11s 199ms/step - loss: 0.0519 - accuracy: 0.9844 - val_loss: 0.2964 - val_accuracy: 0.9359\n",
            "Epoch 108/500\n",
            "58/58 [==============================] - 11s 196ms/step - loss: 0.0403 - accuracy: 0.9860 - val_loss: 0.2774 - val_accuracy: 0.9382\n",
            "Epoch 109/500\n",
            "58/58 [==============================] - 12s 200ms/step - loss: 0.0454 - accuracy: 0.9835 - val_loss: 0.2814 - val_accuracy: 0.9304\n",
            "Epoch 110/500\n",
            "58/58 [==============================] - 11s 183ms/step - loss: 0.0393 - accuracy: 0.9856 - val_loss: 0.2899 - val_accuracy: 0.9352\n",
            "Epoch 111/500\n",
            "58/58 [==============================] - 12s 197ms/step - loss: 0.0459 - accuracy: 0.9857 - val_loss: 0.2502 - val_accuracy: 0.9359\n",
            "Epoch 112/500\n",
            "58/58 [==============================] - 12s 201ms/step - loss: 0.0428 - accuracy: 0.9839 - val_loss: 0.3177 - val_accuracy: 0.9155\n",
            "Epoch 113/500\n",
            "58/58 [==============================] - 12s 201ms/step - loss: 0.0586 - accuracy: 0.9820 - val_loss: 0.2611 - val_accuracy: 0.9348\n",
            "Epoch 114/500\n",
            "58/58 [==============================] - 11s 198ms/step - loss: 0.0549 - accuracy: 0.9842 - val_loss: 0.2725 - val_accuracy: 0.9315\n",
            "Epoch 115/500\n",
            "58/58 [==============================] - 11s 197ms/step - loss: 0.0371 - accuracy: 0.9868 - val_loss: 0.2670 - val_accuracy: 0.9393\n",
            "Epoch 116/500\n",
            "58/58 [==============================] - 11s 198ms/step - loss: 0.0455 - accuracy: 0.9834 - val_loss: 0.2974 - val_accuracy: 0.9382\n",
            "Epoch 117/500\n",
            "58/58 [==============================] - 10s 170ms/step - loss: 0.0396 - accuracy: 0.9854 - val_loss: 0.2907 - val_accuracy: 0.9338\n",
            "Epoch 118/500\n",
            "58/58 [==============================] - 11s 194ms/step - loss: 0.0345 - accuracy: 0.9882 - val_loss: 0.3024 - val_accuracy: 0.9365\n",
            "Epoch 119/500\n",
            "58/58 [==============================] - 11s 193ms/step - loss: 0.0593 - accuracy: 0.9803 - val_loss: 0.2782 - val_accuracy: 0.9342\n",
            "Epoch 120/500\n",
            "58/58 [==============================] - 11s 193ms/step - loss: 0.0730 - accuracy: 0.9778 - val_loss: 0.2120 - val_accuracy: 0.9444\n",
            "Epoch 121/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0436 - accuracy: 0.9848 - val_loss: 0.2422 - val_accuracy: 0.9359\n",
            "Epoch 122/500\n",
            "58/58 [==============================] - 10s 174ms/step - loss: 0.0274 - accuracy: 0.9903 - val_loss: 0.2881 - val_accuracy: 0.9379\n",
            "Epoch 123/500\n",
            "58/58 [==============================] - 11s 181ms/step - loss: 0.0458 - accuracy: 0.9860 - val_loss: 0.2414 - val_accuracy: 0.9338\n",
            "Epoch 124/500\n",
            "58/58 [==============================] - 11s 191ms/step - loss: 0.0388 - accuracy: 0.9875 - val_loss: 0.2315 - val_accuracy: 0.9447\n",
            "Epoch 125/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0414 - accuracy: 0.9853 - val_loss: 0.2949 - val_accuracy: 0.9386\n",
            "Epoch 126/500\n",
            "58/58 [==============================] - 11s 191ms/step - loss: 0.0316 - accuracy: 0.9882 - val_loss: 0.2742 - val_accuracy: 0.9386\n",
            "Epoch 127/500\n",
            "58/58 [==============================] - 11s 198ms/step - loss: 0.0348 - accuracy: 0.9887 - val_loss: 0.3200 - val_accuracy: 0.9335\n",
            "Epoch 128/500\n",
            "58/58 [==============================] - 11s 196ms/step - loss: 0.0549 - accuracy: 0.9827 - val_loss: 0.2439 - val_accuracy: 0.9308\n",
            "Epoch 129/500\n",
            "58/58 [==============================] - 10s 176ms/step - loss: 0.0326 - accuracy: 0.9899 - val_loss: 0.3147 - val_accuracy: 0.9311\n",
            "Epoch 130/500\n",
            "58/58 [==============================] - 11s 197ms/step - loss: 0.0385 - accuracy: 0.9861 - val_loss: 0.2661 - val_accuracy: 0.9352\n",
            "Epoch 131/500\n",
            "58/58 [==============================] - 11s 195ms/step - loss: 0.0279 - accuracy: 0.9899 - val_loss: 0.2709 - val_accuracy: 0.9393\n",
            "Epoch 132/500\n",
            "58/58 [==============================] - 11s 193ms/step - loss: 0.0342 - accuracy: 0.9872 - val_loss: 0.2742 - val_accuracy: 0.9345\n",
            "Epoch 133/500\n",
            "58/58 [==============================] - 11s 194ms/step - loss: 0.0224 - accuracy: 0.9924 - val_loss: 0.2854 - val_accuracy: 0.9369\n",
            "Epoch 134/500\n",
            "58/58 [==============================] - 10s 180ms/step - loss: 0.0417 - accuracy: 0.9860 - val_loss: 0.3064 - val_accuracy: 0.9369\n",
            "Epoch 135/500\n",
            "58/58 [==============================] - 10s 172ms/step - loss: 0.0274 - accuracy: 0.9894 - val_loss: 0.2980 - val_accuracy: 0.9365\n",
            "Epoch 136/500\n",
            "58/58 [==============================] - 11s 192ms/step - loss: 0.0295 - accuracy: 0.9905 - val_loss: 0.2683 - val_accuracy: 0.9376\n",
            "Epoch 137/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0389 - accuracy: 0.9861 - val_loss: 0.2650 - val_accuracy: 0.9376\n",
            "Epoch 138/500\n",
            "58/58 [==============================] - 11s 188ms/step - loss: 0.0502 - accuracy: 0.9826 - val_loss: 0.3233 - val_accuracy: 0.9277\n",
            "Epoch 139/500\n",
            "58/58 [==============================] - 11s 189ms/step - loss: 0.0289 - accuracy: 0.9891 - val_loss: 0.3774 - val_accuracy: 0.9213\n",
            "Epoch 140/500\n",
            "58/58 [==============================] - 9s 162ms/step - loss: 0.0364 - accuracy: 0.9871 - val_loss: 0.2365 - val_accuracy: 0.9406\n",
            "Epoch 141/500\n",
            "58/58 [==============================] - 11s 187ms/step - loss: 0.0315 - accuracy: 0.9880 - val_loss: 0.2864 - val_accuracy: 0.9355\n",
            "Epoch 142/500\n",
            "58/58 [==============================] - 11s 188ms/step - loss: 0.0246 - accuracy: 0.9908 - val_loss: 0.2741 - val_accuracy: 0.9437\n",
            "Epoch 143/500\n",
            "58/58 [==============================] - 11s 187ms/step - loss: 0.0265 - accuracy: 0.9898 - val_loss: 0.2705 - val_accuracy: 0.9372\n",
            "Epoch 144/500\n",
            "58/58 [==============================] - 11s 193ms/step - loss: 0.0210 - accuracy: 0.9921 - val_loss: 0.3396 - val_accuracy: 0.9335\n",
            "Epoch 145/500\n",
            "58/58 [==============================] - 9s 161ms/step - loss: 0.0291 - accuracy: 0.9883 - val_loss: 0.3064 - val_accuracy: 0.9365\n",
            "Epoch 146/500\n",
            "58/58 [==============================] - 11s 185ms/step - loss: 0.0185 - accuracy: 0.9922 - val_loss: 0.3714 - val_accuracy: 0.9359\n",
            "Epoch 147/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0225 - accuracy: 0.9906 - val_loss: 0.3245 - val_accuracy: 0.9365\n",
            "Epoch 148/500\n",
            "58/58 [==============================] - 11s 189ms/step - loss: 0.0218 - accuracy: 0.9922 - val_loss: 0.3134 - val_accuracy: 0.9328\n",
            "Epoch 149/500\n",
            "58/58 [==============================] - 12s 199ms/step - loss: 0.0295 - accuracy: 0.9895 - val_loss: 0.2797 - val_accuracy: 0.9372\n",
            "Epoch 150/500\n",
            "58/58 [==============================] - 10s 168ms/step - loss: 0.0299 - accuracy: 0.9897 - val_loss: 0.2640 - val_accuracy: 0.9386\n",
            "Epoch 151/500\n",
            "58/58 [==============================] - 11s 186ms/step - loss: 0.0545 - accuracy: 0.9846 - val_loss: 0.3472 - val_accuracy: 0.9165\n",
            "Epoch 152/500\n",
            "58/58 [==============================] - 11s 189ms/step - loss: 0.0454 - accuracy: 0.9865 - val_loss: 0.3070 - val_accuracy: 0.9277\n",
            "Epoch 153/500\n",
            "58/58 [==============================] - 11s 188ms/step - loss: 0.0353 - accuracy: 0.9891 - val_loss: 0.2394 - val_accuracy: 0.9321\n",
            "Epoch 154/500\n",
            "58/58 [==============================] - 11s 187ms/step - loss: 0.0504 - accuracy: 0.9838 - val_loss: 0.2374 - val_accuracy: 0.9376\n",
            "Epoch 155/500\n",
            "58/58 [==============================] - 11s 189ms/step - loss: 0.0224 - accuracy: 0.9931 - val_loss: 0.3179 - val_accuracy: 0.9332\n",
            "Epoch 156/500\n",
            "58/58 [==============================] - 10s 171ms/step - loss: 0.0244 - accuracy: 0.9921 - val_loss: 0.2537 - val_accuracy: 0.9406\n",
            "Epoch 157/500\n",
            "58/58 [==============================] - 11s 191ms/step - loss: 0.0203 - accuracy: 0.9927 - val_loss: 0.3259 - val_accuracy: 0.9311\n",
            "Epoch 158/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0323 - accuracy: 0.9906 - val_loss: 0.2494 - val_accuracy: 0.9450\n",
            "Epoch 159/500\n",
            "58/58 [==============================] - 11s 192ms/step - loss: 0.0259 - accuracy: 0.9913 - val_loss: 0.2910 - val_accuracy: 0.9335\n",
            "Epoch 160/500\n",
            "58/58 [==============================] - 11s 185ms/step - loss: 0.0220 - accuracy: 0.9928 - val_loss: 0.2813 - val_accuracy: 0.9372\n",
            "Epoch 161/500\n",
            "58/58 [==============================] - 9s 164ms/step - loss: 0.0280 - accuracy: 0.9914 - val_loss: 0.3105 - val_accuracy: 0.9342\n",
            "Epoch 162/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0289 - accuracy: 0.9914 - val_loss: 0.3042 - val_accuracy: 0.9386\n",
            "Epoch 163/500\n",
            "58/58 [==============================] - 11s 188ms/step - loss: 0.0234 - accuracy: 0.9940 - val_loss: 0.2739 - val_accuracy: 0.9457\n",
            "Epoch 164/500\n",
            "58/58 [==============================] - 11s 192ms/step - loss: 0.0259 - accuracy: 0.9925 - val_loss: 0.3011 - val_accuracy: 0.9389\n",
            "Epoch 165/500\n",
            "58/58 [==============================] - 11s 191ms/step - loss: 0.0361 - accuracy: 0.9886 - val_loss: 0.2661 - val_accuracy: 0.9376\n",
            "Epoch 166/500\n",
            "58/58 [==============================] - 11s 199ms/step - loss: 0.0320 - accuracy: 0.9884 - val_loss: 0.2972 - val_accuracy: 0.9301\n",
            "Epoch 167/500\n",
            "58/58 [==============================] - 11s 182ms/step - loss: 0.0323 - accuracy: 0.9888 - val_loss: 0.2369 - val_accuracy: 0.9362\n",
            "Epoch 168/500\n",
            "58/58 [==============================] - 12s 199ms/step - loss: 0.0198 - accuracy: 0.9929 - val_loss: 0.3148 - val_accuracy: 0.9386\n",
            "Epoch 169/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0392 - accuracy: 0.9864 - val_loss: 0.2441 - val_accuracy: 0.9420\n",
            "Epoch 170/500\n",
            "58/58 [==============================] - 11s 187ms/step - loss: 0.0407 - accuracy: 0.9891 - val_loss: 0.3001 - val_accuracy: 0.9359\n",
            "Epoch 171/500\n",
            "58/58 [==============================] - 12s 203ms/step - loss: 0.0227 - accuracy: 0.9928 - val_loss: 0.2946 - val_accuracy: 0.9345\n",
            "Epoch 172/500\n",
            "58/58 [==============================] - 10s 171ms/step - loss: 0.0289 - accuracy: 0.9897 - val_loss: 0.3095 - val_accuracy: 0.9420\n",
            "Epoch 173/500\n",
            "58/58 [==============================] - 10s 176ms/step - loss: 0.0225 - accuracy: 0.9917 - val_loss: 0.3681 - val_accuracy: 0.9325\n",
            "Epoch 174/500\n",
            "58/58 [==============================] - 11s 191ms/step - loss: 0.0181 - accuracy: 0.9943 - val_loss: 0.3519 - val_accuracy: 0.9372\n",
            "Epoch 175/500\n",
            "58/58 [==============================] - 11s 188ms/step - loss: 0.0309 - accuracy: 0.9893 - val_loss: 0.3365 - val_accuracy: 0.9287\n",
            "Epoch 176/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0240 - accuracy: 0.9916 - val_loss: 0.3278 - val_accuracy: 0.9359\n",
            "Epoch 177/500\n",
            "58/58 [==============================] - 11s 187ms/step - loss: 0.0232 - accuracy: 0.9921 - val_loss: 0.2924 - val_accuracy: 0.9379\n",
            "Epoch 178/500\n",
            "58/58 [==============================] - 10s 167ms/step - loss: 0.0184 - accuracy: 0.9944 - val_loss: 0.3382 - val_accuracy: 0.9311\n",
            "Epoch 179/500\n",
            "58/58 [==============================] - 11s 192ms/step - loss: 0.0162 - accuracy: 0.9942 - val_loss: 0.2962 - val_accuracy: 0.9362\n",
            "Epoch 180/500\n",
            "58/58 [==============================] - 11s 193ms/step - loss: 0.0256 - accuracy: 0.9921 - val_loss: 0.3178 - val_accuracy: 0.9318\n",
            "Epoch 181/500\n",
            "58/58 [==============================] - 13s 220ms/step - loss: 0.0250 - accuracy: 0.9918 - val_loss: 0.2759 - val_accuracy: 0.9386\n",
            "Epoch 182/500\n",
            "58/58 [==============================] - 12s 199ms/step - loss: 0.0313 - accuracy: 0.9909 - val_loss: 0.3399 - val_accuracy: 0.9301\n",
            "Epoch 183/500\n",
            "58/58 [==============================] - 11s 197ms/step - loss: 0.0374 - accuracy: 0.9899 - val_loss: 0.3017 - val_accuracy: 0.9376\n",
            "Epoch 184/500\n",
            "58/58 [==============================] - 11s 183ms/step - loss: 0.0276 - accuracy: 0.9921 - val_loss: 0.3108 - val_accuracy: 0.9376\n",
            "Epoch 185/500\n",
            "58/58 [==============================] - 11s 188ms/step - loss: 0.0259 - accuracy: 0.9895 - val_loss: 0.2853 - val_accuracy: 0.9359\n",
            "Epoch 186/500\n",
            "58/58 [==============================] - 12s 200ms/step - loss: 0.0426 - accuracy: 0.9867 - val_loss: 0.2809 - val_accuracy: 0.9403\n",
            "Epoch 187/500\n",
            "58/58 [==============================] - 12s 201ms/step - loss: 0.0278 - accuracy: 0.9921 - val_loss: 0.3347 - val_accuracy: 0.9338\n",
            "Epoch 188/500\n",
            "58/58 [==============================] - 12s 199ms/step - loss: 0.0322 - accuracy: 0.9894 - val_loss: 0.2736 - val_accuracy: 0.9335\n",
            "Epoch 189/500\n",
            "58/58 [==============================] - 11s 199ms/step - loss: 0.0246 - accuracy: 0.9929 - val_loss: 0.3358 - val_accuracy: 0.9325\n",
            "Epoch 190/500\n",
            "58/58 [==============================] - 11s 191ms/step - loss: 0.0138 - accuracy: 0.9947 - val_loss: 0.3600 - val_accuracy: 0.9348\n",
            "Epoch 191/500\n",
            "58/58 [==============================] - 10s 168ms/step - loss: 0.0221 - accuracy: 0.9933 - val_loss: 0.3384 - val_accuracy: 0.9332\n",
            "Epoch 192/500\n",
            "58/58 [==============================] - 11s 194ms/step - loss: 0.0316 - accuracy: 0.9913 - val_loss: 0.3020 - val_accuracy: 0.9393\n",
            "Epoch 193/500\n",
            "58/58 [==============================] - 11s 192ms/step - loss: 0.0400 - accuracy: 0.9888 - val_loss: 0.2824 - val_accuracy: 0.9287\n",
            "Epoch 194/500\n",
            "58/58 [==============================] - 11s 193ms/step - loss: 0.0284 - accuracy: 0.9918 - val_loss: 0.3229 - val_accuracy: 0.9315\n",
            "Epoch 195/500\n",
            "58/58 [==============================] - 12s 211ms/step - loss: 0.0230 - accuracy: 0.9939 - val_loss: 0.2907 - val_accuracy: 0.9264\n",
            "Epoch 196/500\n",
            "58/58 [==============================] - 11s 191ms/step - loss: 0.0329 - accuracy: 0.9903 - val_loss: 0.3004 - val_accuracy: 0.9270\n",
            "Epoch 197/500\n",
            "58/58 [==============================] - 9s 162ms/step - loss: 0.0253 - accuracy: 0.9927 - val_loss: 0.3399 - val_accuracy: 0.9287\n",
            "Epoch 198/500\n",
            "58/58 [==============================] - 11s 192ms/step - loss: 0.0194 - accuracy: 0.9942 - val_loss: 0.4028 - val_accuracy: 0.9230\n",
            "Epoch 199/500\n",
            "58/58 [==============================] - 11s 187ms/step - loss: 0.0157 - accuracy: 0.9946 - val_loss: 0.4418 - val_accuracy: 0.9277\n",
            "Epoch 200/500\n",
            "58/58 [==============================] - 11s 189ms/step - loss: 0.0127 - accuracy: 0.9961 - val_loss: 0.3676 - val_accuracy: 0.9335\n",
            "Epoch 201/500\n",
            "58/58 [==============================] - 11s 198ms/step - loss: 0.0132 - accuracy: 0.9963 - val_loss: 0.3927 - val_accuracy: 0.9308\n",
            "Epoch 202/500\n",
            "58/58 [==============================] - 9s 162ms/step - loss: 0.0314 - accuracy: 0.9906 - val_loss: 0.3087 - val_accuracy: 0.9362\n",
            "Epoch 203/500\n",
            "58/58 [==============================] - 11s 191ms/step - loss: 0.0477 - accuracy: 0.9875 - val_loss: 0.3029 - val_accuracy: 0.9318\n",
            "Epoch 204/500\n",
            "58/58 [==============================] - 11s 192ms/step - loss: 0.0235 - accuracy: 0.9914 - val_loss: 0.3304 - val_accuracy: 0.9345\n",
            "Epoch 205/500\n",
            "58/58 [==============================] - 11s 192ms/step - loss: 0.0227 - accuracy: 0.9931 - val_loss: 0.2875 - val_accuracy: 0.9281\n",
            "Epoch 206/500\n",
            "58/58 [==============================] - 12s 211ms/step - loss: 0.0271 - accuracy: 0.9916 - val_loss: 0.3264 - val_accuracy: 0.9318\n",
            "Epoch 207/500\n",
            "58/58 [==============================] - 11s 197ms/step - loss: 0.0130 - accuracy: 0.9955 - val_loss: 0.3440 - val_accuracy: 0.9328\n",
            "Epoch 208/500\n",
            "58/58 [==============================] - 9s 164ms/step - loss: 0.0314 - accuracy: 0.9909 - val_loss: 0.3357 - val_accuracy: 0.9284\n",
            "Epoch 209/500\n",
            "58/58 [==============================] - 11s 193ms/step - loss: 0.0203 - accuracy: 0.9940 - val_loss: 0.3417 - val_accuracy: 0.9291\n",
            "Epoch 210/500\n",
            "58/58 [==============================] - 11s 192ms/step - loss: 0.0196 - accuracy: 0.9939 - val_loss: 0.3394 - val_accuracy: 0.9325\n",
            "Epoch 211/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0301 - accuracy: 0.9914 - val_loss: 0.3052 - val_accuracy: 0.9348\n",
            "Epoch 212/500\n",
            "58/58 [==============================] - 11s 188ms/step - loss: 0.0201 - accuracy: 0.9944 - val_loss: 0.3278 - val_accuracy: 0.9335\n",
            "Epoch 213/500\n",
            "58/58 [==============================] - 9s 160ms/step - loss: 0.0255 - accuracy: 0.9928 - val_loss: 0.2731 - val_accuracy: 0.9342\n",
            "Epoch 214/500\n",
            "58/58 [==============================] - 11s 185ms/step - loss: 0.0179 - accuracy: 0.9951 - val_loss: 0.3871 - val_accuracy: 0.9281\n",
            "Epoch 215/500\n",
            "58/58 [==============================] - 11s 188ms/step - loss: 0.0367 - accuracy: 0.9894 - val_loss: 0.3441 - val_accuracy: 0.9220\n",
            "Epoch 216/500\n",
            "58/58 [==============================] - 11s 192ms/step - loss: 0.0345 - accuracy: 0.9895 - val_loss: 0.3181 - val_accuracy: 0.9311\n",
            "Epoch 217/500\n",
            "58/58 [==============================] - 12s 204ms/step - loss: 0.0378 - accuracy: 0.9879 - val_loss: 0.2407 - val_accuracy: 0.9382\n",
            "Epoch 218/500\n",
            "58/58 [==============================] - 10s 172ms/step - loss: 0.0190 - accuracy: 0.9952 - val_loss: 0.3358 - val_accuracy: 0.9359\n",
            "Epoch 219/500\n",
            "58/58 [==============================] - 10s 176ms/step - loss: 0.0195 - accuracy: 0.9933 - val_loss: 0.3426 - val_accuracy: 0.9321\n",
            "Epoch 220/500\n",
            "58/58 [==============================] - 11s 188ms/step - loss: 0.0163 - accuracy: 0.9950 - val_loss: 0.3158 - val_accuracy: 0.9342\n",
            "Epoch 221/500\n",
            "58/58 [==============================] - 11s 188ms/step - loss: 0.0350 - accuracy: 0.9887 - val_loss: 0.2444 - val_accuracy: 0.9342\n",
            "Epoch 222/500\n",
            "58/58 [==============================] - 11s 187ms/step - loss: 0.0250 - accuracy: 0.9928 - val_loss: 0.2697 - val_accuracy: 0.9376\n",
            "Epoch 223/500\n",
            "58/58 [==============================] - 10s 166ms/step - loss: 0.0198 - accuracy: 0.9947 - val_loss: 0.2917 - val_accuracy: 0.9406\n",
            "Epoch 224/500\n",
            "58/58 [==============================] - 11s 184ms/step - loss: 0.0338 - accuracy: 0.9887 - val_loss: 0.2631 - val_accuracy: 0.9328\n",
            "Epoch 225/500\n",
            "58/58 [==============================] - 11s 192ms/step - loss: 0.0314 - accuracy: 0.9912 - val_loss: 0.2927 - val_accuracy: 0.9355\n",
            "Epoch 226/500\n",
            "58/58 [==============================] - 11s 192ms/step - loss: 0.0115 - accuracy: 0.9967 - val_loss: 0.3149 - val_accuracy: 0.9362\n",
            "Epoch 227/500\n",
            "58/58 [==============================] - 11s 193ms/step - loss: 0.0375 - accuracy: 0.9876 - val_loss: 0.2867 - val_accuracy: 0.9386\n",
            "Epoch 228/500\n",
            "58/58 [==============================] - 10s 171ms/step - loss: 0.0138 - accuracy: 0.9954 - val_loss: 0.3407 - val_accuracy: 0.9413\n",
            "Epoch 229/500\n",
            "58/58 [==============================] - 10s 176ms/step - loss: 0.0205 - accuracy: 0.9948 - val_loss: 0.2909 - val_accuracy: 0.9382\n",
            "Epoch 230/500\n",
            "58/58 [==============================] - 11s 191ms/step - loss: 0.0152 - accuracy: 0.9942 - val_loss: 0.2988 - val_accuracy: 0.9427\n",
            "Epoch 231/500\n",
            "58/58 [==============================] - 11s 191ms/step - loss: 0.0203 - accuracy: 0.9943 - val_loss: 0.2870 - val_accuracy: 0.9433\n",
            "Epoch 232/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0231 - accuracy: 0.9924 - val_loss: 0.3241 - val_accuracy: 0.9386\n",
            "Epoch 233/500\n",
            "58/58 [==============================] - 10s 173ms/step - loss: 0.0183 - accuracy: 0.9950 - val_loss: 0.3381 - val_accuracy: 0.9416\n",
            "Epoch 234/500\n",
            "58/58 [==============================] - 10s 174ms/step - loss: 0.0307 - accuracy: 0.9906 - val_loss: 0.3227 - val_accuracy: 0.9345\n",
            "Epoch 235/500\n",
            "58/58 [==============================] - 11s 192ms/step - loss: 0.0158 - accuracy: 0.9948 - val_loss: 0.3571 - val_accuracy: 0.9348\n",
            "Epoch 236/500\n",
            "58/58 [==============================] - 11s 196ms/step - loss: 0.0227 - accuracy: 0.9937 - val_loss: 0.2964 - val_accuracy: 0.9342\n",
            "Epoch 237/500\n",
            "58/58 [==============================] - 11s 194ms/step - loss: 0.0486 - accuracy: 0.9868 - val_loss: 0.2923 - val_accuracy: 0.9308\n",
            "Epoch 238/500\n",
            "58/58 [==============================] - 11s 193ms/step - loss: 0.0162 - accuracy: 0.9956 - val_loss: 0.3378 - val_accuracy: 0.9396\n",
            "Epoch 239/500\n",
            "58/58 [==============================] - 10s 170ms/step - loss: 0.0180 - accuracy: 0.9944 - val_loss: 0.2981 - val_accuracy: 0.9372\n",
            "Epoch 240/500\n",
            "58/58 [==============================] - 11s 192ms/step - loss: 0.0143 - accuracy: 0.9958 - val_loss: 0.3057 - val_accuracy: 0.9352\n",
            "Epoch 241/500\n",
            "58/58 [==============================] - 11s 196ms/step - loss: 0.0163 - accuracy: 0.9952 - val_loss: 0.3327 - val_accuracy: 0.9328\n",
            "Epoch 242/500\n",
            "58/58 [==============================] - 11s 196ms/step - loss: 0.0158 - accuracy: 0.9952 - val_loss: 0.2924 - val_accuracy: 0.9389\n",
            "Epoch 243/500\n",
            "58/58 [==============================] - 11s 196ms/step - loss: 0.0128 - accuracy: 0.9965 - val_loss: 0.2803 - val_accuracy: 0.9393\n",
            "Epoch 244/500\n",
            "58/58 [==============================] - 11s 197ms/step - loss: 0.0243 - accuracy: 0.9935 - val_loss: 0.3768 - val_accuracy: 0.9311\n",
            "Epoch 245/500\n",
            "58/58 [==============================] - 9s 164ms/step - loss: 0.0193 - accuracy: 0.9946 - val_loss: 0.3379 - val_accuracy: 0.9427\n",
            "Epoch 246/500\n",
            "58/58 [==============================] - 11s 194ms/step - loss: 0.0135 - accuracy: 0.9950 - val_loss: 0.3258 - val_accuracy: 0.9338\n",
            "Epoch 247/500\n",
            "58/58 [==============================] - 11s 193ms/step - loss: 0.0602 - accuracy: 0.9822 - val_loss: 0.4118 - val_accuracy: 0.9084\n",
            "Epoch 248/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0820 - accuracy: 0.9737 - val_loss: 0.2919 - val_accuracy: 0.9403\n",
            "Epoch 249/500\n",
            "58/58 [==============================] - 12s 207ms/step - loss: 0.0148 - accuracy: 0.9961 - val_loss: 0.2954 - val_accuracy: 0.9379\n",
            "Epoch 250/500\n",
            "58/58 [==============================] - 10s 174ms/step - loss: 0.0127 - accuracy: 0.9952 - val_loss: 0.3664 - val_accuracy: 0.9348\n",
            "Epoch 251/500\n",
            "58/58 [==============================] - 10s 176ms/step - loss: 0.0153 - accuracy: 0.9955 - val_loss: 0.2737 - val_accuracy: 0.9406\n",
            "Epoch 252/500\n",
            "58/58 [==============================] - 11s 191ms/step - loss: 0.0201 - accuracy: 0.9932 - val_loss: 0.3286 - val_accuracy: 0.9386\n",
            "Epoch 253/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0143 - accuracy: 0.9950 - val_loss: 0.3543 - val_accuracy: 0.9396\n",
            "Epoch 254/500\n",
            "58/58 [==============================] - 11s 193ms/step - loss: 0.0144 - accuracy: 0.9958 - val_loss: 0.3477 - val_accuracy: 0.9338\n",
            "Epoch 255/500\n",
            "58/58 [==============================] - 11s 192ms/step - loss: 0.0183 - accuracy: 0.9948 - val_loss: 0.3343 - val_accuracy: 0.9345\n",
            "Epoch 256/500\n",
            "58/58 [==============================] - 10s 170ms/step - loss: 0.0184 - accuracy: 0.9948 - val_loss: 0.2824 - val_accuracy: 0.9345\n",
            "Epoch 257/500\n",
            "58/58 [==============================] - 11s 196ms/step - loss: 0.0112 - accuracy: 0.9970 - val_loss: 0.2847 - val_accuracy: 0.9382\n",
            "Epoch 258/500\n",
            "58/58 [==============================] - 11s 194ms/step - loss: 0.0249 - accuracy: 0.9924 - val_loss: 0.2588 - val_accuracy: 0.9440\n",
            "Epoch 259/500\n",
            "58/58 [==============================] - 11s 193ms/step - loss: 0.0244 - accuracy: 0.9921 - val_loss: 0.3508 - val_accuracy: 0.9332\n",
            "Epoch 260/500\n",
            "58/58 [==============================] - 11s 194ms/step - loss: 0.0197 - accuracy: 0.9935 - val_loss: 0.3113 - val_accuracy: 0.9396\n",
            "Epoch 261/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0110 - accuracy: 0.9970 - val_loss: 0.3388 - val_accuracy: 0.9379\n",
            "Epoch 262/500\n",
            "58/58 [==============================] - 10s 169ms/step - loss: 0.0152 - accuracy: 0.9952 - val_loss: 0.3605 - val_accuracy: 0.9260\n",
            "Epoch 263/500\n",
            "58/58 [==============================] - 11s 189ms/step - loss: 0.0209 - accuracy: 0.9936 - val_loss: 0.3142 - val_accuracy: 0.9342\n",
            "Epoch 264/500\n",
            "58/58 [==============================] - 11s 188ms/step - loss: 0.0107 - accuracy: 0.9973 - val_loss: 0.3856 - val_accuracy: 0.9311\n",
            "Epoch 265/500\n",
            "58/58 [==============================] - 11s 188ms/step - loss: 0.0112 - accuracy: 0.9966 - val_loss: 0.3431 - val_accuracy: 0.9359\n",
            "Epoch 266/500\n",
            "58/58 [==============================] - 11s 191ms/step - loss: 0.0090 - accuracy: 0.9966 - val_loss: 0.3439 - val_accuracy: 0.9362\n",
            "Epoch 267/500\n",
            "58/58 [==============================] - 9s 161ms/step - loss: 0.0136 - accuracy: 0.9971 - val_loss: 0.3606 - val_accuracy: 0.9362\n",
            "Epoch 268/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0193 - accuracy: 0.9939 - val_loss: 0.3759 - val_accuracy: 0.9321\n",
            "Epoch 269/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0188 - accuracy: 0.9948 - val_loss: 0.2618 - val_accuracy: 0.9396\n",
            "Epoch 270/500\n",
            "58/58 [==============================] - 11s 188ms/step - loss: 0.0167 - accuracy: 0.9951 - val_loss: 0.3692 - val_accuracy: 0.9342\n",
            "Epoch 271/500\n",
            "58/58 [==============================] - 11s 198ms/step - loss: 0.0174 - accuracy: 0.9947 - val_loss: 0.2846 - val_accuracy: 0.9440\n",
            "Epoch 272/500\n",
            "58/58 [==============================] - 10s 167ms/step - loss: 0.0273 - accuracy: 0.9932 - val_loss: 0.3232 - val_accuracy: 0.9311\n",
            "Epoch 273/500\n",
            "58/58 [==============================] - 11s 187ms/step - loss: 0.0100 - accuracy: 0.9973 - val_loss: 0.3974 - val_accuracy: 0.9335\n",
            "Epoch 274/500\n",
            "58/58 [==============================] - 11s 191ms/step - loss: 0.0138 - accuracy: 0.9967 - val_loss: 0.3259 - val_accuracy: 0.9389\n",
            "Epoch 275/500\n",
            "58/58 [==============================] - 11s 192ms/step - loss: 0.0153 - accuracy: 0.9951 - val_loss: 0.3852 - val_accuracy: 0.9325\n",
            "Epoch 276/500\n",
            "58/58 [==============================] - 11s 191ms/step - loss: 0.0118 - accuracy: 0.9966 - val_loss: 0.3318 - val_accuracy: 0.9355\n",
            "Epoch 277/500\n",
            "58/58 [==============================] - 11s 189ms/step - loss: 0.0172 - accuracy: 0.9940 - val_loss: 0.3254 - val_accuracy: 0.9362\n",
            "Epoch 278/500\n",
            "58/58 [==============================] - 10s 165ms/step - loss: 0.0140 - accuracy: 0.9958 - val_loss: 0.3953 - val_accuracy: 0.9315\n",
            "Epoch 279/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0086 - accuracy: 0.9973 - val_loss: 0.4202 - val_accuracy: 0.9253\n",
            "Epoch 280/500\n",
            "58/58 [==============================] - 12s 215ms/step - loss: 0.0184 - accuracy: 0.9954 - val_loss: 0.3093 - val_accuracy: 0.9325\n",
            "Epoch 281/500\n",
            "58/58 [==============================] - 11s 193ms/step - loss: 0.0323 - accuracy: 0.9903 - val_loss: 0.3370 - val_accuracy: 0.9345\n",
            "Epoch 282/500\n",
            "58/58 [==============================] - 11s 194ms/step - loss: 0.0215 - accuracy: 0.9947 - val_loss: 0.3337 - val_accuracy: 0.9389\n",
            "Epoch 283/500\n",
            "58/58 [==============================] - 11s 188ms/step - loss: 0.0149 - accuracy: 0.9962 - val_loss: 0.3381 - val_accuracy: 0.9389\n",
            "Epoch 284/500\n",
            "58/58 [==============================] - 10s 173ms/step - loss: 0.0109 - accuracy: 0.9967 - val_loss: 0.3776 - val_accuracy: 0.9359\n",
            "Epoch 285/500\n",
            "58/58 [==============================] - 11s 194ms/step - loss: 0.0182 - accuracy: 0.9943 - val_loss: 0.3510 - val_accuracy: 0.9362\n",
            "Epoch 286/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0129 - accuracy: 0.9963 - val_loss: 0.3568 - val_accuracy: 0.9413\n",
            "Epoch 287/500\n",
            "58/58 [==============================] - 11s 192ms/step - loss: 0.0115 - accuracy: 0.9969 - val_loss: 0.3722 - val_accuracy: 0.9362\n",
            "Epoch 288/500\n",
            "58/58 [==============================] - 11s 192ms/step - loss: 0.0099 - accuracy: 0.9977 - val_loss: 0.3546 - val_accuracy: 0.9328\n",
            "Epoch 289/500\n",
            "58/58 [==============================] - 11s 189ms/step - loss: 0.0178 - accuracy: 0.9944 - val_loss: 0.3518 - val_accuracy: 0.9321\n",
            "Epoch 290/500\n",
            "58/58 [==============================] - 10s 176ms/step - loss: 0.0188 - accuracy: 0.9943 - val_loss: 0.4678 - val_accuracy: 0.9074\n",
            "Epoch 291/500\n",
            "58/58 [==============================] - 11s 194ms/step - loss: 0.0410 - accuracy: 0.9891 - val_loss: 0.3546 - val_accuracy: 0.9321\n",
            "Epoch 292/500\n",
            "58/58 [==============================] - 11s 193ms/step - loss: 0.0279 - accuracy: 0.9925 - val_loss: 0.4303 - val_accuracy: 0.9237\n",
            "Epoch 293/500\n",
            "58/58 [==============================] - 11s 195ms/step - loss: 0.0211 - accuracy: 0.9950 - val_loss: 0.3614 - val_accuracy: 0.9328\n",
            "Epoch 294/500\n",
            "58/58 [==============================] - 12s 211ms/step - loss: 0.0138 - accuracy: 0.9959 - val_loss: 0.3712 - val_accuracy: 0.9335\n",
            "Epoch 295/500\n",
            "58/58 [==============================] - 10s 182ms/step - loss: 0.0150 - accuracy: 0.9958 - val_loss: 0.3784 - val_accuracy: 0.9304\n",
            "Epoch 296/500\n",
            "58/58 [==============================] - 10s 172ms/step - loss: 0.0174 - accuracy: 0.9951 - val_loss: 0.3403 - val_accuracy: 0.9301\n",
            "Epoch 297/500\n",
            "58/58 [==============================] - 11s 192ms/step - loss: 0.0116 - accuracy: 0.9967 - val_loss: 0.4072 - val_accuracy: 0.9291\n",
            "Epoch 298/500\n",
            "58/58 [==============================] - 11s 195ms/step - loss: 0.0092 - accuracy: 0.9985 - val_loss: 0.3883 - val_accuracy: 0.9328\n",
            "Epoch 299/500\n",
            "58/58 [==============================] - 11s 198ms/step - loss: 0.0150 - accuracy: 0.9961 - val_loss: 0.3356 - val_accuracy: 0.9270\n",
            "Epoch 300/500\n",
            "58/58 [==============================] - 13s 221ms/step - loss: 0.0180 - accuracy: 0.9943 - val_loss: 0.3573 - val_accuracy: 0.9281\n",
            "Epoch 301/500\n",
            "58/58 [==============================] - 13s 218ms/step - loss: 0.0106 - accuracy: 0.9971 - val_loss: 0.3948 - val_accuracy: 0.9315\n",
            "Epoch 302/500\n",
            "58/58 [==============================] - 12s 209ms/step - loss: 0.0189 - accuracy: 0.9959 - val_loss: 0.3610 - val_accuracy: 0.9321\n",
            "Epoch 303/500\n",
            "58/58 [==============================] - 11s 188ms/step - loss: 0.0381 - accuracy: 0.9890 - val_loss: 0.3443 - val_accuracy: 0.9315\n",
            "Epoch 304/500\n",
            "58/58 [==============================] - 13s 218ms/step - loss: 0.0191 - accuracy: 0.9950 - val_loss: 0.3593 - val_accuracy: 0.9325\n",
            "Epoch 305/500\n",
            "58/58 [==============================] - 12s 201ms/step - loss: 0.0105 - accuracy: 0.9976 - val_loss: 0.4227 - val_accuracy: 0.9253\n",
            "Epoch 306/500\n",
            "58/58 [==============================] - 12s 202ms/step - loss: 0.0199 - accuracy: 0.9933 - val_loss: 0.3115 - val_accuracy: 0.9348\n",
            "Epoch 307/500\n",
            "58/58 [==============================] - 12s 204ms/step - loss: 0.0187 - accuracy: 0.9944 - val_loss: 0.3571 - val_accuracy: 0.9345\n",
            "Epoch 308/500\n",
            "58/58 [==============================] - 12s 200ms/step - loss: 0.0101 - accuracy: 0.9976 - val_loss: 0.3948 - val_accuracy: 0.9362\n",
            "Epoch 309/500\n",
            "58/58 [==============================] - 11s 195ms/step - loss: 0.0247 - accuracy: 0.9935 - val_loss: 0.3362 - val_accuracy: 0.9359\n",
            "Epoch 310/500\n",
            "58/58 [==============================] - 10s 172ms/step - loss: 0.0190 - accuracy: 0.9952 - val_loss: 0.3378 - val_accuracy: 0.9348\n",
            "Epoch 311/500\n",
            "58/58 [==============================] - 11s 195ms/step - loss: 0.0164 - accuracy: 0.9962 - val_loss: 0.4330 - val_accuracy: 0.9281\n",
            "Epoch 312/500\n",
            "58/58 [==============================] - 11s 196ms/step - loss: 0.0145 - accuracy: 0.9969 - val_loss: 0.3504 - val_accuracy: 0.9315\n",
            "Epoch 313/500\n",
            "58/58 [==============================] - 11s 193ms/step - loss: 0.0094 - accuracy: 0.9973 - val_loss: 0.4212 - val_accuracy: 0.9287\n",
            "Epoch 314/500\n",
            "58/58 [==============================] - 11s 192ms/step - loss: 0.0103 - accuracy: 0.9967 - val_loss: 0.3638 - val_accuracy: 0.9338\n",
            "Epoch 315/500\n",
            "58/58 [==============================] - 11s 193ms/step - loss: 0.0173 - accuracy: 0.9947 - val_loss: 0.2837 - val_accuracy: 0.9376\n",
            "Epoch 316/500\n",
            "58/58 [==============================] - 10s 167ms/step - loss: 0.0551 - accuracy: 0.9839 - val_loss: 0.4770 - val_accuracy: 0.9114\n",
            "Epoch 317/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0524 - accuracy: 0.9844 - val_loss: 0.3420 - val_accuracy: 0.9372\n",
            "Epoch 318/500\n",
            "58/58 [==============================] - 11s 195ms/step - loss: 0.0250 - accuracy: 0.9947 - val_loss: 0.3717 - val_accuracy: 0.9298\n",
            "Epoch 319/500\n",
            "58/58 [==============================] - 11s 194ms/step - loss: 0.0126 - accuracy: 0.9967 - val_loss: 0.4111 - val_accuracy: 0.9277\n",
            "Epoch 320/500\n",
            "58/58 [==============================] - 12s 214ms/step - loss: 0.0166 - accuracy: 0.9966 - val_loss: 0.3633 - val_accuracy: 0.9325\n",
            "Epoch 321/500\n",
            "58/58 [==============================] - 11s 183ms/step - loss: 0.0128 - accuracy: 0.9971 - val_loss: 0.3334 - val_accuracy: 0.9342\n",
            "Epoch 322/500\n",
            "58/58 [==============================] - 10s 170ms/step - loss: 0.0117 - accuracy: 0.9970 - val_loss: 0.3217 - val_accuracy: 0.9376\n",
            "Epoch 323/500\n",
            "58/58 [==============================] - 11s 191ms/step - loss: 0.0092 - accuracy: 0.9973 - val_loss: 0.3540 - val_accuracy: 0.9372\n",
            "Epoch 324/500\n",
            "58/58 [==============================] - 11s 191ms/step - loss: 0.0229 - accuracy: 0.9943 - val_loss: 0.3804 - val_accuracy: 0.9223\n",
            "Epoch 325/500\n",
            "58/58 [==============================] - 11s 189ms/step - loss: 0.0288 - accuracy: 0.9921 - val_loss: 0.3486 - val_accuracy: 0.9352\n",
            "Epoch 326/500\n",
            "58/58 [==============================] - 12s 199ms/step - loss: 0.0251 - accuracy: 0.9927 - val_loss: 0.3007 - val_accuracy: 0.9321\n",
            "Epoch 327/500\n",
            "58/58 [==============================] - 11s 182ms/step - loss: 0.0149 - accuracy: 0.9959 - val_loss: 0.3594 - val_accuracy: 0.9301\n",
            "Epoch 328/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0098 - accuracy: 0.9971 - val_loss: 0.3699 - val_accuracy: 0.9345\n",
            "Epoch 329/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0116 - accuracy: 0.9970 - val_loss: 0.3532 - val_accuracy: 0.9382\n",
            "Epoch 330/500\n",
            "58/58 [==============================] - 11s 189ms/step - loss: 0.0167 - accuracy: 0.9950 - val_loss: 0.3502 - val_accuracy: 0.9359\n",
            "Epoch 331/500\n",
            "58/58 [==============================] - 11s 189ms/step - loss: 0.0123 - accuracy: 0.9971 - val_loss: 0.3524 - val_accuracy: 0.9355\n",
            "Epoch 332/500\n",
            "58/58 [==============================] - 10s 166ms/step - loss: 0.0151 - accuracy: 0.9959 - val_loss: 0.3073 - val_accuracy: 0.9427\n",
            "Epoch 333/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0053 - accuracy: 0.9990 - val_loss: 0.3787 - val_accuracy: 0.9396\n",
            "Epoch 334/500\n",
            "58/58 [==============================] - 11s 193ms/step - loss: 0.0063 - accuracy: 0.9982 - val_loss: 0.3867 - val_accuracy: 0.9359\n",
            "Epoch 335/500\n",
            "58/58 [==============================] - 11s 197ms/step - loss: 0.0113 - accuracy: 0.9969 - val_loss: 0.3211 - val_accuracy: 0.9372\n",
            "Epoch 336/500\n",
            "58/58 [==============================] - 11s 191ms/step - loss: 0.0103 - accuracy: 0.9970 - val_loss: 0.2897 - val_accuracy: 0.9406\n",
            "Epoch 337/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0066 - accuracy: 0.9986 - val_loss: 0.3607 - val_accuracy: 0.9379\n",
            "Epoch 338/500\n",
            "58/58 [==============================] - 10s 167ms/step - loss: 0.0101 - accuracy: 0.9974 - val_loss: 0.3524 - val_accuracy: 0.9379\n",
            "Epoch 339/500\n",
            "58/58 [==============================] - 11s 193ms/step - loss: 0.0109 - accuracy: 0.9969 - val_loss: 0.3385 - val_accuracy: 0.9413\n",
            "Epoch 340/500\n",
            "58/58 [==============================] - 11s 194ms/step - loss: 0.0164 - accuracy: 0.9958 - val_loss: 0.3285 - val_accuracy: 0.9352\n",
            "Epoch 341/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0115 - accuracy: 0.9970 - val_loss: 0.3534 - val_accuracy: 0.9386\n",
            "Epoch 342/500\n",
            "58/58 [==============================] - 12s 209ms/step - loss: 0.0198 - accuracy: 0.9942 - val_loss: 0.2879 - val_accuracy: 0.9342\n",
            "Epoch 343/500\n",
            "58/58 [==============================] - 12s 205ms/step - loss: 0.0193 - accuracy: 0.9943 - val_loss: 0.3228 - val_accuracy: 0.9382\n",
            "Epoch 344/500\n",
            "58/58 [==============================] - 12s 201ms/step - loss: 0.0097 - accuracy: 0.9974 - val_loss: 0.3791 - val_accuracy: 0.9335\n",
            "Epoch 345/500\n",
            "58/58 [==============================] - 11s 184ms/step - loss: 0.0133 - accuracy: 0.9974 - val_loss: 0.3073 - val_accuracy: 0.9430\n",
            "Epoch 346/500\n",
            "58/58 [==============================] - 13s 223ms/step - loss: 0.0135 - accuracy: 0.9970 - val_loss: 0.3211 - val_accuracy: 0.9379\n",
            "Epoch 347/500\n",
            "58/58 [==============================] - 13s 224ms/step - loss: 0.0419 - accuracy: 0.9884 - val_loss: 0.3208 - val_accuracy: 0.9155\n",
            "Epoch 348/500\n",
            "58/58 [==============================] - 12s 201ms/step - loss: 0.0329 - accuracy: 0.9913 - val_loss: 0.2845 - val_accuracy: 0.9365\n",
            "Epoch 349/500\n",
            "58/58 [==============================] - 13s 225ms/step - loss: 0.0105 - accuracy: 0.9973 - val_loss: 0.3795 - val_accuracy: 0.9342\n",
            "Epoch 350/500\n",
            "58/58 [==============================] - 12s 199ms/step - loss: 0.0184 - accuracy: 0.9948 - val_loss: 0.3203 - val_accuracy: 0.9365\n",
            "Epoch 351/500\n",
            "58/58 [==============================] - 11s 197ms/step - loss: 0.0144 - accuracy: 0.9963 - val_loss: 0.3521 - val_accuracy: 0.9376\n",
            "Epoch 352/500\n",
            "58/58 [==============================] - 11s 197ms/step - loss: 0.0099 - accuracy: 0.9963 - val_loss: 0.3929 - val_accuracy: 0.9399\n",
            "Epoch 353/500\n",
            "58/58 [==============================] - 11s 194ms/step - loss: 0.0178 - accuracy: 0.9948 - val_loss: 0.2632 - val_accuracy: 0.9365\n",
            "Epoch 354/500\n",
            "58/58 [==============================] - 10s 167ms/step - loss: 0.0145 - accuracy: 0.9948 - val_loss: 0.3358 - val_accuracy: 0.9379\n",
            "Epoch 355/500\n",
            "58/58 [==============================] - 11s 192ms/step - loss: 0.0156 - accuracy: 0.9952 - val_loss: 0.3223 - val_accuracy: 0.9267\n",
            "Epoch 356/500\n",
            "58/58 [==============================] - 11s 196ms/step - loss: 0.0121 - accuracy: 0.9963 - val_loss: 0.3714 - val_accuracy: 0.9379\n",
            "Epoch 357/500\n",
            "58/58 [==============================] - 11s 194ms/step - loss: 0.0083 - accuracy: 0.9980 - val_loss: 0.3541 - val_accuracy: 0.9325\n",
            "Epoch 358/500\n",
            "58/58 [==============================] - 11s 193ms/step - loss: 0.0116 - accuracy: 0.9963 - val_loss: 0.3958 - val_accuracy: 0.9332\n",
            "Epoch 359/500\n",
            "58/58 [==============================] - 10s 179ms/step - loss: 0.0066 - accuracy: 0.9982 - val_loss: 0.4069 - val_accuracy: 0.9311\n",
            "Epoch 360/500\n",
            "58/58 [==============================] - 10s 171ms/step - loss: 0.0039 - accuracy: 0.9993 - val_loss: 0.3922 - val_accuracy: 0.9332\n",
            "Epoch 361/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0063 - accuracy: 0.9988 - val_loss: 0.3396 - val_accuracy: 0.9359\n",
            "Epoch 362/500\n",
            "58/58 [==============================] - 11s 187ms/step - loss: 0.0134 - accuracy: 0.9966 - val_loss: 0.3500 - val_accuracy: 0.9369\n",
            "Epoch 363/500\n",
            "58/58 [==============================] - 11s 188ms/step - loss: 0.0084 - accuracy: 0.9978 - val_loss: 0.3641 - val_accuracy: 0.9362\n",
            "Epoch 364/500\n",
            "58/58 [==============================] - 11s 185ms/step - loss: 0.0081 - accuracy: 0.9986 - val_loss: 0.3712 - val_accuracy: 0.9315\n",
            "Epoch 365/500\n",
            "58/58 [==============================] - 10s 166ms/step - loss: 0.0142 - accuracy: 0.9967 - val_loss: 0.3565 - val_accuracy: 0.9359\n",
            "Epoch 366/500\n",
            "58/58 [==============================] - 11s 188ms/step - loss: 0.0270 - accuracy: 0.9940 - val_loss: 0.3804 - val_accuracy: 0.9342\n",
            "Epoch 367/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0232 - accuracy: 0.9929 - val_loss: 0.3770 - val_accuracy: 0.9342\n",
            "Epoch 368/500\n",
            "58/58 [==============================] - 11s 188ms/step - loss: 0.0143 - accuracy: 0.9970 - val_loss: 0.3047 - val_accuracy: 0.9332\n",
            "Epoch 369/500\n",
            "58/58 [==============================] - 12s 210ms/step - loss: 0.0242 - accuracy: 0.9929 - val_loss: 0.3093 - val_accuracy: 0.9382\n",
            "Epoch 370/500\n",
            "58/58 [==============================] - 11s 195ms/step - loss: 0.0229 - accuracy: 0.9947 - val_loss: 0.3232 - val_accuracy: 0.9382\n",
            "Epoch 371/500\n",
            "58/58 [==============================] - 9s 164ms/step - loss: 0.0087 - accuracy: 0.9985 - val_loss: 0.3570 - val_accuracy: 0.9382\n",
            "Epoch 372/500\n",
            "58/58 [==============================] - 13s 218ms/step - loss: 0.0059 - accuracy: 0.9988 - val_loss: 0.3901 - val_accuracy: 0.9345\n",
            "Epoch 373/500\n",
            "58/58 [==============================] - 11s 194ms/step - loss: 0.0123 - accuracy: 0.9965 - val_loss: 0.2931 - val_accuracy: 0.9413\n",
            "Epoch 374/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0279 - accuracy: 0.9917 - val_loss: 0.2845 - val_accuracy: 0.9444\n",
            "Epoch 375/500\n",
            "58/58 [==============================] - 11s 192ms/step - loss: 0.0128 - accuracy: 0.9962 - val_loss: 0.3556 - val_accuracy: 0.9355\n",
            "Epoch 376/500\n",
            "58/58 [==============================] - 11s 189ms/step - loss: 0.0191 - accuracy: 0.9951 - val_loss: 0.2692 - val_accuracy: 0.9376\n",
            "Epoch 377/500\n",
            "58/58 [==============================] - 10s 171ms/step - loss: 0.0125 - accuracy: 0.9970 - val_loss: 0.3513 - val_accuracy: 0.9393\n",
            "Epoch 378/500\n",
            "58/58 [==============================] - 10s 176ms/step - loss: 0.0072 - accuracy: 0.9977 - val_loss: 0.3659 - val_accuracy: 0.9362\n",
            "Epoch 379/500\n",
            "58/58 [==============================] - 11s 187ms/step - loss: 0.0136 - accuracy: 0.9973 - val_loss: 0.4465 - val_accuracy: 0.9179\n",
            "Epoch 380/500\n",
            "58/58 [==============================] - 11s 189ms/step - loss: 0.0076 - accuracy: 0.9988 - val_loss: 0.3842 - val_accuracy: 0.9369\n",
            "Epoch 381/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0133 - accuracy: 0.9977 - val_loss: 0.3579 - val_accuracy: 0.9345\n",
            "Epoch 382/500\n",
            "58/58 [==============================] - 10s 178ms/step - loss: 0.0106 - accuracy: 0.9971 - val_loss: 0.3139 - val_accuracy: 0.9321\n",
            "Epoch 383/500\n",
            "58/58 [==============================] - 10s 169ms/step - loss: 0.0102 - accuracy: 0.9971 - val_loss: 0.3616 - val_accuracy: 0.9369\n",
            "Epoch 384/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0067 - accuracy: 0.9986 - val_loss: 0.3429 - val_accuracy: 0.9365\n",
            "Epoch 385/500\n",
            "58/58 [==============================] - 11s 191ms/step - loss: 0.0193 - accuracy: 0.9946 - val_loss: 0.3228 - val_accuracy: 0.9396\n",
            "Epoch 386/500\n",
            "58/58 [==============================] - 11s 193ms/step - loss: 0.0106 - accuracy: 0.9970 - val_loss: 0.3506 - val_accuracy: 0.9379\n",
            "Epoch 387/500\n",
            "58/58 [==============================] - 12s 210ms/step - loss: 0.0093 - accuracy: 0.9977 - val_loss: 0.3652 - val_accuracy: 0.9355\n",
            "Epoch 388/500\n",
            "58/58 [==============================] - 10s 180ms/step - loss: 0.0068 - accuracy: 0.9985 - val_loss: 0.3758 - val_accuracy: 0.9379\n",
            "Epoch 389/500\n",
            "58/58 [==============================] - 10s 173ms/step - loss: 0.0108 - accuracy: 0.9973 - val_loss: 0.3407 - val_accuracy: 0.9403\n",
            "Epoch 390/500\n",
            "58/58 [==============================] - 11s 191ms/step - loss: 0.0071 - accuracy: 0.9984 - val_loss: 0.3340 - val_accuracy: 0.9399\n",
            "Epoch 391/500\n",
            "58/58 [==============================] - 11s 189ms/step - loss: 0.0147 - accuracy: 0.9962 - val_loss: 0.3240 - val_accuracy: 0.9332\n",
            "Epoch 392/500\n",
            "58/58 [==============================] - 11s 191ms/step - loss: 0.0139 - accuracy: 0.9965 - val_loss: 0.3623 - val_accuracy: 0.9403\n",
            "Epoch 393/500\n",
            "58/58 [==============================] - 12s 205ms/step - loss: 0.0120 - accuracy: 0.9967 - val_loss: 0.3235 - val_accuracy: 0.9433\n",
            "Epoch 394/500\n",
            "58/58 [==============================] - 11s 191ms/step - loss: 0.0276 - accuracy: 0.9942 - val_loss: 0.3076 - val_accuracy: 0.9427\n",
            "Epoch 395/500\n",
            "58/58 [==============================] - 12s 203ms/step - loss: 0.0088 - accuracy: 0.9971 - val_loss: 0.3317 - val_accuracy: 0.9447\n",
            "Epoch 396/500\n",
            "58/58 [==============================] - 10s 174ms/step - loss: 0.0064 - accuracy: 0.9989 - val_loss: 0.3410 - val_accuracy: 0.9440\n",
            "Epoch 397/500\n",
            "58/58 [==============================] - 11s 191ms/step - loss: 0.0157 - accuracy: 0.9966 - val_loss: 0.3133 - val_accuracy: 0.9423\n",
            "Epoch 398/500\n",
            "58/58 [==============================] - 11s 189ms/step - loss: 0.0156 - accuracy: 0.9966 - val_loss: 0.3403 - val_accuracy: 0.9389\n",
            "Epoch 399/500\n",
            "58/58 [==============================] - 11s 189ms/step - loss: 0.0296 - accuracy: 0.9909 - val_loss: 0.2780 - val_accuracy: 0.9423\n",
            "Epoch 400/500\n",
            "58/58 [==============================] - 12s 202ms/step - loss: 0.0090 - accuracy: 0.9981 - val_loss: 0.3000 - val_accuracy: 0.9416\n",
            "Epoch 401/500\n",
            "58/58 [==============================] - 11s 191ms/step - loss: 0.0089 - accuracy: 0.9986 - val_loss: 0.3448 - val_accuracy: 0.9399\n",
            "Epoch 402/500\n",
            "58/58 [==============================] - 10s 165ms/step - loss: 0.0065 - accuracy: 0.9988 - val_loss: 0.3100 - val_accuracy: 0.9416\n",
            "Epoch 403/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0221 - accuracy: 0.9947 - val_loss: 0.3472 - val_accuracy: 0.9386\n",
            "Epoch 404/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0116 - accuracy: 0.9980 - val_loss: 0.3239 - val_accuracy: 0.9328\n",
            "Epoch 405/500\n",
            "58/58 [==============================] - 11s 196ms/step - loss: 0.0166 - accuracy: 0.9962 - val_loss: 0.3487 - val_accuracy: 0.9352\n",
            "Epoch 406/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0154 - accuracy: 0.9961 - val_loss: 0.3016 - val_accuracy: 0.9376\n",
            "Epoch 407/500\n",
            "58/58 [==============================] - 10s 172ms/step - loss: 0.0118 - accuracy: 0.9971 - val_loss: 0.3093 - val_accuracy: 0.9376\n",
            "Epoch 408/500\n",
            "58/58 [==============================] - 10s 177ms/step - loss: 0.0162 - accuracy: 0.9956 - val_loss: 0.3031 - val_accuracy: 0.9372\n",
            "Epoch 409/500\n",
            "58/58 [==============================] - 11s 192ms/step - loss: 0.0193 - accuracy: 0.9952 - val_loss: 0.3569 - val_accuracy: 0.9291\n",
            "Epoch 410/500\n",
            "58/58 [==============================] - 11s 194ms/step - loss: 0.0173 - accuracy: 0.9961 - val_loss: 0.3421 - val_accuracy: 0.9243\n",
            "Epoch 411/500\n",
            "58/58 [==============================] - 11s 194ms/step - loss: 0.0135 - accuracy: 0.9967 - val_loss: 0.3719 - val_accuracy: 0.9304\n",
            "Epoch 412/500\n",
            "58/58 [==============================] - 12s 207ms/step - loss: 0.0146 - accuracy: 0.9956 - val_loss: 0.3060 - val_accuracy: 0.9328\n",
            "Epoch 413/500\n",
            "58/58 [==============================] - 10s 174ms/step - loss: 0.0139 - accuracy: 0.9959 - val_loss: 0.3828 - val_accuracy: 0.9298\n",
            "Epoch 414/500\n",
            "58/58 [==============================] - 10s 177ms/step - loss: 0.0051 - accuracy: 0.9988 - val_loss: 0.3943 - val_accuracy: 0.9325\n",
            "Epoch 415/500\n",
            "58/58 [==============================] - 11s 188ms/step - loss: 0.0176 - accuracy: 0.9950 - val_loss: 0.3218 - val_accuracy: 0.9420\n",
            "Epoch 416/500\n",
            "58/58 [==============================] - 11s 188ms/step - loss: 0.0193 - accuracy: 0.9954 - val_loss: 0.2841 - val_accuracy: 0.9410\n",
            "Epoch 417/500\n",
            "58/58 [==============================] - 11s 189ms/step - loss: 0.0125 - accuracy: 0.9970 - val_loss: 0.3503 - val_accuracy: 0.9399\n",
            "Epoch 418/500\n",
            "58/58 [==============================] - 12s 212ms/step - loss: 0.0057 - accuracy: 0.9990 - val_loss: 0.3996 - val_accuracy: 0.9365\n",
            "Epoch 419/500\n",
            "58/58 [==============================] - 11s 188ms/step - loss: 0.0124 - accuracy: 0.9969 - val_loss: 0.3588 - val_accuracy: 0.9342\n",
            "Epoch 420/500\n",
            "58/58 [==============================] - 10s 164ms/step - loss: 0.0109 - accuracy: 0.9974 - val_loss: 0.3259 - val_accuracy: 0.9345\n",
            "Epoch 421/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0093 - accuracy: 0.9984 - val_loss: 0.3271 - val_accuracy: 0.9369\n",
            "Epoch 422/500\n",
            "58/58 [==============================] - 11s 191ms/step - loss: 0.0093 - accuracy: 0.9978 - val_loss: 0.3215 - val_accuracy: 0.9444\n",
            "Epoch 423/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0083 - accuracy: 0.9980 - val_loss: 0.3713 - val_accuracy: 0.9345\n",
            "Epoch 424/500\n",
            "58/58 [==============================] - 11s 189ms/step - loss: 0.0076 - accuracy: 0.9980 - val_loss: 0.3696 - val_accuracy: 0.9338\n",
            "Epoch 425/500\n",
            "58/58 [==============================] - 10s 168ms/step - loss: 0.0043 - accuracy: 0.9995 - val_loss: 0.3907 - val_accuracy: 0.9342\n",
            "Epoch 426/500\n",
            "58/58 [==============================] - 11s 188ms/step - loss: 0.0084 - accuracy: 0.9982 - val_loss: 0.3348 - val_accuracy: 0.9430\n",
            "Epoch 427/500\n",
            "58/58 [==============================] - 11s 197ms/step - loss: 0.0104 - accuracy: 0.9969 - val_loss: 0.3410 - val_accuracy: 0.9423\n",
            "Epoch 428/500\n",
            "58/58 [==============================] - 11s 198ms/step - loss: 0.0207 - accuracy: 0.9942 - val_loss: 0.2830 - val_accuracy: 0.9430\n",
            "Epoch 429/500\n",
            "58/58 [==============================] - 11s 198ms/step - loss: 0.0169 - accuracy: 0.9952 - val_loss: 0.3061 - val_accuracy: 0.9372\n",
            "Epoch 430/500\n",
            "58/58 [==============================] - 13s 217ms/step - loss: 0.0302 - accuracy: 0.9932 - val_loss: 0.3099 - val_accuracy: 0.9444\n",
            "Epoch 431/500\n",
            "58/58 [==============================] - 12s 203ms/step - loss: 0.0158 - accuracy: 0.9963 - val_loss: 0.3161 - val_accuracy: 0.9406\n",
            "Epoch 432/500\n",
            "58/58 [==============================] - 11s 192ms/step - loss: 0.0215 - accuracy: 0.9956 - val_loss: 0.2619 - val_accuracy: 0.9399\n",
            "Epoch 433/500\n",
            "58/58 [==============================] - 10s 169ms/step - loss: 0.0098 - accuracy: 0.9977 - val_loss: 0.3328 - val_accuracy: 0.9413\n",
            "Epoch 434/500\n",
            "58/58 [==============================] - 11s 192ms/step - loss: 0.0117 - accuracy: 0.9971 - val_loss: 0.2909 - val_accuracy: 0.9420\n",
            "Epoch 435/500\n",
            "58/58 [==============================] - 11s 191ms/step - loss: 0.0102 - accuracy: 0.9982 - val_loss: 0.3291 - val_accuracy: 0.9359\n",
            "Epoch 436/500\n",
            "58/58 [==============================] - 11s 192ms/step - loss: 0.0126 - accuracy: 0.9970 - val_loss: 0.3402 - val_accuracy: 0.9342\n",
            "Epoch 437/500\n",
            "58/58 [==============================] - 11s 192ms/step - loss: 0.0112 - accuracy: 0.9971 - val_loss: 0.3471 - val_accuracy: 0.9379\n",
            "Epoch 438/500\n",
            "58/58 [==============================] - 10s 177ms/step - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.4073 - val_accuracy: 0.9332\n",
            "Epoch 439/500\n",
            "58/58 [==============================] - 11s 184ms/step - loss: 0.0105 - accuracy: 0.9978 - val_loss: 0.3696 - val_accuracy: 0.9352\n",
            "Epoch 440/500\n",
            "58/58 [==============================] - 11s 195ms/step - loss: 0.0131 - accuracy: 0.9970 - val_loss: 0.3562 - val_accuracy: 0.9352\n",
            "Epoch 441/500\n",
            "58/58 [==============================] - 13s 219ms/step - loss: 0.0133 - accuracy: 0.9973 - val_loss: 0.3101 - val_accuracy: 0.9355\n",
            "Epoch 442/500\n",
            "58/58 [==============================] - 11s 194ms/step - loss: 0.0199 - accuracy: 0.9939 - val_loss: 0.2875 - val_accuracy: 0.9416\n",
            "Epoch 443/500\n",
            "58/58 [==============================] - 11s 189ms/step - loss: 0.0182 - accuracy: 0.9944 - val_loss: 0.2370 - val_accuracy: 0.9420\n",
            "Epoch 444/500\n",
            "58/58 [==============================] - 11s 194ms/step - loss: 0.0148 - accuracy: 0.9954 - val_loss: 0.3464 - val_accuracy: 0.9410\n",
            "Epoch 445/500\n",
            "58/58 [==============================] - 9s 164ms/step - loss: 0.0169 - accuracy: 0.9948 - val_loss: 0.3147 - val_accuracy: 0.9372\n",
            "Epoch 446/500\n",
            "58/58 [==============================] - 11s 188ms/step - loss: 0.0094 - accuracy: 0.9978 - val_loss: 0.3619 - val_accuracy: 0.9399\n",
            "Epoch 447/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0059 - accuracy: 0.9988 - val_loss: 0.3863 - val_accuracy: 0.9403\n",
            "Epoch 448/500\n",
            "58/58 [==============================] - 11s 193ms/step - loss: 0.0175 - accuracy: 0.9947 - val_loss: 0.3215 - val_accuracy: 0.9348\n",
            "Epoch 449/500\n",
            "58/58 [==============================] - 11s 193ms/step - loss: 0.0112 - accuracy: 0.9976 - val_loss: 0.3818 - val_accuracy: 0.9352\n",
            "Epoch 450/500\n",
            "58/58 [==============================] - 11s 191ms/step - loss: 0.0079 - accuracy: 0.9978 - val_loss: 0.3905 - val_accuracy: 0.9325\n",
            "Epoch 451/500\n",
            "58/58 [==============================] - 10s 171ms/step - loss: 0.0146 - accuracy: 0.9946 - val_loss: 0.3534 - val_accuracy: 0.9332\n",
            "Epoch 452/500\n",
            "58/58 [==============================] - 11s 189ms/step - loss: 0.0313 - accuracy: 0.9902 - val_loss: 0.3528 - val_accuracy: 0.9318\n",
            "Epoch 453/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0079 - accuracy: 0.9980 - val_loss: 0.3995 - val_accuracy: 0.9335\n",
            "Epoch 454/500\n",
            "58/58 [==============================] - 11s 191ms/step - loss: 0.0035 - accuracy: 0.9996 - val_loss: 0.4262 - val_accuracy: 0.9328\n",
            "Epoch 455/500\n",
            "58/58 [==============================] - 11s 191ms/step - loss: 0.0051 - accuracy: 0.9990 - val_loss: 0.3824 - val_accuracy: 0.9325\n",
            "Epoch 456/500\n",
            "58/58 [==============================] - 11s 191ms/step - loss: 0.0134 - accuracy: 0.9962 - val_loss: 0.3463 - val_accuracy: 0.9365\n",
            "Epoch 457/500\n",
            "58/58 [==============================] - 10s 168ms/step - loss: 0.0102 - accuracy: 0.9977 - val_loss: 0.3766 - val_accuracy: 0.9335\n",
            "Epoch 458/500\n",
            "58/58 [==============================] - 11s 193ms/step - loss: 0.0225 - accuracy: 0.9954 - val_loss: 0.3766 - val_accuracy: 0.9365\n",
            "Epoch 459/500\n",
            "58/58 [==============================] - 11s 192ms/step - loss: 0.0111 - accuracy: 0.9981 - val_loss: 0.3794 - val_accuracy: 0.9355\n",
            "Epoch 460/500\n",
            "58/58 [==============================] - 11s 193ms/step - loss: 0.0136 - accuracy: 0.9974 - val_loss: 0.2807 - val_accuracy: 0.9430\n",
            "Epoch 461/500\n",
            "58/58 [==============================] - 11s 191ms/step - loss: 0.0086 - accuracy: 0.9978 - val_loss: 0.3294 - val_accuracy: 0.9372\n",
            "Epoch 462/500\n",
            "58/58 [==============================] - 11s 194ms/step - loss: 0.0048 - accuracy: 0.9989 - val_loss: 0.3744 - val_accuracy: 0.9379\n",
            "Epoch 463/500\n",
            "58/58 [==============================] - 10s 165ms/step - loss: 0.0086 - accuracy: 0.9978 - val_loss: 0.3542 - val_accuracy: 0.9379\n",
            "Epoch 464/500\n",
            "58/58 [==============================] - 13s 219ms/step - loss: 0.0120 - accuracy: 0.9965 - val_loss: 0.3394 - val_accuracy: 0.9335\n",
            "Epoch 465/500\n",
            "58/58 [==============================] - 11s 195ms/step - loss: 0.0112 - accuracy: 0.9967 - val_loss: 0.3202 - val_accuracy: 0.9399\n",
            "Epoch 466/500\n",
            "58/58 [==============================] - 11s 196ms/step - loss: 0.0122 - accuracy: 0.9966 - val_loss: 0.3569 - val_accuracy: 0.9335\n",
            "Epoch 467/500\n",
            "58/58 [==============================] - 11s 195ms/step - loss: 0.0056 - accuracy: 0.9990 - val_loss: 0.3855 - val_accuracy: 0.9359\n",
            "Epoch 468/500\n",
            "58/58 [==============================] - 10s 179ms/step - loss: 0.0097 - accuracy: 0.9970 - val_loss: 0.3335 - val_accuracy: 0.9369\n",
            "Epoch 469/500\n",
            "58/58 [==============================] - 10s 178ms/step - loss: 0.0047 - accuracy: 0.9990 - val_loss: 0.3696 - val_accuracy: 0.9369\n",
            "Epoch 470/500\n",
            "58/58 [==============================] - 11s 194ms/step - loss: 0.0050 - accuracy: 0.9992 - val_loss: 0.3738 - val_accuracy: 0.9355\n",
            "Epoch 471/500\n",
            "58/58 [==============================] - 11s 189ms/step - loss: 0.0061 - accuracy: 0.9989 - val_loss: 0.4028 - val_accuracy: 0.9376\n",
            "Epoch 472/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0105 - accuracy: 0.9976 - val_loss: 0.3554 - val_accuracy: 0.9403\n",
            "Epoch 473/500\n",
            "58/58 [==============================] - 11s 186ms/step - loss: 0.0266 - accuracy: 0.9929 - val_loss: 0.2645 - val_accuracy: 0.9379\n",
            "Epoch 474/500\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0357 - accuracy: 0.9901 - val_loss: 0.3394 - val_accuracy: 0.9294\n",
            "Epoch 475/500\n",
            "58/58 [==============================] - 11s 181ms/step - loss: 0.0180 - accuracy: 0.9950 - val_loss: 0.3496 - val_accuracy: 0.9393\n",
            "Epoch 476/500\n",
            "58/58 [==============================] - 12s 203ms/step - loss: 0.0117 - accuracy: 0.9971 - val_loss: 0.3467 - val_accuracy: 0.9382\n",
            "Epoch 477/500\n",
            "58/58 [==============================] - 13s 225ms/step - loss: 0.0102 - accuracy: 0.9974 - val_loss: 0.3877 - val_accuracy: 0.9389\n",
            "Epoch 478/500\n",
            "58/58 [==============================] - 12s 202ms/step - loss: 0.0135 - accuracy: 0.9956 - val_loss: 0.3619 - val_accuracy: 0.9399\n",
            "Epoch 479/500\n",
            "58/58 [==============================] - 13s 224ms/step - loss: 0.0080 - accuracy: 0.9980 - val_loss: 0.3455 - val_accuracy: 0.9420\n",
            "Epoch 480/500\n",
            "58/58 [==============================] - 12s 202ms/step - loss: 0.0175 - accuracy: 0.9951 - val_loss: 0.2924 - val_accuracy: 0.9430\n",
            "Epoch 481/500\n",
            "58/58 [==============================] - 12s 203ms/step - loss: 0.0098 - accuracy: 0.9969 - val_loss: 0.3142 - val_accuracy: 0.9416\n",
            "Epoch 482/500\n",
            "58/58 [==============================] - 12s 201ms/step - loss: 0.0145 - accuracy: 0.9954 - val_loss: 0.3564 - val_accuracy: 0.9416\n",
            "Epoch 483/500\n",
            "58/58 [==============================] - 12s 215ms/step - loss: 0.0116 - accuracy: 0.9970 - val_loss: 0.3563 - val_accuracy: 0.9437\n",
            "Epoch 484/500\n",
            "58/58 [==============================] - 11s 191ms/step - loss: 0.0078 - accuracy: 0.9977 - val_loss: 0.3846 - val_accuracy: 0.9379\n",
            "Epoch 485/500\n",
            "58/58 [==============================] - 11s 184ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.3781 - val_accuracy: 0.9410\n",
            "Epoch 486/500\n",
            "58/58 [==============================] - 13s 226ms/step - loss: 0.0237 - accuracy: 0.9948 - val_loss: 0.3308 - val_accuracy: 0.9355\n",
            "Epoch 487/500\n",
            "58/58 [==============================] - 11s 196ms/step - loss: 0.0438 - accuracy: 0.9879 - val_loss: 0.2829 - val_accuracy: 0.9430\n",
            "Epoch 488/500\n",
            "58/58 [==============================] - 11s 196ms/step - loss: 0.0076 - accuracy: 0.9981 - val_loss: 0.3340 - val_accuracy: 0.9433\n",
            "Epoch 489/500\n",
            "58/58 [==============================] - 11s 192ms/step - loss: 0.0086 - accuracy: 0.9977 - val_loss: 0.3999 - val_accuracy: 0.9355\n",
            "Epoch 490/500\n",
            "58/58 [==============================] - 11s 198ms/step - loss: 0.0151 - accuracy: 0.9952 - val_loss: 0.2788 - val_accuracy: 0.9433\n",
            "Epoch 491/500\n",
            "58/58 [==============================] - 10s 168ms/step - loss: 0.0116 - accuracy: 0.9974 - val_loss: 0.3015 - val_accuracy: 0.9420\n",
            "Epoch 492/500\n",
            "58/58 [==============================] - 11s 188ms/step - loss: 0.0069 - accuracy: 0.9981 - val_loss: 0.3401 - val_accuracy: 0.9420\n",
            "Epoch 493/500\n",
            "58/58 [==============================] - 11s 196ms/step - loss: 0.0043 - accuracy: 0.9990 - val_loss: 0.3767 - val_accuracy: 0.9423\n",
            "Epoch 494/500\n",
            "58/58 [==============================] - 11s 196ms/step - loss: 0.0033 - accuracy: 0.9995 - val_loss: 0.3811 - val_accuracy: 0.9433\n",
            "Epoch 495/500\n",
            "58/58 [==============================] - 11s 194ms/step - loss: 0.0123 - accuracy: 0.9967 - val_loss: 0.3571 - val_accuracy: 0.9372\n",
            "Epoch 496/500\n",
            "58/58 [==============================] - 12s 206ms/step - loss: 0.0509 - accuracy: 0.9871 - val_loss: 0.2938 - val_accuracy: 0.9393\n",
            "Epoch 497/500\n",
            "58/58 [==============================] - 10s 176ms/step - loss: 0.0090 - accuracy: 0.9976 - val_loss: 0.3924 - val_accuracy: 0.9365\n",
            "Epoch 498/500\n",
            "58/58 [==============================] - 11s 183ms/step - loss: 0.0077 - accuracy: 0.9982 - val_loss: 0.4493 - val_accuracy: 0.9332\n",
            "Epoch 499/500\n",
            "58/58 [==============================] - 11s 198ms/step - loss: 0.0050 - accuracy: 0.9995 - val_loss: 0.4400 - val_accuracy: 0.9328\n",
            "Epoch 500/500\n",
            "58/58 [==============================] - 11s 195ms/step - loss: 0.0053 - accuracy: 0.9990 - val_loss: 0.4342 - val_accuracy: 0.9325\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x789c182182e0>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_classifier = HART((128, 9), 6)\n",
        "\n",
        "model_classifier.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "history = model_classifier.fit(x=X_train,\n",
        "    y=Y_train,\n",
        "    validation_data = (X_test,Y_test),\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    verbose=1, callbacks=[checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hKKkcqwzKgfh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Validation Accuracy: 0.9551153521462504\n"
          ]
        }
      ],
      "source": [
        "# Maximum validation Accuracy\n",
        "val_accuracy_tensor = history.history['val_accuracy']\n",
        "best_val_accuracy = np.max(val_accuracy_tensor)\n",
        "print(\"Best Validation Accuracy:\", best_val_accuracy)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
