{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9ECDQJJyKgfc"
      },
      "outputs": [],
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import dstack\n",
        "from pandas import read_csv\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from matplotlib import pyplot as plt\n",
        "import keras\n",
        "from keras import initializers\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras.backend import *\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import Dense, Layer, Lambda, Input, Flatten, Dropout, Conv2D, MaxPooling2D, Conv1D, MaxPooling1D, LSTM, TimeDistributed, ConvLSTM2D, Permute, Reshape, Conv2D\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.python.ops import array_ops\n",
        "from tensorflow.python.ops import math_ops\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping\n",
        "\n",
        "def own_batch_dot(x, y, axes=None):\n",
        "\t\"\"\"Batchwise dot product.\n",
        "\t`batch_dot` is used to compute dot product of `x` and `y` when\n",
        "\t`x` and `y` are data in batch, i.e. in a shape of\n",
        "\t`(batch_size, :)`.\n",
        "\t`batch_dot` results in a tensor or variable with less dimensions\n",
        "\tthan the input. If the number of dimensions is reduced to 1,\n",
        "\twe use `expand_dims` to make sure that ndim is at least 2.\n",
        "\tArguments:\n",
        "\t\tx: Keras tensor or variable with `ndim >= 2`.\n",
        "\t\ty: Keras tensor or variable with `ndim >= 2`.\n",
        "\t\taxes: list of (or single) int with target dimensions.\n",
        "\t\t\tThe lengths of `axes[0]` and `axes[1]` should be the same.\n",
        "\tReturns:\n",
        "\t\tA tensor with shape equal to the concatenation of `x`'s shape\n",
        "\t\t(less the dimension that was summed over) and `y`'s shape\n",
        "\t\t(less the batch dimension and the dimension that was summed over).\n",
        "\t\tIf the final rank is 1, we reshape it to `(batch_size, 1)`.\n",
        "\tExamples:\n",
        "\t\tAssume `x = [[1, 2], [3, 4]]` and `y = [[5, 6], [7, 8]]`\n",
        "\t\t`batch_dot(x, y, axes=1) = [[17, 53]]` which is the main diagonal\n",
        "\t\tof `x.dot(y.T)`, although we never have to calculate the off-diagonal\n",
        "\t\telements.\n",
        "\t\tShape inference:\n",
        "\t\tLet `x`'s shape be `(100, 20)` and `y`'s shape be `(100, 30, 20)`.\n",
        "\t\tIf `axes` is (1, 2), to find the output shape of resultant tensor,\n",
        "\t\t\tloop through each dimension in `x`'s shape and `y`'s shape:\n",
        "\t\t* `x.shape[0]` : 100 : append to output shape\n",
        "\t\t* `x.shape[1]` : 20 : do not append to output shape,\n",
        "\t\t\tdimension 1 of `x` has been summed over. (`dot_axes[0]` = 1)\n",
        "\t\t* `y.shape[0]` : 100 : do not append to output shape,\n",
        "\t\t\talways ignore first dimension of `y`\n",
        "\t\t* `y.shape[1]` : 30 : append to output shape\n",
        "\t\t* `y.shape[2]` : 20 : do not append to output shape,\n",
        "\t\t\tdimension 2 of `y` has been summed over. (`dot_axes[1]` = 2)\n",
        "\t\t`output_shape` = `(100, 30)`\n",
        "\t```python\n",
        "\t\t>>> x_batch = K.ones(shape=(32, 20, 1))\n",
        "\t\t>>> y_batch = K.ones(shape=(32, 30, 20))\n",
        "\t\t>>> xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=[1, 2])\n",
        "\t\t>>> K.int_shape(xy_batch_dot)\n",
        "\t\t(32, 1, 30)\n",
        "\t```\n",
        "\t\"\"\"\n",
        "\tif isinstance(axes, int):\n",
        "\t\taxes = (axes, axes)\n",
        "\tx_ndim = ndim(x)\n",
        "\ty_ndim = ndim(y)\n",
        "\tif axes is None:\n",
        "\t\t# behaves like tf.batch_matmul as default\n",
        "\t\taxes = [x_ndim - 1, y_ndim - 2]\n",
        "\tif x_ndim > y_ndim:\n",
        "\t\tdiff = x_ndim - y_ndim\n",
        "\t\ty = array_ops.reshape(y,\n",
        "\t\t\t\t\t\t\tarray_ops.concat(\n",
        "\t\t\t\t\t\t\t\t[array_ops.shape(y), [1] * (diff)], axis=0))\n",
        "\telif y_ndim > x_ndim:\n",
        "\t\tdiff = y_ndim - x_ndim\n",
        "\t\tx = array_ops.reshape(x,\n",
        "\t\t\t\t\t\t\tarray_ops.concat(\n",
        "\t\t\t\t\t\t\t\t[array_ops.shape(x), [1] * (diff)], axis=0))\n",
        "\telse:\n",
        "\t\tdiff = 0\n",
        "\tif ndim(x) == 2 and ndim(y) == 2:\n",
        "\t\tif axes[0] == axes[1]:\n",
        "\t\t\tout = math_ops.reduce_sum(math_ops.multiply(x, y), axes[0])\n",
        "\t\telse:\n",
        "\t\t\tout = math_ops.reduce_sum(\n",
        "\t\t\tmath_ops.multiply(array_ops.transpose(x, [1, 0]), y), axes[1])\n",
        "\telse:\n",
        "\t\tadj_x = None if axes[0] == ndim(x) - 1 else True\n",
        "\t\tadj_y = True if axes[1] == ndim(y) - 1 else None\n",
        "\t\tout = math_ops.matmul(x, y, adjoint_a=adj_x, adjoint_b=adj_y)\n",
        "\tif diff:\n",
        "\t\tif x_ndim > y_ndim:\n",
        "\t\t\tidx = x_ndim + y_ndim - 3\n",
        "\t\telse:\n",
        "\t\t\tidx = x_ndim - 1\n",
        "\t\t\tout = array_ops.squeeze(out, list(range(idx, idx + diff)))\n",
        "\tif ndim(out) == 1:\n",
        "\t\tout = expand_dims(out, 1)\n",
        "\treturn out\n",
        "\n",
        "class DropPath(layers.Layer):\n",
        "    def __init__(self, drop_prob=0.0, **kwargs):\n",
        "        super(DropPath, self).__init__(**kwargs)\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def call(self, x,training=None):\n",
        "        if(training):\n",
        "            input_shape = tf.shape(x)\n",
        "            batch_size = input_shape[0]\n",
        "            rank = x.shape.rank\n",
        "            shape = (batch_size,) + (1,) * (rank - 1)\n",
        "            random_tensor = (1 - self.drop_prob) + tf.random.uniform(shape, dtype=x.dtype)\n",
        "            path_mask = tf.floor(random_tensor)\n",
        "            output = tf.math.divide(x, 1 - self.drop_prob) * path_mask\n",
        "            return output\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'drop_prob': self.drop_prob,})\n",
        "        return config\n",
        "\n",
        "class GatedLinearUnit(layers.Layer):\n",
        "    def __init__(self,units,**kwargs):\n",
        "        super(GatedLinearUnit, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.linear = layers.Dense(units * 2)\n",
        "        self.sigmoid = tf.keras.activations.sigmoid\n",
        "    def call(self, inputs):\n",
        "        linearProjection = self.linear(inputs)\n",
        "        softMaxProjection = self.sigmoid(linearProjection[:,:,self.units:])\n",
        "        return tf.multiply(linearProjection[:,:,:self.units],softMaxProjection)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'units': self.units,})\n",
        "        return config\n",
        "\n",
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim,**kwargs):\n",
        "        super(PatchEncoder, self).__init__(**kwargs)\n",
        "        self.num_patches = num_patches\n",
        "        self.projection_dim = projection_dim\n",
        "        self.position_embedding = layers.Embedding(input_dim=num_patches, output_dim=projection_dim)\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = patch + self.position_embedding(positions)\n",
        "        return encoded\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'num_patches': self.num_patches,\n",
        "            'projection_dim': self.projection_dim,})\n",
        "        return config\n",
        "\n",
        "class ClassToken(layers.Layer):\n",
        "    def __init__(self, hidden_size,**kwargs):\n",
        "        super(ClassToken, self).__init__(**kwargs)\n",
        "        self.cls_init = tf.random.normal\n",
        "        self.hidden_size = hidden_size\n",
        "        self.cls = tf.Variable(\n",
        "            name=\"cls\",\n",
        "            initial_value=self.cls_init(shape=(1, 1, self.hidden_size), seed=randomSeed, dtype=\"float32\"),\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        cls_broadcasted = tf.cast(\n",
        "            tf.broadcast_to(self.cls, [batch_size, 1, self.hidden_size]),\n",
        "            dtype=inputs.dtype,\n",
        "        )\n",
        "        return tf.concat([cls_broadcasted, inputs], 1)\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'hidden_size': self.hidden_size,})\n",
        "        return config\n",
        "\n",
        "class Prompts(layers.Layer):\n",
        "    def __init__(self, projectionDims,promptCount = 1,**kwargs):\n",
        "        super(Prompts, self).__init__(**kwargs)\n",
        "        self.cls_init = tf.random.normal\n",
        "        self.projectionDims = projectionDims\n",
        "        self.promptCount = promptCount\n",
        "        self.prompts = [tf.Variable(\n",
        "            name=\"prompt\"+str(_),\n",
        "            initial_value=self.cls_init(shape=(1, 1, self.projectionDims), seed=randomSeed, dtype=\"float32\"),\n",
        "            trainable=True,\n",
        "        )  for _ in range(promptCount)]\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        prompt_broadcasted = tf.concat([tf.cast(tf.broadcast_to(promptInits, [batch_size, 1, self.projectionDims]),dtype=inputs.dtype,)for promptInits in self.prompts],1)\n",
        "        return tf.concat([inputs,prompt_broadcasted], 1)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'projectionDims': self.projectionDims,\n",
        "            'promptCount': self.promptCount,})\n",
        "        return config\n",
        "\n",
        "class SensorWiseMHA(layers.Layer):\n",
        "    def __init__(self, projectionQuarter, num_heads,startIndex,stopIndex,dropout_rate = 0.0,dropPathRate = 0.0, **kwargs):\n",
        "        super(SensorWiseMHA, self).__init__(**kwargs)\n",
        "        self.projectionQuarter = projectionQuarter\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.MHA = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.projectionQuarter, dropout = dropout_rate )\n",
        "        self.startIndex = startIndex\n",
        "        self.stopIndex = stopIndex\n",
        "        self.dropPathRate = dropPathRate\n",
        "        self.DropPath = DropPath(dropPathRate)\n",
        "    def call(self, inputData, training=None, return_attention_scores = False):\n",
        "        extractedInput = inputData[:,:,self.startIndex:self.stopIndex]\n",
        "        if(return_attention_scores):\n",
        "            MHA_Outputs, attentionScores = self.MHA(extractedInput,extractedInput,return_attention_scores = True )\n",
        "            return MHA_Outputs , attentionScores\n",
        "        else:\n",
        "            MHA_Outputs = self.MHA(extractedInput,extractedInput)\n",
        "            MHA_Outputs = self.DropPath(MHA_Outputs)\n",
        "            return MHA_Outputs\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'projectionQuarter': self.projectionQuarter,\n",
        "            'num_heads': self.num_heads,\n",
        "            'startIndex': self.startIndex,\n",
        "            'dropout_rate': self.dropout_rate,\n",
        "            'stopIndex': self.stopIndex,\n",
        "            'dropPathRate': self.dropPathRate,})\n",
        "        return config\n",
        "def softDepthConv(inputs):\n",
        "    kernel = inputs[0]\n",
        "    inputData = inputs[1]\n",
        "    convOutputs = tf.nn.conv1d(\n",
        "    inputData,\n",
        "    kernel,\n",
        "    stride = 1,\n",
        "    padding = 'SAME',\n",
        "    data_format='NCW',)\n",
        "    return convOutputs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class liteFormer(layers.Layer):\n",
        "    def __init__(self,startIndex,stopIndex, projectionSize, kernelSize = 16, attentionHead = 3, use_bias=False, dropPathRate = 0.0,dropout_rate = 0,**kwargs):\n",
        "        super(liteFormer, self).__init__(**kwargs)\n",
        "        self.use_bias = use_bias\n",
        "        self.startIndex = startIndex\n",
        "        self.stopIndex = stopIndex\n",
        "        self.kernelSize = kernelSize\n",
        "        self.softmax = tf.nn.softmax\n",
        "        self.projectionSize = projectionSize\n",
        "        self.attentionHead = attentionHead\n",
        "        self.DropPathLayer = DropPath(dropPathRate)\n",
        "        self.projectionHalf = projectionSize // 2\n",
        "    def build(self,inputShape):\n",
        "        self.depthwise_kernel = [self.add_weight(\n",
        "            shape=(self.kernelSize,1,1),\n",
        "            initializer=\"glorot_uniform\",\n",
        "            trainable=True,\n",
        "            name=\"convWeights\"+str(_),\n",
        "            dtype=\"float32\") for _ in range(self.attentionHead)]\n",
        "        if self.use_bias:\n",
        "            self.convBias = self.add_weight(\n",
        "                shape=(self.attentionHead,),\n",
        "                initializer=\"glorot_uniform\",\n",
        "                trainable=True,\n",
        "                name=\"biasWeights\",\n",
        "                dtype=\"float32\"\n",
        "            )\n",
        "\n",
        "    def call(self, inputs,training=None):\n",
        "        formattedInputs = inputs[:,:,self.startIndex:self.stopIndex]\n",
        "        inputShape = tf.shape(formattedInputs)\n",
        "        reshapedInputs = tf.reshape(formattedInputs,(-1,self.attentionHead,inputShape[1]))\n",
        "        if(training):\n",
        "            for convIndex in range(self.attentionHead):\n",
        "                self.depthwise_kernel[convIndex].assign(self.softmax(self.depthwise_kernel[convIndex], axis=0))\n",
        "        convOutputs = tf.convert_to_tensor([tf.nn.conv1d(\n",
        "            reshapedInputs[:,convIndex:convIndex+1,:],\n",
        "            self.depthwise_kernel[convIndex],\n",
        "            stride = 1,\n",
        "            padding = 'SAME',\n",
        "            data_format='NCW',) for convIndex in range(self.attentionHead) ])\n",
        "        convOutputsDropPath = self.DropPathLayer(convOutputs)\n",
        "        localAttention = tf.reshape(convOutputsDropPath,(-1,inputShape[1],self.projectionSize))\n",
        "        return localAttention\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'use_bias': self.use_bias,\n",
        "            'kernelSize': self.kernelSize,\n",
        "            'startIndex': self.startIndex,\n",
        "            'stopIndex': self.stopIndex,\n",
        "            'projectionSize': self.projectionSize,\n",
        "            'attentionHead': self.attentionHead,})\n",
        "        return config\n",
        "\n",
        "class mixAccGyro(layers.Layer):\n",
        "    def __init__(self,projectionQuarter,projectionHalf,projection_dim,**kwargs):\n",
        "        super(mixAccGyro, self).__init__(**kwargs)\n",
        "        self.projectionQuarter = projectionQuarter\n",
        "        self.projectionHalf = projectionHalf\n",
        "        self.projection_dim = projection_dim\n",
        "        self.projectionThreeFourth = self.projectionHalf+self.projectionQuarter\n",
        "        self.mixedAccGyroIndex = tf.reshape(tf.transpose(tf.stack(\n",
        "            [np.arange(projectionQuarter,projectionHalf), np.arange(projectionHalf,projectionHalf + projectionQuarter)])),[-1])\n",
        "        self.newArrangement = tf.concat((np.arange(0,projectionQuarter),self.mixedAccGyroIndex,np.arange(self.projectionThreeFourth,projection_dim)),axis = 0)\n",
        "    def call(self, inputs):\n",
        "        return tf.gather(inputs,self.newArrangement,axis= 2)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'projectionQuarter': self.projectionQuarter,\n",
        "            'projectionHalf': self.projectionHalf,\n",
        "            'projection_dim': self.projection_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.swish)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x\n",
        "\n",
        "def mlp2(x, hidden_units, dropout_rate):\n",
        "    x = layers.Dense(hidden_units[0],activation=tf.nn.swish)(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "    x = layers.Dense(hidden_units[1])(x)\n",
        "    return x\n",
        "\n",
        "def depthMLP(x, hidden_units, dropout_rate):\n",
        "    x = layers.Dense(hidden_units[0])(x)\n",
        "    x = layers.DepthwiseConv1D(3,data_format='channels_first',activation=tf.nn.swish)(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "    x = layers.Dense(hidden_units[1])(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "    return x\n",
        "\n",
        "class SensorPatchesTimeDistributed(layers.Layer):\n",
        "    def __init__(self, projection_dim,filterCount,patchCount,frameSize = 128, channelsCount = 6,**kwargs):\n",
        "        super(SensorPatchesTimeDistributed, self).__init__(**kwargs)\n",
        "        self.projection_dim = projection_dim\n",
        "        self.frameSize = frameSize\n",
        "        self.channelsCount = channelsCount\n",
        "        self.patchCount = patchCount\n",
        "        self.filterCount = filterCount\n",
        "        self.reshapeInputs = layers.Reshape((patchCount, frameSize // patchCount, channelsCount))\n",
        "        self.kernelSize = (projection_dim//2 + filterCount) // filterCount\n",
        "        self.accProjection = layers.TimeDistributed(layers.Conv1D(filters = filterCount,kernel_size = self.kernelSize,strides = 1, data_format = \"channels_last\"))\n",
        "        self.gyroProjection = layers.TimeDistributed(layers.Conv1D(filters = filterCount,kernel_size = self.kernelSize,strides = 1, data_format = \"channels_last\"))\n",
        "        self.flattenTime = layers.TimeDistributed(layers.Flatten())\n",
        "        assert (projection_dim//2 + filterCount) / filterCount % self.kernelSize == 0\n",
        "        print(\"Kernel Size is \"+str((projection_dim//2 + filterCount) / filterCount))\n",
        "#         assert\n",
        "    def call(self, inputData):\n",
        "        inputData = self.reshapeInputs(inputData)\n",
        "        accProjections = self.flattenTime(self.accProjection(inputData[:,:,:,:3]))\n",
        "        gyroProjections = self.flattenTime(self.gyroProjection(inputData[:,:,:,3:]))\n",
        "        Projections = tf.concat((accProjections,gyroProjections),axis=2)\n",
        "        return Projections\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'projection_dim': self.projection_dim,\n",
        "            'filterCount': self.filterCount,\n",
        "            'patchCount': self.patchCount,\n",
        "            'frameSize': self.frameSize,\n",
        "            'channelsCount': self.channelsCount,})\n",
        "        return config\n",
        "\n",
        "class SensorPatches(layers.Layer):\n",
        "    def __init__(self, projection_dim, patchSize,timeStep, **kwargs):\n",
        "        super(SensorPatches, self).__init__(**kwargs)\n",
        "        self.patchSize = patchSize\n",
        "        self.timeStep = timeStep\n",
        "        self.projection_dim = projection_dim\n",
        "        self.accProjection = layers.Conv1D(filters = int(projection_dim/2),kernel_size = patchSize,strides = timeStep, data_format = \"channels_last\")\n",
        "        self.gyroProjection = layers.Conv1D(filters = int(projection_dim/2),kernel_size = patchSize,strides = timeStep, data_format = \"channels_last\")\n",
        "    def call(self, inputData):\n",
        "\n",
        "        accProjections = self.accProjection(inputData[:,:,:3])\n",
        "        gyroProjections = self.gyroProjection(inputData[:,:,3:])\n",
        "        Projections = tf.concat((accProjections,gyroProjections),axis=2)\n",
        "        return Projections\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'patchSize': self.patchSize,\n",
        "            'projection_dim': self.projection_dim,\n",
        "            'timeStep': self.timeStep,})\n",
        "        return config\n",
        "\n",
        "\n",
        "class threeSensorPatches(layers.Layer):\n",
        "    def __init__(self, projection_dim, patchSize,timeStep, **kwargs):\n",
        "        super(threeSensorPatches, self).__init__(**kwargs)\n",
        "        self.patchSize = patchSize\n",
        "        self.timeStep = timeStep\n",
        "        self.projection_dim = projection_dim\n",
        "        self.accProjection = layers.Conv1D(filters = int(projection_dim//3),kernel_size = patchSize,strides = timeStep, data_format = \"channels_last\")\n",
        "        self.gyroProjection = layers.Conv1D(filters = int(projection_dim//3),kernel_size = patchSize,strides = timeStep, data_format = \"channels_last\")\n",
        "        self.magProjection = layers.Conv1D(filters = int(projection_dim//3),kernel_size = patchSize,strides = timeStep, data_format = \"channels_last\")\n",
        "\n",
        "    def call(self, inputData):\n",
        "\n",
        "        accProjections = self.accProjection(inputData[:,:,:3])\n",
        "        gyroProjections = self.gyroProjection(inputData[:,:,3:6])\n",
        "        magProjections = self.magProjection(inputData[:,:,6:])\n",
        "\n",
        "        Projections = tf.concat((accProjections,gyroProjections,magProjections),axis=2)\n",
        "        return Projections\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'patchSize': self.patchSize,\n",
        "            'projection_dim': self.projection_dim,\n",
        "            'timeStep': self.timeStep,})\n",
        "        return config\n",
        "\n",
        "\n",
        "class fourSensorPatches(layers.Layer):\n",
        "    def __init__(self, projection_dim, patchSize,timeStep, **kwargs):\n",
        "        super(fourSensorPatches, self).__init__(**kwargs)\n",
        "        self.patchSize = patchSize\n",
        "        self.timeStep = timeStep\n",
        "        self.projection_dim = projection_dim\n",
        "        self.accProjection = layers.Conv1D(filters = int(projection_dim/4),kernel_size = patchSize,strides = timeStep, data_format = \"channels_last\")\n",
        "        self.gyroProjection = layers.Conv1D(filters = int(projection_dim/4),kernel_size = patchSize,strides = timeStep, data_format = \"channels_last\")\n",
        "        self.magProjection = layers.Conv1D(filters = int(projection_dim/4),kernel_size = patchSize,strides = timeStep, data_format = \"channels_last\")\n",
        "        self.altProjection = layers.Conv1D(filters = int(projection_dim/4),kernel_size = patchSize,strides = timeStep, data_format = \"channels_last\")\n",
        "\n",
        "    def call(self, inputData):\n",
        "\n",
        "        accProjections = self.accProjection(inputData[:,:,:3])\n",
        "        gyroProjections = self.gyroProjection(inputData[:,:,3:6])\n",
        "        magProjection = self.magProjection(inputData[:,:,6:9])\n",
        "        altProjection = self.altProjection(inputData[:,:,9:])\n",
        "\n",
        "        Projections = tf.concat((accProjections,gyroProjections,magProjection,altProjection),axis=2)\n",
        "        return Projections\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'patchSize': self.patchSize,\n",
        "            'projection_dim': self.projection_dim,\n",
        "            'timeStep': self.timeStep,})\n",
        "        return config\n",
        "\n",
        "def extract_intermediate_model_from_base_model(base_model, intermediate_layer=4):\n",
        "    model = tf.keras.Model(inputs=base_model.inputs, outputs=base_model.layers[intermediate_layer].output, name=base_model.name + \"_layer_\" + str(intermediate_layer))\n",
        "    return model\n",
        "\n",
        "def HART(input_shape,activityCount, projection_dim = 192,patchSize = 16,timeStep = 16,num_heads = 3,filterAttentionHead = 4, convKernels = [3, 7, 15, 31, 31, 31], mlp_head_units = [1024],dropout_rate = 0.3,useTokens = False):\n",
        "    projectionHalf = projection_dim//2\n",
        "    projectionQuarter = projection_dim//4\n",
        "    dropPathRate = np.linspace(0, dropout_rate* 10, len(convKernels)) * 0.1\n",
        "    transformer_units = [\n",
        "    projection_dim * 2,\n",
        "    projection_dim,]\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    patches = SensorPatches(projection_dim,patchSize,timeStep)(inputs)\n",
        "    if(useTokens):\n",
        "        patches = ClassToken(projection_dim)(patches)\n",
        "    patchCount = patches.shape[1]\n",
        "    encoded_patches = PatchEncoder(patchCount, projection_dim)(patches)\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for layerIndex, kernelLength in enumerate(convKernels):\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6 , name = \"normalizedInputs_\"+str(layerIndex))(encoded_patches)\n",
        "        branch1 = liteFormer(\n",
        "                          startIndex = projectionQuarter,\n",
        "                          stopIndex = projectionQuarter + projectionHalf,\n",
        "                          projectionSize = projectionHalf,\n",
        "                          attentionHead =  filterAttentionHead,\n",
        "                          kernelSize = kernelLength,\n",
        "                          dropPathRate = dropPathRate[layerIndex],\n",
        "                          dropout_rate = dropout_rate,\n",
        "                          name = \"liteFormer_\"+str(layerIndex))(x1)\n",
        "\n",
        "\n",
        "        branch2Acc = SensorWiseMHA(projectionQuarter,num_heads,0,projectionQuarter,dropPathRate = dropPathRate[layerIndex],dropout_rate = dropout_rate,name = \"AccMHA_\"+str(layerIndex))(x1)\n",
        "\n",
        "        branch2Gyro = SensorWiseMHA(projectionQuarter,num_heads,projectionQuarter + projectionHalf ,projection_dim,dropPathRate = dropPathRate[layerIndex],dropout_rate = dropout_rate, name = \"GyroMHA_\"+str(layerIndex))(x1)\n",
        "        concatAttention = tf.concat((branch2Acc,branch1,branch2Gyro),axis= 2 )\n",
        "        x2 = layers.Add()([concatAttention, encoded_patches])\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        x3 = mlp2(x3, hidden_units=transformer_units, dropout_rate=dropout_rate)\n",
        "        x3 = DropPath(dropPathRate[layerIndex])(x3)\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    if(useTokens):\n",
        "        representation = layers.Lambda(lambda v: v[:, 0], name=\"ExtractToken\")(representation)\n",
        "    else:\n",
        "        representation = layers.GlobalAveragePooling1D()(representation)\n",
        "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=dropout_rate)\n",
        "    logits = layers.Dense(activityCount,  activation='softmax')(features)\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=logits)\n",
        "    return model\n",
        "# ------------------------------specific module for MobileHART------------------------------\n",
        "\n",
        "def conv_block(x, filters=16, kernel_size=3, strides=2):\n",
        "    conv_layer = layers.Conv1D(\n",
        "        filters, kernel_size, strides=strides, activation=tf.nn.swish, padding=\"same\"\n",
        "    )\n",
        "    return conv_layer(x)\n",
        "\n",
        "def inverted_residual_block(x, expanded_channels, output_channels, strides=1):\n",
        "    m = layers.Conv1D(expanded_channels, 1, padding=\"same\", use_bias=False)(x)\n",
        "    m = layers.BatchNormalization()(m)\n",
        "    m = tf.nn.swish(m)\n",
        "\n",
        "    if strides == 2:\n",
        "        m = layers.ZeroPadding1D(padding=1)(m)\n",
        "    m = layers.DepthwiseConv1D(\n",
        "        3, strides=strides, padding=\"same\" if strides == 1 else \"valid\", use_bias=False\n",
        "    )(m)\n",
        "    m = layers.BatchNormalization()(m)\n",
        "    m = tf.nn.swish(m)\n",
        "\n",
        "    m = layers.Conv1D(output_channels, 1, padding=\"same\", use_bias=False)(m)\n",
        "    m = layers.BatchNormalization()(m)\n",
        "\n",
        "    if tf.math.equal(x.shape[-1], output_channels) and strides == 1:\n",
        "        return layers.Add()([m, x])\n",
        "    return m\n",
        "\n",
        "def transformer_block(x, transformer_layers, projection_dim, dropout_rate = 0.3,num_heads=2):\n",
        "\n",
        "    dropPathRate = np.linspace(0, dropout_rate* 10,transformer_layers) * 0.1\n",
        "\n",
        "\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=dropout_rate\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, x])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp2(\n",
        "            x3,\n",
        "            hidden_units=[x.shape[-1] * 2, x.shape[-1]],\n",
        "            dropout_rate=dropout_rate,\n",
        "        )\n",
        "        # Skip connection 2.\n",
        "        x = layers.Add()([x3, x2])\n",
        "\n",
        "    return x\n",
        "\n",
        "def mobilevit_block(x, num_blocks, projection_dim, strides=1):\n",
        "    # Local projection with convolutions.\n",
        "    local_features = conv_block(x, filters=projection_dim, strides=strides)\n",
        "    local_features = conv_block(\n",
        "        local_features, filters=projection_dim, kernel_size=1, strides=strides\n",
        "    )\n",
        "    global_features = transformer_block(\n",
        "        local_features, num_blocks, projection_dim\n",
        "    )\n",
        "\n",
        "    # Apply point-wise conv -> concatenate with the input features.\n",
        "    folded_feature_map = conv_block(\n",
        "        global_features, filters=x.shape[-1], kernel_size=1, strides=strides\n",
        "    )\n",
        "    local_global_features = layers.Concatenate(axis=-1)([x, folded_feature_map])\n",
        "\n",
        "    # Fuse the local and global features using a convoluion layer.\n",
        "    local_global_features = conv_block(\n",
        "        local_global_features, filters=projection_dim, strides=strides\n",
        "    )\n",
        "\n",
        "    return local_global_features\n",
        "\n",
        "\n",
        "def sensorWiseTransformer_block(xAcc, xGyro, patchCount,transformer_layers, projection_dim,kernelSize = 4,  dropout_rate = 0.3,num_heads=2):\n",
        "    projectionQuarter = projection_dim // 4\n",
        "    projectionHalf = projection_dim // 2\n",
        "    dropPathRate = np.linspace(0, dropout_rate* 10,transformer_layers) * 0.1\n",
        "\n",
        "    x = tf.concat((xAcc,xGyro),axis= 2 )\n",
        "    for layerIndex in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6, name = \"normalizedInputs_\"+str(layerIndex))(x)\n",
        "\n",
        "        branch1 = liteFormer(\n",
        "                            startIndex = projectionQuarter,\n",
        "                            stopIndex = projectionQuarter + projectionHalf,\n",
        "                            projectionSize = projectionHalf,\n",
        "                            attentionHead =  num_heads,\n",
        "                            kernelSize = kernelSize,\n",
        "                            dropPathRate = dropPathRate[layerIndex],\n",
        "                            name = \"liteFormer_\"+str(layerIndex))(x1)\n",
        "\n",
        "        branch2Acc = SensorWiseMHA(projectionQuarter,num_heads,0,projectionQuarter,dropPathRate = dropPathRate[layerIndex],dropout_rate = dropout_rate,name = \"AccMHA_\"+str(layerIndex))(x1)\n",
        "        branch2Gyro = SensorWiseMHA(projectionQuarter,num_heads,projectionQuarter + projectionHalf ,projection_dim,dropPathRate = dropPathRate[layerIndex],dropout_rate = dropout_rate,name = \"GyroMHA_\"+str(layerIndex))(x1)\n",
        "        concatAttention = tf.concat((branch2Acc,branch1,branch2Gyro),axis= 2 )\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([concatAttention, x])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp2(\n",
        "            x3,\n",
        "            hidden_units=[x.shape[-1] * 2, x.shape[-1]],\n",
        "            dropout_rate=dropout_rate,\n",
        "        )\n",
        "        x3 = DropPath(dropPathRate[layerIndex])(x3)\n",
        "        # Skip connection 2.\n",
        "        x = layers.Add()([x3, x2])\n",
        "\n",
        "    return x\n",
        "def sensorWiseHART(xAcc,xGyro, num_blocks, projection_dim, kernelSize = 4, strides=1):\n",
        "    # Local projection with convolutions.\n",
        "#     ---------------acc--------------\n",
        "    local_featuresAcc = conv_block(xAcc, filters=projection_dim//2, strides=strides)\n",
        "    local_featuresAcc = conv_block(\n",
        "        local_featuresAcc, filters=projection_dim//2, kernel_size=1, strides=strides\n",
        "    )\n",
        "\n",
        "#     ---------------gyro--------------\n",
        "\n",
        "    local_featuresGyro = conv_block(xGyro, filters=projection_dim//2, strides=strides)\n",
        "    local_featuresGyro = conv_block(\n",
        "        local_featuresGyro, filters=projection_dim//2, kernel_size=1, strides=strides\n",
        "    )\n",
        "    global_features = sensorWiseTransformer_block(local_featuresAcc,\n",
        "        local_featuresGyro, local_featuresGyro.shape[1], num_blocks, projection_dim, kernelSize = kernelSize\n",
        "    )\n",
        "\n",
        "    folded_feature_map_acc = conv_block(\n",
        "        global_features[:,:,:projection_dim//2], filters=xAcc.shape[-1], kernel_size=1, strides=strides\n",
        "    )\n",
        "    local_global_features_acc = layers.Concatenate(axis=-1)([xAcc, folded_feature_map_acc])\n",
        "        # Fuse the local and global features using a convoluion layer.\n",
        "    local_global_features_acc = conv_block(\n",
        "        local_global_features_acc, filters=projection_dim//2, strides=strides\n",
        "    )\n",
        "\n",
        "    folded_feature_map_gyro = conv_block(\n",
        "        global_features[:,:,projection_dim//2:], filters=xGyro.shape[-1], kernel_size=1, strides=strides\n",
        "    )\n",
        "    local_global_features_gyro = layers.Concatenate(axis=-1)([xGyro, folded_feature_map_gyro])\n",
        "\n",
        "    local_global_features_gyro = conv_block(\n",
        "        local_global_features_gyro, filters=projection_dim//2, strides=strides\n",
        "    )\n",
        "\n",
        "    return local_global_features_acc, local_global_features_gyro\n",
        "def mv2Block(x,expansion_factor,filterCount):\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=filterCount[0] * expansion_factor, output_channels=filterCount[1]\n",
        "    )\n",
        "    # Downsampling with MV2 block.\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=filterCount[1] * expansion_factor, output_channels=filterCount[2], strides=2\n",
        "    )\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=filterCount[2] * expansion_factor, output_channels=filterCount[2]\n",
        "    )\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=filterCount[2] * expansion_factor, output_channels=filterCount[2]\n",
        "    )\n",
        "    # First MV2 -> MobileViT block.\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=filterCount[2] * expansion_factor, output_channels=filterCount[3], strides=2\n",
        "    )\n",
        "    return x\n",
        "    # def hartModel(input_shape,activityCount, projection_dim,patchSize,timeStep,num_heads,filterAttentionHead, convKernels = [3, 7, 15, 31, 31, 31], mlp_head_units = [1024],dropout_rate = 0.3,useTokens = True):\n",
        "\n",
        "def mobileHART_XS(input_shape,activityCount,projectionDims = [96,120,144],filterCount = [16//2,32//2,48//2,64//2,80,96,384],expansion_factor=4,mlp_head_units = [1024],dropout_rate = 0.3):\n",
        "\n",
        "    # inputs = keras.Input((segment_size, num_input_channels))\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Initial conv-stem -> MV2 block.\n",
        "    accX = conv_block(inputs[:,:,:3],filters=filterCount[0])\n",
        "    gyroX = conv_block(inputs[:,:,3:],filters=filterCount[0])\n",
        "    accX = mv2Block(accX,expansion_factor,filterCount)\n",
        "    gyroX = mv2Block(gyroX,expansion_factor,filterCount)\n",
        "    accX, gyroX  = sensorWiseHART(accX,gyroX, num_blocks=2, projection_dim=projectionDims[0])\n",
        "    x = tf.concat((accX,gyroX), axis = 2)\n",
        "    x = layers.Dense(projectionDims[0],activation=tf.nn.swish)(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    # Second MV2 -> MobileViT block.\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=projectionDims[0] * expansion_factor, output_channels=filterCount[4], strides=2\n",
        "    )\n",
        "    x = mobilevit_block(x, num_blocks=4, projection_dim=projectionDims[1])\n",
        "\n",
        "    # Third MV2 -> MobileViT block.\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=projectionDims[1] * expansion_factor, output_channels=filterCount[5], strides=2\n",
        "    )\n",
        "    x = mobilevit_block(x, num_blocks=3, projection_dim=projectionDims[2])\n",
        "    x = conv_block(x, filters=filterCount[6], kernel_size=1, strides=1)\n",
        "    # Classification head.\n",
        "    x = layers.GlobalAvgPool1D(name = \"GAP\")(x)\n",
        "\n",
        "    x = mlp(x, hidden_units=mlp_head_units, dropout_rate=dropout_rate)\n",
        "\n",
        "    outputs = layers.Dense(activityCount, activation=\"softmax\")(x)\n",
        "# f.keras.Model(inputs=inputs, outputs=logits)\n",
        "    return tf.keras.Model(inputs, outputs)\n",
        "\n",
        "def mobileHART_XXS(input_shape,activityCount,projectionDims = [64,80,96],filterCount = [16//2,16//2,24//2,48//2,64,80,320],expansion_factor=2,mlp_head_units = [1024],dropout_rate = 0.3):\n",
        "\n",
        "    # inputs = keras.Input((segment_size, num_input_channels))\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Initial conv-stem -> MV2 block.\n",
        "    accX = conv_block(inputs[:,:,:3],filters=filterCount[0])\n",
        "    gyroX = conv_block(inputs[:,:,3:],filters=filterCount[0])\n",
        "    accX = mv2Block(accX,expansion_factor,filterCount)\n",
        "    gyroX = mv2Block(gyroX,expansion_factor,filterCount)\n",
        "    accX, gyroX  = sensorWiseHART(accX,gyroX, num_blocks=2, projection_dim=projectionDims[0])\n",
        "    x = tf.concat((accX,gyroX), axis = 2)\n",
        "    x = layers.Dense(projectionDims[0],activation=tf.nn.swish)(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    # Second MV2 -> MobileViT block.\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=projectionDims[0] * expansion_factor, output_channels=filterCount[4], strides=2\n",
        "    )\n",
        "    x = mobilevit_block(x, num_blocks=4, projection_dim=projectionDims[1])\n",
        "\n",
        "    # Third MV2 -> MobileViT block.\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=projectionDims[1] * expansion_factor, output_channels=filterCount[5], strides=2\n",
        "    )\n",
        "    x = mobilevit_block(x, num_blocks=3, projection_dim=projectionDims[2])\n",
        "    x = conv_block(x, filters=filterCount[6], kernel_size=1, strides=1)\n",
        "    # Classification head.\n",
        "    x = layers.GlobalAvgPool1D(name = \"GAP\")(x)\n",
        "\n",
        "    x = mlp(x, hidden_units=mlp_head_units, dropout_rate=dropout_rate)\n",
        "\n",
        "    outputs = layers.Dense(activityCount, activation=\"softmax\")(x)\n",
        "# f.keras.Model(inputs=inputs, outputs=logits)\n",
        "    return tf.keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayIEcaBiK446",
        "outputId": "fea5edc4-12f5-4fa4-8c9e-02c63d8943cf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "V-9Tyf2WKgfe"
      },
      "outputs": [],
      "source": [
        "# load a single file as a numpy array\n",
        "def load_file(filepath):\n",
        "    dataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
        "    return dataframe.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RFtsLcBTKgff"
      },
      "outputs": [],
      "source": [
        "# load a list of files and return as a 3d numpy array\n",
        "def load_group(filenames, prefix=''):\n",
        "    loaded = list()\n",
        "    for name in filenames:\n",
        "        data = load_file(prefix + name)\n",
        "        loaded.append(data)\n",
        "    # stack group so that features are the 3rd dimension\n",
        "    loaded = dstack(loaded)\n",
        "    return loaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IDNdqOsIKgff"
      },
      "outputs": [],
      "source": [
        "# load a dataset group, such as train or test\n",
        "def load_dataset_group(group, prefix=''):\n",
        "    filepath = prefix + group + '/Inertial Signals/'\n",
        "    print('File Path : ',filepath)\n",
        "    # load all 9 files as a single array\n",
        "    filenames = list()\n",
        "    # total acceleration\n",
        "    filenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
        "    # body acceleration\n",
        "    filenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
        "    # body gyroscope\n",
        "    filenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
        "    # load input data\n",
        "    X = load_group(filenames, filepath)\n",
        "    # load class output\n",
        "    y = load_file(prefix + group + '/y_'+group+'.txt')\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pGNunmWKgfg",
        "outputId": "53fdb50c-9959-432c-d97f-df2ac5fae154"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File Path :  /content/drive/MyDrive/HARCNNLSTM/UCIDataset/train/Inertial Signals/\n",
            "File Path :  /content/drive/MyDrive/HARCNNLSTM/UCIDataset/test/Inertial Signals/\n",
            "X_train.shape :  (7352, 128, 9)\n",
            "Y_train.shape :  (7352, 6)\n",
            "X_test.shape :  (2947, 128, 9)\n",
            "Y_test.shape :  (2947, 6)\n"
          ]
        }
      ],
      "source": [
        "# load all train\n",
        "X_train, Y_train = load_dataset_group('train', '/content/drive/MyDrive/HARCNNLSTM/UCIDataset/')\n",
        "# load all test\n",
        "X_test, Y_test = load_dataset_group('test', '/content/drive/MyDrive/HARCNNLSTM/UCIDataset/')\n",
        "\n",
        "# zero-offset class values\n",
        "Y_train = Y_train - 1\n",
        "Y_test = Y_test - 1\n",
        "# one hot encode y\n",
        "Y_train = to_categorical(Y_train)\n",
        "Y_test = to_categorical(Y_test)\n",
        "\n",
        "print('X_train.shape : ', X_train.shape)\n",
        "print('Y_train.shape : ', Y_train.shape)\n",
        "print('X_test.shape : ', X_test.shape)\n",
        "print('Y_test.shape : ', Y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_zhoqoAJKgfg"
      },
      "outputs": [],
      "source": [
        "verbose = 1\n",
        "epochs = 50\n",
        "batch_size = 128\n",
        "\n",
        "n_timesteps = X_train.shape[1]\n",
        "n_features = X_train.shape[2]\n",
        "n_outputs = Y_train.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SCAD5m6UKgfg"
      },
      "outputs": [],
      "source": [
        "checkpoint = ModelCheckpoint(\"HART_Baseline.h5\", monitor='val_accuracy', verbose=1,\n",
        "                             save_best_only=True, save_weights_only=False, mode='auto')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_classifier = HART((128, 6), 6)\n",
        "X_train_new = X_train[:, :, :6]\n",
        "X_test_new = X_test[:, :, :6]\n",
        "\n",
        "model_classifier.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "history = model_classifier.fit(x=X_train_new,\n",
        "    y=Y_train,\n",
        "    validation_data = (X_test_new,Y_test),\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tn19lw-rCUI-",
        "outputId": "43465500-049d-4ece-b367-e3cd58ccf80d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "368/368 [==============================] - 56s 67ms/step - loss: 0.6972 - accuracy: 0.6942 - val_loss: 0.5886 - val_accuracy: 0.7957\n",
            "Epoch 2/50\n",
            "368/368 [==============================] - 22s 60ms/step - loss: 0.3335 - accuracy: 0.8640 - val_loss: 0.4838 - val_accuracy: 0.8483\n",
            "Epoch 3/50\n",
            "368/368 [==============================] - 22s 59ms/step - loss: 0.2632 - accuracy: 0.8909 - val_loss: 0.4694 - val_accuracy: 0.8626\n",
            "Epoch 4/50\n",
            "368/368 [==============================] - 22s 60ms/step - loss: 0.2485 - accuracy: 0.9026 - val_loss: 0.4982 - val_accuracy: 0.8463\n",
            "Epoch 5/50\n",
            "368/368 [==============================] - 22s 60ms/step - loss: 0.2232 - accuracy: 0.9113 - val_loss: 0.4048 - val_accuracy: 0.8765\n",
            "Epoch 6/50\n",
            "368/368 [==============================] - 24s 66ms/step - loss: 0.1944 - accuracy: 0.9210 - val_loss: 0.3763 - val_accuracy: 0.8846\n",
            "Epoch 7/50\n",
            "368/368 [==============================] - 21s 58ms/step - loss: 0.1763 - accuracy: 0.9313 - val_loss: 0.3716 - val_accuracy: 0.8904\n",
            "Epoch 8/50\n",
            "368/368 [==============================] - 23s 62ms/step - loss: 0.1851 - accuracy: 0.9290 - val_loss: 0.5454 - val_accuracy: 0.8449\n",
            "Epoch 9/50\n",
            "368/368 [==============================] - 21s 58ms/step - loss: 0.1675 - accuracy: 0.9342 - val_loss: 0.4300 - val_accuracy: 0.8870\n",
            "Epoch 10/50\n",
            "368/368 [==============================] - 23s 61ms/step - loss: 0.1628 - accuracy: 0.9346 - val_loss: 0.4047 - val_accuracy: 0.8901\n",
            "Epoch 11/50\n",
            "368/368 [==============================] - 22s 60ms/step - loss: 0.1648 - accuracy: 0.9334 - val_loss: 0.3466 - val_accuracy: 0.8975\n",
            "Epoch 12/50\n",
            "368/368 [==============================] - 22s 60ms/step - loss: 0.1642 - accuracy: 0.9377 - val_loss: 0.4374 - val_accuracy: 0.8938\n",
            "Epoch 13/50\n",
            "368/368 [==============================] - 22s 61ms/step - loss: 0.1663 - accuracy: 0.9347 - val_loss: 0.3704 - val_accuracy: 0.8955\n",
            "Epoch 14/50\n",
            "368/368 [==============================] - 23s 63ms/step - loss: 0.1439 - accuracy: 0.9422 - val_loss: 0.3589 - val_accuracy: 0.8894\n",
            "Epoch 15/50\n",
            "368/368 [==============================] - 24s 64ms/step - loss: 0.1557 - accuracy: 0.9395 - val_loss: 0.3456 - val_accuracy: 0.8979\n",
            "Epoch 16/50\n",
            "368/368 [==============================] - 21s 57ms/step - loss: 0.1389 - accuracy: 0.9445 - val_loss: 0.3865 - val_accuracy: 0.9006\n",
            "Epoch 17/50\n",
            "368/368 [==============================] - 22s 61ms/step - loss: 0.1542 - accuracy: 0.9393 - val_loss: 0.3770 - val_accuracy: 0.8914\n",
            "Epoch 18/50\n",
            "368/368 [==============================] - 22s 60ms/step - loss: 0.1418 - accuracy: 0.9434 - val_loss: 0.4300 - val_accuracy: 0.8829\n",
            "Epoch 19/50\n",
            "368/368 [==============================] - 21s 58ms/step - loss: 0.1484 - accuracy: 0.9412 - val_loss: 0.3712 - val_accuracy: 0.8985\n",
            "Epoch 20/50\n",
            "368/368 [==============================] - 22s 59ms/step - loss: 0.1410 - accuracy: 0.9433 - val_loss: 0.3775 - val_accuracy: 0.8924\n",
            "Epoch 21/50\n",
            "368/368 [==============================] - 20s 54ms/step - loss: 0.1353 - accuracy: 0.9463 - val_loss: 0.3948 - val_accuracy: 0.8734\n",
            "Epoch 22/50\n",
            "368/368 [==============================] - 22s 60ms/step - loss: 0.1495 - accuracy: 0.9422 - val_loss: 0.4355 - val_accuracy: 0.8761\n",
            "Epoch 23/50\n",
            "368/368 [==============================] - 23s 64ms/step - loss: 0.1473 - accuracy: 0.9410 - val_loss: 0.3645 - val_accuracy: 0.8890\n",
            "Epoch 24/50\n",
            "368/368 [==============================] - 23s 62ms/step - loss: 0.1411 - accuracy: 0.9446 - val_loss: 0.4048 - val_accuracy: 0.8863\n",
            "Epoch 25/50\n",
            "368/368 [==============================] - 23s 63ms/step - loss: 0.1248 - accuracy: 0.9476 - val_loss: 0.3627 - val_accuracy: 0.9013\n",
            "Epoch 26/50\n",
            "368/368 [==============================] - 23s 62ms/step - loss: 0.1361 - accuracy: 0.9476 - val_loss: 0.3560 - val_accuracy: 0.8894\n",
            "Epoch 27/50\n",
            "368/368 [==============================] - 21s 57ms/step - loss: 0.1287 - accuracy: 0.9478 - val_loss: 0.3752 - val_accuracy: 0.8958\n",
            "Epoch 28/50\n",
            "368/368 [==============================] - 22s 60ms/step - loss: 0.1245 - accuracy: 0.9468 - val_loss: 0.4280 - val_accuracy: 0.8921\n",
            "Epoch 29/50\n",
            "368/368 [==============================] - 22s 60ms/step - loss: 0.1292 - accuracy: 0.9476 - val_loss: 0.4207 - val_accuracy: 0.8951\n",
            "Epoch 30/50\n",
            "368/368 [==============================] - 21s 56ms/step - loss: 0.1465 - accuracy: 0.9425 - val_loss: 0.4114 - val_accuracy: 0.8931\n",
            "Epoch 31/50\n",
            "368/368 [==============================] - 23s 64ms/step - loss: 0.1391 - accuracy: 0.9434 - val_loss: 0.3513 - val_accuracy: 0.9023\n",
            "Epoch 32/50\n",
            "368/368 [==============================] - 22s 59ms/step - loss: 0.1192 - accuracy: 0.9489 - val_loss: 0.3610 - val_accuracy: 0.8992\n",
            "Epoch 33/50\n",
            "368/368 [==============================] - 24s 64ms/step - loss: 0.1211 - accuracy: 0.9475 - val_loss: 0.4558 - val_accuracy: 0.8965\n",
            "Epoch 34/50\n",
            "368/368 [==============================] - 23s 63ms/step - loss: 0.1265 - accuracy: 0.9491 - val_loss: 0.4032 - val_accuracy: 0.9016\n",
            "Epoch 35/50\n",
            "368/368 [==============================] - 22s 59ms/step - loss: 0.1333 - accuracy: 0.9442 - val_loss: 0.3928 - val_accuracy: 0.9002\n",
            "Epoch 36/50\n",
            "368/368 [==============================] - 22s 59ms/step - loss: 0.1275 - accuracy: 0.9474 - val_loss: 0.4917 - val_accuracy: 0.8772\n",
            "Epoch 37/50\n",
            "368/368 [==============================] - 22s 60ms/step - loss: 0.1187 - accuracy: 0.9509 - val_loss: 0.4586 - val_accuracy: 0.8890\n",
            "Epoch 38/50\n",
            "368/368 [==============================] - 22s 59ms/step - loss: 0.1159 - accuracy: 0.9525 - val_loss: 0.4195 - val_accuracy: 0.8972\n",
            "Epoch 39/50\n",
            "368/368 [==============================] - 23s 62ms/step - loss: 0.1218 - accuracy: 0.9495 - val_loss: 0.4702 - val_accuracy: 0.8646\n",
            "Epoch 40/50\n",
            "368/368 [==============================] - 22s 59ms/step - loss: 0.1190 - accuracy: 0.9490 - val_loss: 0.5801 - val_accuracy: 0.8687\n",
            "Epoch 41/50\n",
            "368/368 [==============================] - 24s 66ms/step - loss: 0.1284 - accuracy: 0.9470 - val_loss: 0.4840 - val_accuracy: 0.8884\n",
            "Epoch 42/50\n",
            "368/368 [==============================] - 22s 59ms/step - loss: 0.1166 - accuracy: 0.9494 - val_loss: 0.4572 - val_accuracy: 0.8914\n",
            "Epoch 43/50\n",
            "368/368 [==============================] - 22s 61ms/step - loss: 0.1141 - accuracy: 0.9527 - val_loss: 0.3752 - val_accuracy: 0.9043\n",
            "Epoch 44/50\n",
            "368/368 [==============================] - 23s 62ms/step - loss: 0.1161 - accuracy: 0.9506 - val_loss: 0.5404 - val_accuracy: 0.8985\n",
            "Epoch 45/50\n",
            "368/368 [==============================] - 22s 60ms/step - loss: 0.1132 - accuracy: 0.9531 - val_loss: 0.4522 - val_accuracy: 0.8972\n",
            "Epoch 46/50\n",
            "368/368 [==============================] - 22s 60ms/step - loss: 0.1119 - accuracy: 0.9513 - val_loss: 0.4156 - val_accuracy: 0.8924\n",
            "Epoch 47/50\n",
            "368/368 [==============================] - 24s 65ms/step - loss: 0.1133 - accuracy: 0.9505 - val_loss: 0.4789 - val_accuracy: 0.8968\n",
            "Epoch 48/50\n",
            "368/368 [==============================] - 22s 59ms/step - loss: 0.1191 - accuracy: 0.9508 - val_loss: 0.5931 - val_accuracy: 0.8748\n",
            "Epoch 49/50\n",
            "368/368 [==============================] - 21s 58ms/step - loss: 0.1093 - accuracy: 0.9550 - val_loss: 0.4615 - val_accuracy: 0.8962\n",
            "Epoch 50/50\n",
            "368/368 [==============================] - 24s 64ms/step - loss: 0.1087 - accuracy: 0.9523 - val_loss: 0.4846 - val_accuracy: 0.8975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "hKKkcqwzKgfh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "570ae8f8-d3fc-44cd-8912-ec2709952fab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Validation Accuracy: 0.9043094515800476\n"
          ]
        }
      ],
      "source": [
        "# Maximum validation Accuracy\n",
        "val_accuracy_tensor = history.history['val_accuracy']\n",
        "best_val_accuracy = np.max(val_accuracy_tensor)\n",
        "print(\"Best Validation Accuracy:\", best_val_accuracy)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}