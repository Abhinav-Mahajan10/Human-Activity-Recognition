{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9ECDQJJyKgfc"
      },
      "outputs": [],
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import dstack\n",
        "from pandas import read_csv\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from matplotlib import pyplot as plt\n",
        "import keras\n",
        "from keras import initializers\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras.backend import *\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import Dense, Layer, Lambda, Input, Flatten, Dropout, Conv2D, MaxPooling2D, Conv1D, MaxPooling1D, LSTM, TimeDistributed, ConvLSTM2D, Permute, Reshape, Conv2D\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.python.ops import array_ops\n",
        "from tensorflow.python.ops import math_ops\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping\n",
        "\n",
        "def own_batch_dot(x, y, axes=None):\n",
        "\t\"\"\"Batchwise dot product.\n",
        "\t`batch_dot` is used to compute dot product of `x` and `y` when\n",
        "\t`x` and `y` are data in batch, i.e. in a shape of\n",
        "\t`(batch_size, :)`.\n",
        "\t`batch_dot` results in a tensor or variable with less dimensions\n",
        "\tthan the input. If the number of dimensions is reduced to 1,\n",
        "\twe use `expand_dims` to make sure that ndim is at least 2.\n",
        "\tArguments:\n",
        "\t\tx: Keras tensor or variable with `ndim >= 2`.\n",
        "\t\ty: Keras tensor or variable with `ndim >= 2`.\n",
        "\t\taxes: list of (or single) int with target dimensions.\n",
        "\t\t\tThe lengths of `axes[0]` and `axes[1]` should be the same.\n",
        "\tReturns:\n",
        "\t\tA tensor with shape equal to the concatenation of `x`'s shape\n",
        "\t\t(less the dimension that was summed over) and `y`'s shape\n",
        "\t\t(less the batch dimension and the dimension that was summed over).\n",
        "\t\tIf the final rank is 1, we reshape it to `(batch_size, 1)`.\n",
        "\tExamples:\n",
        "\t\tAssume `x = [[1, 2], [3, 4]]` and `y = [[5, 6], [7, 8]]`\n",
        "\t\t`batch_dot(x, y, axes=1) = [[17, 53]]` which is the main diagonal\n",
        "\t\tof `x.dot(y.T)`, although we never have to calculate the off-diagonal\n",
        "\t\telements.\n",
        "\t\tShape inference:\n",
        "\t\tLet `x`'s shape be `(100, 20)` and `y`'s shape be `(100, 30, 20)`.\n",
        "\t\tIf `axes` is (1, 2), to find the output shape of resultant tensor,\n",
        "\t\t\tloop through each dimension in `x`'s shape and `y`'s shape:\n",
        "\t\t* `x.shape[0]` : 100 : append to output shape\n",
        "\t\t* `x.shape[1]` : 20 : do not append to output shape,\n",
        "\t\t\tdimension 1 of `x` has been summed over. (`dot_axes[0]` = 1)\n",
        "\t\t* `y.shape[0]` : 100 : do not append to output shape,\n",
        "\t\t\talways ignore first dimension of `y`\n",
        "\t\t* `y.shape[1]` : 30 : append to output shape\n",
        "\t\t* `y.shape[2]` : 20 : do not append to output shape,\n",
        "\t\t\tdimension 2 of `y` has been summed over. (`dot_axes[1]` = 2)\n",
        "\t\t`output_shape` = `(100, 30)`\n",
        "\t```python\n",
        "\t\t>>> x_batch = K.ones(shape=(32, 20, 1))\n",
        "\t\t>>> y_batch = K.ones(shape=(32, 30, 20))\n",
        "\t\t>>> xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=[1, 2])\n",
        "\t\t>>> K.int_shape(xy_batch_dot)\n",
        "\t\t(32, 1, 30)\n",
        "\t```\n",
        "\t\"\"\"\n",
        "\tif isinstance(axes, int):\n",
        "\t\taxes = (axes, axes)\n",
        "\tx_ndim = ndim(x)\n",
        "\ty_ndim = ndim(y)\n",
        "\tif axes is None:\n",
        "\t\t# behaves like tf.batch_matmul as default\n",
        "\t\taxes = [x_ndim - 1, y_ndim - 2]\n",
        "\tif x_ndim > y_ndim:\n",
        "\t\tdiff = x_ndim - y_ndim\n",
        "\t\ty = array_ops.reshape(y,\n",
        "\t\t\t\t\t\t\tarray_ops.concat(\n",
        "\t\t\t\t\t\t\t\t[array_ops.shape(y), [1] * (diff)], axis=0))\n",
        "\telif y_ndim > x_ndim:\n",
        "\t\tdiff = y_ndim - x_ndim\n",
        "\t\tx = array_ops.reshape(x,\n",
        "\t\t\t\t\t\t\tarray_ops.concat(\n",
        "\t\t\t\t\t\t\t\t[array_ops.shape(x), [1] * (diff)], axis=0))\n",
        "\telse:\n",
        "\t\tdiff = 0\n",
        "\tif ndim(x) == 2 and ndim(y) == 2:\n",
        "\t\tif axes[0] == axes[1]:\n",
        "\t\t\tout = math_ops.reduce_sum(math_ops.multiply(x, y), axes[0])\n",
        "\t\telse:\n",
        "\t\t\tout = math_ops.reduce_sum(\n",
        "\t\t\tmath_ops.multiply(array_ops.transpose(x, [1, 0]), y), axes[1])\n",
        "\telse:\n",
        "\t\tadj_x = None if axes[0] == ndim(x) - 1 else True\n",
        "\t\tadj_y = True if axes[1] == ndim(y) - 1 else None\n",
        "\t\tout = math_ops.matmul(x, y, adjoint_a=adj_x, adjoint_b=adj_y)\n",
        "\tif diff:\n",
        "\t\tif x_ndim > y_ndim:\n",
        "\t\t\tidx = x_ndim + y_ndim - 3\n",
        "\t\telse:\n",
        "\t\t\tidx = x_ndim - 1\n",
        "\t\t\tout = array_ops.squeeze(out, list(range(idx, idx + diff)))\n",
        "\tif ndim(out) == 1:\n",
        "\t\tout = expand_dims(out, 1)\n",
        "\treturn out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "V-9Tyf2WKgfe"
      },
      "outputs": [],
      "source": [
        "# load a single file as a numpy array\n",
        "def load_file(filepath):\n",
        "    dataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
        "    return dataframe.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RFtsLcBTKgff"
      },
      "outputs": [],
      "source": [
        "# load a list of files and return as a 3d numpy array\n",
        "def load_group(filenames, prefix=''):\n",
        "    loaded = list()\n",
        "    for name in filenames:\n",
        "        data = load_file(prefix + name)\n",
        "        loaded.append(data)\n",
        "    # stack group so that features are the 3rd dimension\n",
        "    loaded = dstack(loaded)\n",
        "    return loaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IDNdqOsIKgff"
      },
      "outputs": [],
      "source": [
        "# load a dataset group, such as train or test\n",
        "def load_dataset_group(group, prefix=''):\n",
        "    filepath = prefix + group + '/Inertial Signals/'\n",
        "    print('File Path : ',filepath)\n",
        "    # load all 9 files as a single array\n",
        "    filenames = list()\n",
        "    # total acceleration\n",
        "    filenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
        "    # body acceleration\n",
        "    filenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
        "    # body gyroscope\n",
        "    filenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
        "    # load input data\n",
        "    X = load_group(filenames, filepath)\n",
        "    # load class output\n",
        "    y = load_file(prefix + group + '/y_'+group+'.txt')\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pGNunmWKgfg",
        "outputId": "aadd67a0-8440-43af-bf4c-08d9ae17adde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File Path :  UCIDataset/train/Inertial Signals/\n",
            "File Path :  UCIDataset/test/Inertial Signals/\n",
            "X_train.shape :  (7352, 128, 9)\n",
            "Y_train.shape :  (7352, 6)\n",
            "X_test.shape :  (2947, 128, 9)\n",
            "Y_test.shape :  (2947, 6)\n"
          ]
        }
      ],
      "source": [
        "# load all train\n",
        "X_train, Y_train = load_dataset_group('train', 'UCIDataset/')\n",
        "# load all test\n",
        "X_test, Y_test = load_dataset_group('test', 'UCIDataset/')\n",
        "\n",
        "# zero-offset class values\n",
        "Y_train = Y_train - 1\n",
        "Y_test = Y_test - 1\n",
        "# one hot encode y\n",
        "Y_train = to_categorical(Y_train)\n",
        "Y_test = to_categorical(Y_test)\n",
        "\n",
        "print('X_train.shape : ', X_train.shape)\n",
        "print('Y_train.shape : ', Y_train.shape)\n",
        "print('X_test.shape : ', X_test.shape)\n",
        "print('Y_test.shape : ', Y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "_zhoqoAJKgfg"
      },
      "outputs": [],
      "source": [
        "verbose = 1\n",
        "epochs = 50\n",
        "batch_size = 128\n",
        "\n",
        "n_timesteps = X_train.shape[1]\n",
        "n_features = X_train.shape[2]\n",
        "n_outputs = Y_train.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "SCAD5m6UKgfg"
      },
      "outputs": [],
      "source": [
        "checkpoint = ModelCheckpoint(\"CNN_Capsule_weights.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='auto')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRrzk8FTKgfg",
        "outputId": "1cb3c0f7-e6c6-42b2-a862-8760c0b86f11",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensor(\"digit_capsule_layer_12/transpose_1:0\", shape=(None, 6, 496), dtype=float32)\n",
            "Tensor(\"digit_capsule_layer_12/Squeeze:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Tensor(\"digit_capsule_layer_12/mul:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Tensor(\"digit_capsule_layer_12/transpose_3:0\", shape=(None, 6, 496), dtype=float32)\n",
            "Tensor(\"digit_capsule_layer_12/Squeeze_2:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Tensor(\"digit_capsule_layer_12/mul_1:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Model: \"model_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_23 (InputLayer)       [(None, 128, 9)]          0         \n",
            "                                                                 \n",
            " conv1d_24 (Conv1D)          (None, 125, 128)          4736      \n",
            "                                                                 \n",
            " max_pooling1d_24 (MaxPoolin  (None, 62, 128)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_25 (Conv1D)          (None, 62, 128)           65664     \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, 62, 128)           0         \n",
            "                                                                 \n",
            " max_pooling1d_25 (MaxPoolin  (None, 31, 128)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " reshape_13 (Reshape)        (None, 496, 8)            0         \n",
            "                                                                 \n",
            " lambda_26 (Lambda)          (None, 496, 8)            0         \n",
            "                                                                 \n",
            " digit_capsule_layer_12 (Dig  (None, 6, 16)            380928    \n",
            " itCapsuleLayer)                                                 \n",
            "                                                                 \n",
            " lambda_27 (Lambda)          (None, 6)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 451,328\n",
            "Trainable params: 451,328\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "Tensor(\"model_12/digit_capsule_layer_12/transpose_1:0\", shape=(None, 6, 496), dtype=float32)\n",
            "Tensor(\"model_12/digit_capsule_layer_12/Squeeze:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Tensor(\"model_12/digit_capsule_layer_12/mul:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Tensor(\"model_12/digit_capsule_layer_12/transpose_3:0\", shape=(None, 6, 496), dtype=float32)\n",
            "Tensor(\"model_12/digit_capsule_layer_12/Squeeze_2:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Tensor(\"model_12/digit_capsule_layer_12/mul_1:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Tensor(\"model_12/digit_capsule_layer_12/transpose_1:0\", shape=(None, 6, 496), dtype=float32)\n",
            "Tensor(\"model_12/digit_capsule_layer_12/Squeeze:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Tensor(\"model_12/digit_capsule_layer_12/mul:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Tensor(\"model_12/digit_capsule_layer_12/transpose_3:0\", shape=(None, 6, 496), dtype=float32)\n",
            "Tensor(\"model_12/digit_capsule_layer_12/Squeeze_2:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Tensor(\"model_12/digit_capsule_layer_12/mul_1:0\", shape=(None, 6, 16), dtype=float32)\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.5666 - accuracy: 0.8039Tensor(\"model_12/digit_capsule_layer_12/transpose_1:0\", shape=(None, 6, 496), dtype=float32)\n",
            "Tensor(\"model_12/digit_capsule_layer_12/Squeeze:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Tensor(\"model_12/digit_capsule_layer_12/mul:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Tensor(\"model_12/digit_capsule_layer_12/transpose_3:0\", shape=(None, 6, 496), dtype=float32)\n",
            "Tensor(\"model_12/digit_capsule_layer_12/Squeeze_2:0\", shape=(None, 6, 16), dtype=float32)\n",
            "Tensor(\"model_12/digit_capsule_layer_12/mul_1:0\", shape=(None, 6, 16), dtype=float32)\n",
            "\n",
            "Epoch 1: val_accuracy improved from -inf to 0.88938, saving model to CNN_Capsule_weights.keras\n",
            "58/58 [==============================] - 12s 179ms/step - loss: 0.5666 - accuracy: 0.8039 - val_loss: 0.4009 - val_accuracy: 0.8894\n",
            "Epoch 2/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.1763 - accuracy: 0.9472\n",
            "Epoch 2: val_accuracy improved from 0.88938 to 0.91822, saving model to CNN_Capsule_weights.keras\n",
            "58/58 [==============================] - 10s 171ms/step - loss: 0.1763 - accuracy: 0.9472 - val_loss: 0.2937 - val_accuracy: 0.9182\n",
            "Epoch 3/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.1378 - accuracy: 0.9514\n",
            "Epoch 3: val_accuracy did not improve from 0.91822\n",
            "58/58 [==============================] - 10s 169ms/step - loss: 0.1378 - accuracy: 0.9514 - val_loss: 0.2637 - val_accuracy: 0.9097\n",
            "Epoch 4/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.1253 - accuracy: 0.9540\n",
            "Epoch 4: val_accuracy did not improve from 0.91822\n",
            "58/58 [==============================] - 10s 165ms/step - loss: 0.1253 - accuracy: 0.9540 - val_loss: 0.2610 - val_accuracy: 0.9135\n",
            "Epoch 5/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.1220 - accuracy: 0.9525\n",
            "Epoch 5: val_accuracy did not improve from 0.91822\n",
            "58/58 [==============================] - 10s 166ms/step - loss: 0.1220 - accuracy: 0.9525 - val_loss: 0.2535 - val_accuracy: 0.9145\n",
            "Epoch 6/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.1150 - accuracy: 0.9544\n",
            "Epoch 6: val_accuracy did not improve from 0.91822\n",
            "58/58 [==============================] - 10s 167ms/step - loss: 0.1150 - accuracy: 0.9544 - val_loss: 0.2414 - val_accuracy: 0.9155\n",
            "Epoch 7/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.1114 - accuracy: 0.9540\n",
            "Epoch 7: val_accuracy did not improve from 0.91822\n",
            "58/58 [==============================] - 10s 164ms/step - loss: 0.1114 - accuracy: 0.9540 - val_loss: 0.2522 - val_accuracy: 0.9152\n",
            "Epoch 8/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.1022 - accuracy: 0.9584\n",
            "Epoch 8: val_accuracy did not improve from 0.91822\n",
            "58/58 [==============================] - 10s 166ms/step - loss: 0.1022 - accuracy: 0.9584 - val_loss: 0.2492 - val_accuracy: 0.9172\n",
            "Epoch 9/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0996 - accuracy: 0.9578\n",
            "Epoch 9: val_accuracy did not improve from 0.91822\n",
            "58/58 [==============================] - 10s 166ms/step - loss: 0.0996 - accuracy: 0.9578 - val_loss: 0.2582 - val_accuracy: 0.9182\n",
            "Epoch 10/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0960 - accuracy: 0.9593\n",
            "Epoch 10: val_accuracy did not improve from 0.91822\n",
            "58/58 [==============================] - 10s 172ms/step - loss: 0.0960 - accuracy: 0.9593 - val_loss: 0.2592 - val_accuracy: 0.9131\n",
            "Epoch 11/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0919 - accuracy: 0.9595\n",
            "Epoch 11: val_accuracy did not improve from 0.91822\n",
            "58/58 [==============================] - 11s 182ms/step - loss: 0.0919 - accuracy: 0.9595 - val_loss: 0.2691 - val_accuracy: 0.9135\n",
            "Epoch 12/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0868 - accuracy: 0.9619\n",
            "Epoch 12: val_accuracy did not improve from 0.91822\n",
            "58/58 [==============================] - 11s 181ms/step - loss: 0.0868 - accuracy: 0.9619 - val_loss: 0.2525 - val_accuracy: 0.9175\n",
            "Epoch 13/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0848 - accuracy: 0.9630\n",
            "Epoch 13: val_accuracy improved from 0.91822 to 0.91924, saving model to CNN_Capsule_weights.keras\n",
            "58/58 [==============================] - 11s 188ms/step - loss: 0.0848 - accuracy: 0.9630 - val_loss: 0.2369 - val_accuracy: 0.9192\n",
            "Epoch 14/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0805 - accuracy: 0.9637\n",
            "Epoch 14: val_accuracy did not improve from 0.91924\n",
            "58/58 [==============================] - 10s 170ms/step - loss: 0.0805 - accuracy: 0.9637 - val_loss: 0.2601 - val_accuracy: 0.9175\n",
            "Epoch 15/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0812 - accuracy: 0.9656\n",
            "Epoch 15: val_accuracy did not improve from 0.91924\n",
            "58/58 [==============================] - 10s 168ms/step - loss: 0.0812 - accuracy: 0.9656 - val_loss: 0.2625 - val_accuracy: 0.9091\n",
            "Epoch 16/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0797 - accuracy: 0.9642\n",
            "Epoch 16: val_accuracy improved from 0.91924 to 0.92060, saving model to CNN_Capsule_weights.keras\n",
            "58/58 [==============================] - 10s 177ms/step - loss: 0.0797 - accuracy: 0.9642 - val_loss: 0.2414 - val_accuracy: 0.9206\n",
            "Epoch 17/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0746 - accuracy: 0.9660\n",
            "Epoch 17: val_accuracy improved from 0.92060 to 0.92195, saving model to CNN_Capsule_weights.keras\n",
            "58/58 [==============================] - 11s 189ms/step - loss: 0.0746 - accuracy: 0.9660 - val_loss: 0.2592 - val_accuracy: 0.9220\n",
            "Epoch 18/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0690 - accuracy: 0.9709\n",
            "Epoch 18: val_accuracy did not improve from 0.92195\n",
            "58/58 [==============================] - 11s 184ms/step - loss: 0.0690 - accuracy: 0.9709 - val_loss: 0.2610 - val_accuracy: 0.9128\n",
            "Epoch 19/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9709\n",
            "Epoch 19: val_accuracy did not improve from 0.92195\n",
            "58/58 [==============================] - 10s 180ms/step - loss: 0.0695 - accuracy: 0.9709 - val_loss: 0.2654 - val_accuracy: 0.9175\n",
            "Epoch 20/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0652 - accuracy: 0.9718\n",
            "Epoch 20: val_accuracy did not improve from 0.92195\n",
            "58/58 [==============================] - 10s 174ms/step - loss: 0.0652 - accuracy: 0.9718 - val_loss: 0.2606 - val_accuracy: 0.9162\n",
            "Epoch 21/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0669 - accuracy: 0.9701\n",
            "Epoch 21: val_accuracy did not improve from 0.92195\n",
            "58/58 [==============================] - 10s 169ms/step - loss: 0.0669 - accuracy: 0.9701 - val_loss: 0.2621 - val_accuracy: 0.9148\n",
            "Epoch 22/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0615 - accuracy: 0.9716\n",
            "Epoch 22: val_accuracy did not improve from 0.92195\n",
            "58/58 [==============================] - 10s 171ms/step - loss: 0.0615 - accuracy: 0.9716 - val_loss: 0.2596 - val_accuracy: 0.9165\n",
            "Epoch 23/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0590 - accuracy: 0.9740\n",
            "Epoch 23: val_accuracy did not improve from 0.92195\n",
            "58/58 [==============================] - 10s 179ms/step - loss: 0.0590 - accuracy: 0.9740 - val_loss: 0.2640 - val_accuracy: 0.9152\n",
            "Epoch 24/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0588 - accuracy: 0.9755\n",
            "Epoch 24: val_accuracy did not improve from 0.92195\n",
            "58/58 [==============================] - 11s 183ms/step - loss: 0.0588 - accuracy: 0.9755 - val_loss: 0.2656 - val_accuracy: 0.9209\n",
            "Epoch 25/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0578 - accuracy: 0.9755\n",
            "Epoch 25: val_accuracy improved from 0.92195 to 0.92399, saving model to CNN_Capsule_weights.keras\n",
            "58/58 [==============================] - 11s 191ms/step - loss: 0.0578 - accuracy: 0.9755 - val_loss: 0.2561 - val_accuracy: 0.9240\n",
            "Epoch 26/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0576 - accuracy: 0.9747\n",
            "Epoch 26: val_accuracy did not improve from 0.92399\n",
            "58/58 [==============================] - 11s 185ms/step - loss: 0.0576 - accuracy: 0.9747 - val_loss: 0.2635 - val_accuracy: 0.9179\n",
            "Epoch 27/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0548 - accuracy: 0.9762\n",
            "Epoch 27: val_accuracy improved from 0.92399 to 0.92433, saving model to CNN_Capsule_weights.keras\n",
            "58/58 [==============================] - 11s 182ms/step - loss: 0.0548 - accuracy: 0.9762 - val_loss: 0.2510 - val_accuracy: 0.9243\n",
            "Epoch 28/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0584 - accuracy: 0.9750\n",
            "Epoch 28: val_accuracy did not improve from 0.92433\n",
            "58/58 [==============================] - 10s 172ms/step - loss: 0.0584 - accuracy: 0.9750 - val_loss: 0.2815 - val_accuracy: 0.9162\n",
            "Epoch 29/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0526 - accuracy: 0.9781\n",
            "Epoch 29: val_accuracy did not improve from 0.92433\n",
            "58/58 [==============================] - 10s 169ms/step - loss: 0.0526 - accuracy: 0.9781 - val_loss: 0.2613 - val_accuracy: 0.9230\n",
            "Epoch 30/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0517 - accuracy: 0.9784\n",
            "Epoch 30: val_accuracy did not improve from 0.92433\n",
            "58/58 [==============================] - 10s 177ms/step - loss: 0.0517 - accuracy: 0.9784 - val_loss: 0.2639 - val_accuracy: 0.9111\n",
            "Epoch 31/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0484 - accuracy: 0.9801\n",
            "Epoch 31: val_accuracy did not improve from 0.92433\n",
            "58/58 [==============================] - 10s 179ms/step - loss: 0.0484 - accuracy: 0.9801 - val_loss: 0.2732 - val_accuracy: 0.9216\n",
            "Epoch 32/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0483 - accuracy: 0.9801\n",
            "Epoch 32: val_accuracy did not improve from 0.92433\n",
            "58/58 [==============================] - 10s 179ms/step - loss: 0.0483 - accuracy: 0.9801 - val_loss: 0.2727 - val_accuracy: 0.9148\n",
            "Epoch 33/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0496 - accuracy: 0.9800\n",
            "Epoch 33: val_accuracy improved from 0.92433 to 0.92603, saving model to CNN_Capsule_weights.keras\n",
            "58/58 [==============================] - 11s 185ms/step - loss: 0.0496 - accuracy: 0.9800 - val_loss: 0.2649 - val_accuracy: 0.9260\n",
            "Epoch 34/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0469 - accuracy: 0.9805\n",
            "Epoch 34: val_accuracy did not improve from 0.92603\n",
            "58/58 [==============================] - 11s 182ms/step - loss: 0.0469 - accuracy: 0.9805 - val_loss: 0.2674 - val_accuracy: 0.9253\n",
            "Epoch 35/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0439 - accuracy: 0.9826\n",
            "Epoch 35: val_accuracy did not improve from 0.92603\n",
            "58/58 [==============================] - 10s 179ms/step - loss: 0.0439 - accuracy: 0.9826 - val_loss: 0.2557 - val_accuracy: 0.9216\n",
            "Epoch 36/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0458 - accuracy: 0.9827\n",
            "Epoch 36: val_accuracy improved from 0.92603 to 0.92637, saving model to CNN_Capsule_weights.keras\n",
            "58/58 [==============================] - 11s 190ms/step - loss: 0.0458 - accuracy: 0.9827 - val_loss: 0.2659 - val_accuracy: 0.9264\n",
            "Epoch 37/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0391 - accuracy: 0.9844\n",
            "Epoch 37: val_accuracy improved from 0.92637 to 0.93112, saving model to CNN_Capsule_weights.keras\n",
            "58/58 [==============================] - 11s 183ms/step - loss: 0.0391 - accuracy: 0.9844 - val_loss: 0.2599 - val_accuracy: 0.9311\n",
            "Epoch 38/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0375 - accuracy: 0.9871\n",
            "Epoch 38: val_accuracy did not improve from 0.93112\n",
            "58/58 [==============================] - 10s 176ms/step - loss: 0.0375 - accuracy: 0.9871 - val_loss: 0.2646 - val_accuracy: 0.9223\n",
            "Epoch 39/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0382 - accuracy: 0.9853\n",
            "Epoch 39: val_accuracy did not improve from 0.93112\n",
            "58/58 [==============================] - 10s 169ms/step - loss: 0.0382 - accuracy: 0.9853 - val_loss: 0.3100 - val_accuracy: 0.9111\n",
            "Epoch 40/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0377 - accuracy: 0.9857\n",
            "Epoch 40: val_accuracy did not improve from 0.93112\n",
            "58/58 [==============================] - 10s 171ms/step - loss: 0.0377 - accuracy: 0.9857 - val_loss: 0.2776 - val_accuracy: 0.9145\n",
            "Epoch 41/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0393 - accuracy: 0.9861\n",
            "Epoch 41: val_accuracy did not improve from 0.93112\n",
            "58/58 [==============================] - 10s 181ms/step - loss: 0.0393 - accuracy: 0.9861 - val_loss: 0.2650 - val_accuracy: 0.9298\n",
            "Epoch 42/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0398 - accuracy: 0.9850\n",
            "Epoch 42: val_accuracy did not improve from 0.93112\n",
            "58/58 [==============================] - 10s 177ms/step - loss: 0.0398 - accuracy: 0.9850 - val_loss: 0.2794 - val_accuracy: 0.9108\n",
            "Epoch 43/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0349 - accuracy: 0.9887\n",
            "Epoch 43: val_accuracy did not improve from 0.93112\n",
            "58/58 [==============================] - 10s 180ms/step - loss: 0.0349 - accuracy: 0.9887 - val_loss: 0.2617 - val_accuracy: 0.9213\n",
            "Epoch 44/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0285 - accuracy: 0.9913\n",
            "Epoch 44: val_accuracy did not improve from 0.93112\n",
            "58/58 [==============================] - 11s 181ms/step - loss: 0.0285 - accuracy: 0.9913 - val_loss: 0.2608 - val_accuracy: 0.9209\n",
            "Epoch 45/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0307 - accuracy: 0.9888\n",
            "Epoch 45: val_accuracy did not improve from 0.93112\n",
            "58/58 [==============================] - 10s 179ms/step - loss: 0.0307 - accuracy: 0.9888 - val_loss: 0.2638 - val_accuracy: 0.9192\n",
            "Epoch 46/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0296 - accuracy: 0.9901\n",
            "Epoch 46: val_accuracy did not improve from 0.93112\n",
            "58/58 [==============================] - 11s 195ms/step - loss: 0.0296 - accuracy: 0.9901 - val_loss: 0.2750 - val_accuracy: 0.9203\n",
            "Epoch 47/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 0.9897\n",
            "Epoch 47: val_accuracy did not improve from 0.93112\n",
            "58/58 [==============================] - 11s 182ms/step - loss: 0.0279 - accuracy: 0.9897 - val_loss: 0.2825 - val_accuracy: 0.9158\n",
            "Epoch 48/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0238 - accuracy: 0.9922\n",
            "Epoch 48: val_accuracy did not improve from 0.93112\n",
            "58/58 [==============================] - 11s 185ms/step - loss: 0.0238 - accuracy: 0.9922 - val_loss: 0.2788 - val_accuracy: 0.9213\n",
            "Epoch 49/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 0.9933\n",
            "Epoch 49: val_accuracy did not improve from 0.93112\n",
            "58/58 [==============================] - 10s 180ms/step - loss: 0.0224 - accuracy: 0.9933 - val_loss: 0.2619 - val_accuracy: 0.9213\n",
            "Epoch 50/50\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 0.9893\n",
            "Epoch 50: val_accuracy did not improve from 0.93112\n",
            "58/58 [==============================] - 10s 179ms/step - loss: 0.0288 - accuracy: 0.9893 - val_loss: 0.2676 - val_accuracy: 0.9179\n"
          ]
        }
      ],
      "source": [
        "inputs = layers.Input(shape=(128,9))\n",
        "\n",
        "# input_shape = Input(shape=(n_timesteps,n_features))\n",
        "conv1 = Conv1D(filters=128, kernel_size=4, activation='relu')(inputs)\n",
        "max_pooling = MaxPooling1D(pool_size=2)(conv1)\n",
        "conv2 = Conv1D(filters=128, kernel_size=4, activation='relu', padding='same')(max_pooling)\n",
        "dropout = Dropout(0.5)(conv2)\n",
        "max_pooling = MaxPooling1D(pool_size=2)(dropout)\n",
        "reshaped = Reshape((496,8))(max_pooling)\n",
        "\n",
        "def squash(inputs):\n",
        "    # take norm of input vectors\n",
        "    squared_norm = K.sum(K.square(inputs), axis = -1, keepdims = True)\n",
        "    # use the formula for non-linear function to return squashed output\n",
        "    return ((squared_norm/(1+squared_norm))/(K.sqrt(squared_norm+K.epsilon())))*inputs\n",
        "\n",
        "# squash the reshaped output to make length of vector b/w 0 and 1\n",
        "squashed_output = Lambda(squash)(reshaped)\n",
        "\n",
        "class DigitCapsuleLayer(Layer):\n",
        "    # creating a layer class in keras\n",
        "    def __init__(self, **kwargs):\n",
        "        super(DigitCapsuleLayer, self).__init__(**kwargs)\n",
        "        self.kernel_initializer = initializers.get('glorot_uniform')\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # initialize weight matrix for each capsule in lower layer\n",
        "        self.W = self.add_weight(shape = [6, 496, 16, 8], initializer = self.kernel_initializer, name = 'weights')\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inputs = K.expand_dims(inputs, 1)\n",
        "        inputs = K.tile(inputs, [1, 6, 1, 1])\n",
        "        # matrix multiplication b/w previous layer output and weight matrix\n",
        "        inputs = K.map_fn(lambda x: own_batch_dot(x, self.W, [2, 3]), elems=inputs)\n",
        "        b = tf.zeros(shape = [K.shape(inputs)[0], 6, 496])\n",
        "\n",
        "\t\t# routing algorithm with updating coupling coefficient c, using scalar product b/w input capsule and output capsule\n",
        "        for i in range(3-1):\n",
        "            # print(b)\n",
        "            c = tf.nn.softmax(b, axis=1)\n",
        "            print(c)\n",
        "            s = own_batch_dot(c, inputs, [2, 2])\n",
        "            print(s)\n",
        "            v = squash(s)\n",
        "            print(v)\n",
        "            b = b + own_batch_dot(v, inputs, [2,3])\n",
        "            \n",
        "        return v\n",
        "    \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return tuple([None, 6, 16])\n",
        "\n",
        "def output_layer(inputs):\n",
        "    return K.sqrt(K.sum(K.square(inputs), -1) + K.epsilon())\n",
        "\n",
        "digit_caps = DigitCapsuleLayer()(squashed_output)\n",
        "outputs = Lambda(output_layer)(digit_caps)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test),\n",
        "                    epochs=epochs, batch_size=batch_size, verbose=verbose, callbacks=[checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "SXzSMgFbKgfh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Validation Accuracy: 0.9311164021492004\n"
          ]
        }
      ],
      "source": [
        "# Maximum validation Accuracy\n",
        "val_accuracy_tensor = history.history['val_accuracy']\n",
        "best_val_accuracy = np.max(val_accuracy_tensor)\n",
        "print(\"Best Validation Accuracy:\", best_val_accuracy)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
